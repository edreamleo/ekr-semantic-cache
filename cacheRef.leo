<?xml version="1.0" encoding="utf-8"?>
<!-- Created by Leo: https://leo-editor.github.io/leo-editor/leo_toc.html -->
<leo_file xmlns:leo="https://leo-editor.github.io/leo-editor/namespaces/leo-python-editor/1.1" >
<leo_header file_format="2"/>
<globals/>
<preferences/>
<find_panel_settings/>
<vnodes>
<v t="ekr.20250426044347.1"><vh>Startup</vh>
<v t="ekr.20250426044445.1"><vh>@button backup</vh></v>
<v t="ekr.20250427065404.1"><vh>@button check</vh></v>
<v t="ekr.20250512060544.1"><vh>@button run</vh></v>
<v t="ekr.20250512061621.1"><vh>@button test</vh></v>
<v t="ekr.20250426044713.1"><vh> Recursive import script</vh></v>
</v>
<v t="ekr.20250426202645.1"><vh>--- unused</vh>
<v t="ekr.20250427052613.1"><vh>--- unused imports</vh></v>
<v t="ekr.20250427052747.1"><vh>--- unused bridge-related code</vh></v>
<v t="ekr.20250427065029.1"><vh>class CacheData</vh></v>
<v t="ekr.20250512071928.1"><vh>--- COPIES</vh>
<v t="ekr.20250426045002.1"><vh>COPY: leoAst.py</vh>
<v t="ekr.20250426045002.2"><vh>&lt;&lt; leoAst docstring &gt;&gt;</vh></v>
<v t="ekr.20250426045002.3"><vh>&lt;&lt; leoAst imports &amp; annotations &gt;&gt;</vh></v>
<v t="ekr.20250426045002.4"><vh> leoAst.py: top-level commands</vh>
<v t="ekr.20250426045002.5"><vh>command: fstringify_command</vh></v>
<v t="ekr.20250426045002.6"><vh>command: fstringify_diff_command</vh></v>
<v t="ekr.20250426045002.7"><vh>command: orange_command (leoAst.py)</vh></v>
<v t="ekr.20250426045002.8"><vh>command: orange_diff_command</vh></v>
</v>
<v t="ekr.20250426045002.9"><vh> leoAst.py: top-level utils</vh>
<v t="ekr.20250426045002.10"><vh>function: check_g</vh></v>
<v t="ekr.20250426045002.11"><vh>function: regularize_nls</vh></v>
<v t="ekr.20250426045002.12"><vh>function: write_file</vh></v>
<v t="ekr.20250426045002.13"><vh>functions: dumpers...</vh>
<v t="ekr.20250426045002.14"><vh>function: dump_ast</vh></v>
<v t="ekr.20250426045002.15"><vh>function: dump_contents</vh></v>
<v t="ekr.20250426045002.16"><vh>function: dump_lines</vh></v>
<v t="ekr.20250426045002.17"><vh>function: dump_results</vh></v>
<v t="ekr.20250426045002.18"><vh>function: dump_tokens</vh></v>
<v t="ekr.20250426045002.19"><vh>function: dump_tree</vh></v>
<v t="ekr.20250426045002.20"><vh>function: show_diffs</vh></v>
</v>
<v t="ekr.20250426045002.21"><vh>functions: tokens</vh>
<v t="ekr.20250426045002.22"><vh>function: find_anchor_token</vh></v>
<v t="ekr.20250426045002.23"><vh>function: find_paren_token</vh></v>
<v t="ekr.20250426045002.24"><vh>function: get_node_tokens_list</vh></v>
<v t="ekr.20250426045002.25"><vh>function: is_significant &amp; is_significant_token</vh></v>
<v t="ekr.20250426045002.26"><vh>function: match_parens</vh></v>
<v t="ekr.20250426045002.27"><vh>function: output_tokens_to_string</vh></v>
<v t="ekr.20250426045002.28"><vh>function: tokens_for_node</vh></v>
<v t="ekr.20250426045002.29"><vh>function: tokens_to_string</vh></v>
<v t="ekr.20250426045002.30"><vh>function: input_tokens_to_string</vh></v>
</v>
<v t="ekr.20250426045002.31"><vh>functions: utils...</vh>
<v t="ekr.20250426045002.32"><vh>function: obj_id</vh></v>
<v t="ekr.20250426045002.33"><vh>function: op_name</vh></v>
</v>
<v t="ekr.20250426045002.34"><vh>node/token creators...</vh>
<v t="ekr.20250426045002.35"><vh>function: make_tokens</vh></v>
<v t="ekr.20250426045002.36"><vh>function: parse_ast</vh></v>
</v>
<v t="ekr.20250426045002.37"><vh>node/token nodes...</vh>
<v t="ekr.20250426045002.38"><vh>function: find_statement_node</vh></v>
<v t="ekr.20250426045002.39"><vh>function: is_ancestor</vh></v>
<v t="ekr.20250426045002.40"><vh>function: is_long_statement</vh></v>
<v t="ekr.20250426045002.41"><vh>function: is_statement_node</vh></v>
<v t="ekr.20250426045002.42"><vh>function: nearest_common_ancestor</vh></v>
</v>
<v t="ekr.20250426045002.43"><vh>node/token replacers...</vh>
<v t="ekr.20250426045002.44"><vh>function: add_token_to_token_list</vh></v>
<v t="ekr.20250426045002.45"><vh>function: replace_node</vh></v>
<v t="ekr.20250426045002.46"><vh>function: replace_token</vh></v>
</v>
</v>
<v t="ekr.20250426045002.47"><vh>Exception classes</vh></v>
<v t="ekr.20250426045002.48"><vh>Classes</vh>
<v t="ekr.20250426045002.49"><vh>class AstDumper</vh>
<v t="ekr.20250426045002.50"><vh>dumper.dump_tree &amp; helper</vh>
<v t="ekr.20250426045002.51"><vh>dumper.dump_tree_and_links_helper</vh></v>
</v>
<v t="ekr.20250426045002.52"><vh>dumper.compute_node_string &amp; helpers</vh>
<v t="ekr.20250426045002.53"><vh>dumper.show_fields</vh></v>
<v t="ekr.20250426045002.54"><vh>dumper.show_line_range</vh></v>
<v t="ekr.20250426045002.55"><vh>dumper.show_tokens</vh></v>
</v>
<v t="ekr.20250426045002.56"><vh>dumper.show_header</vh></v>
<v t="ekr.20250426045002.57"><vh>dumper.dump_ast &amp; helper</vh>
<v t="ekr.20250426045002.58"><vh>dumper.get_fields</vh></v>
</v>
</v>
<v t="ekr.20250426045002.59"><vh>class Fstringify</vh>
<v t="ekr.20250426045002.60"><vh>fs.fstringify</vh></v>
<v t="ekr.20250426045002.61"><vh>fs.fstringify_file (entry)</vh></v>
<v t="ekr.20250426045002.62"><vh>fs.fstringify_file_diff (entry)</vh></v>
<v t="ekr.20250426045002.63"><vh>fs.fstringify_file_silent (entry)</vh></v>
<v t="ekr.20250426045002.64"><vh>fs.make_fstring &amp; helpers</vh>
<v t="ekr.20250426045002.65"><vh>fs.clean_ws</vh></v>
<v t="ekr.20250426045002.66"><vh>fs.compute_result &amp; helpers</vh>
<v t="ekr.20250426045002.67"><vh>fs.check_back_slashes</vh></v>
<v t="ekr.20250426045002.68"><vh>fs.change_quotes</vh></v>
</v>
<v t="ekr.20250426045002.69"><vh>fs.munge_spec</vh></v>
<v t="ekr.20250426045002.70"><vh>fs.scan_format_string</vh></v>
<v t="ekr.20250426045002.71"><vh>fs.scan_rhs</vh></v>
<v t="ekr.20250426045002.72"><vh>fs.substitute_values</vh></v>
</v>
<v t="ekr.20250426045002.73"><vh>fs.message</vh></v>
<v t="ekr.20250426045002.74"><vh>fs.replace</vh></v>
</v>
<v t="ekr.20250426045002.75"><vh>class InputToken</vh>
<v t="ekr.20250426045002.76"><vh>itoken.brief_dump</vh></v>
<v t="ekr.20250426045002.77"><vh>itoken.dump</vh></v>
<v t="ekr.20250426045002.78"><vh>itoken.dump_header</vh></v>
<v t="ekr.20250426045002.79"><vh>itoken.error_dump</vh></v>
<v t="ekr.20250426045002.80"><vh>itoken.show_val</vh></v>
</v>
<v t="ekr.20250426045002.81"><vh>class Orange</vh>
<v t="ekr.20250426045002.82"><vh>orange.ctor</vh></v>
<v t="ekr.20250426045002.83"><vh>orange.push_state</vh></v>
<v t="ekr.20250426045002.84"><vh>orange: Entries &amp; helpers</vh>
<v t="ekr.20250426045002.85"><vh>orange.beautify (main token loop)</vh></v>
<v t="ekr.20250426045002.86"><vh>orange.beautify_file (entry)</vh></v>
<v t="ekr.20250426045002.87"><vh>orange.beautify_file_diff (entry)</vh></v>
<v t="ekr.20250426045002.88"><vh>orange.init_tokens_from_file</vh></v>
<v t="ekr.20250426045002.89"><vh>orange.make_tokens</vh></v>
</v>
<v t="ekr.20250426045002.90"><vh>orange: Input token handlers</vh>
<v t="ekr.20250426045002.91"><vh>orange.do_comment</vh></v>
<v t="ekr.20250426045002.92"><vh>orange.do_encoding</vh></v>
<v t="ekr.20250426045002.93"><vh>orange.do_endmarker</vh></v>
<v t="ekr.20250426045002.94"><vh>orange.do_fstring_start &amp; continue_fstring</vh></v>
<v t="ekr.20250426045002.95"><vh>orange.do_indent &amp; do_dedent &amp; helper</vh>
<v t="ekr.20250426045002.96"><vh>orange.handle_dedent_after_class_or_def</vh></v>
</v>
<v t="ekr.20250426045002.97"><vh>orange.do_name</vh></v>
<v t="ekr.20250426045002.98"><vh>orange.do_newline &amp; do_nl</vh></v>
<v t="ekr.20250426045002.99"><vh>orange.do_number</vh></v>
<v t="ekr.20250426045002.100"><vh>orange.do_op &amp; helper</vh>
<v t="ekr.20250426045002.101"><vh>orange.do_equal_op</vh></v>
</v>
<v t="ekr.20250426045002.102"><vh>orange.do_string</vh></v>
<v t="ekr.20250426045002.103"><vh>orange.do_verbatim</vh></v>
<v t="ekr.20250426045002.104"><vh>orange.do_ws</vh></v>
</v>
<v t="ekr.20250426045002.105"><vh>orange: Output token generators</vh>
<v t="ekr.20250426045002.106"><vh>orange.add_line_end</vh></v>
<v t="ekr.20250426045002.107"><vh>orange.add_token</vh></v>
<v t="ekr.20250426045002.108"><vh>orange.blank</vh></v>
<v t="ekr.20250426045002.109"><vh>orange.blank_lines (black only)</vh></v>
<v t="ekr.20250426045002.110"><vh>orange.clean</vh></v>
<v t="ekr.20250426045002.111"><vh>orange.clean_blank_lines</vh></v>
<v t="ekr.20250426045002.112"><vh>orange.colon</vh></v>
<v t="ekr.20250426045002.113"><vh>orange.line_end</vh></v>
<v t="ekr.20250426045002.114"><vh>orange.line_indent</vh></v>
<v t="ekr.20250426045002.115"><vh>orange.lt &amp; rt</vh>
<v t="ekr.20250426045002.116"><vh>orange.lt</vh></v>
<v t="ekr.20250426045002.117"><vh>orange.rt</vh></v>
</v>
<v t="ekr.20250426045002.118"><vh>orange.possible_unary_op &amp; unary_op</vh></v>
<v t="ekr.20250426045002.119"><vh>orange.star_op</vh></v>
<v t="ekr.20250426045002.120"><vh>orange.star_star_op</vh></v>
<v t="ekr.20250426045002.121"><vh>orange.word &amp; word_op</vh></v>
</v>
<v t="ekr.20250426045002.122"><vh>orange: Split/join</vh>
<v t="ekr.20250426045002.123"><vh>orange.split_line &amp; helpers</vh>
<v t="ekr.20250426045002.124"><vh>orange.append_tail</vh></v>
<v t="ekr.20250426045002.125"><vh>orange.find_prev_line</vh></v>
<v t="ekr.20250426045002.126"><vh>orange.find_line_prefix</vh></v>
</v>
<v t="ekr.20250426045002.127"><vh>orange.join_lines</vh></v>
</v>
</v>
<v t="ekr.20250426045002.128"><vh>class OutputToken</vh>
<v t="ekr.20250426045002.129"><vh>otoken.show_val</vh></v>
</v>
<v t="ekr.20250426045002.130"><vh>class ParseState</vh></v>
<v t="ekr.20250426045002.131"><vh>class ReassignTokens</vh>
<v t="ekr.20250426045002.132"><vh>reassign.reassign</vh></v>
<v t="ekr.20250426045002.133"><vh>reassign.visit_call</vh></v>
</v>
<v t="ekr.20250426045002.134"><vh>class Token</vh>
<v t="ekr.20250426045002.135"><vh>token.brief_dump</vh></v>
<v t="ekr.20250426045002.136"><vh>token.dump</vh></v>
<v t="ekr.20250426045002.137"><vh>token.dump_header</vh></v>
<v t="ekr.20250426045002.138"><vh>token.error_dump</vh></v>
<v t="ekr.20250426045002.139"><vh>token.show_val</vh></v>
</v>
<v t="ekr.20250426045002.140"><vh>class Tokenizer</vh>
<v t="ekr.20250426045002.141"><vh>tokenizer.add_token</vh></v>
<v t="ekr.20250426045002.142"><vh>tokenizer.check_results</vh></v>
<v t="ekr.20250426045002.143"><vh>tokenizer.create_input_tokens</vh></v>
<v t="ekr.20250426045002.144"><vh>tokenizer.create_tokens</vh></v>
<v t="ekr.20250426045002.145"><vh>tokenizer.do_token (the gem)</vh></v>
</v>
<v t="ekr.20250426045002.146"><vh>class TokenOrderGenerator</vh>
<v t="ekr.20250426045002.147"><vh>tog: Init...</vh>
<v t="ekr.20250426045002.148"><vh>tog.balance_tokens</vh></v>
<v t="ekr.20250426045002.149"><vh>tog.create_links (inits all ivars)</vh></v>
<v t="ekr.20250426045002.150"><vh>tog.init_from_file</vh></v>
<v t="ekr.20250426045002.151"><vh>tog.init_from_string</vh></v>
<v t="ekr.20250426045002.152"><vh>tog.make_tokens</vh></v>
</v>
<v t="ekr.20250426045002.153"><vh>tog: synchronizer...</vh>
<v t="ekr.20250426045002.154"><vh>tog.find_next_significant_token</vh></v>
<v t="ekr.20250426045002.155"><vh>tog.set_links</vh></v>
<v t="ekr.20250426045002.156"><vh>tog.name</vh></v>
<v t="ekr.20250426045002.157"><vh>tog.op</vh></v>
<v t="ekr.20250426045002.158"><vh>tog.token</vh></v>
<v t="ekr.20250426045002.159"><vh>tog.string_helper &amp; helpers</vh>
<v t="ekr.20250426045002.160"><vh>tog.sync_to_kind</vh></v>
<v t="ekr.20250426045002.161"><vh>tog.find_next_non_ws_token</vh></v>
</v>
</v>
<v t="ekr.20250426045002.162"><vh>tog: Traversal...</vh>
<v t="ekr.20250426045002.163"><vh>tog.enter_node</vh></v>
<v t="ekr.20250426045002.164"><vh>tog.leave_node</vh></v>
<v t="ekr.20250426045002.165"><vh>tog.visit</vh></v>
</v>
<v t="ekr.20250426045002.166"><vh>tog: Visitors...</vh>
<v t="ekr.20250426045002.167"><vh> tog.keyword: not called!</vh></v>
<v t="ekr.20250426045002.168"><vh>tog: Contexts</vh>
<v t="ekr.20250426045002.169"><vh> tog.arg</vh></v>
<v t="ekr.20250426045002.170"><vh> tog.arguments</vh></v>
<v t="ekr.20250426045002.171"><vh>tog.AsyncFunctionDef</vh></v>
<v t="ekr.20250426045002.172"><vh>tog.ClassDef</vh></v>
<v t="ekr.20250426045002.173"><vh>tog.FunctionDef</vh></v>
<v t="ekr.20250426045002.174"><vh>tog.Interactive</vh></v>
<v t="ekr.20250426045002.175"><vh>tog.Lambda</vh></v>
<v t="ekr.20250426045002.176"><vh>tog.Module</vh></v>
</v>
<v t="ekr.20250426045002.177"><vh>tog: Expressions</vh>
<v t="ekr.20250426045002.178"><vh>tog.Expr</vh></v>
<v t="ekr.20250426045002.179"><vh>tog.Expression</vh></v>
<v t="ekr.20250426045002.180"><vh>tog.GeneratorExp</vh></v>
<v t="ekr.20250426045002.181"><vh>tog.NamedExpr</vh></v>
</v>
<v t="ekr.20250426045002.182"><vh>tog: Operands</vh>
<v t="ekr.20250426045002.183"><vh>tog.Attribute</vh></v>
<v t="ekr.20250426045002.184"><vh>tog.Bytes</vh></v>
<v t="ekr.20250426045002.185"><vh>tog.comprehension</vh></v>
<v t="ekr.20250426045002.186"><vh>tog.Constant</vh></v>
<v t="ekr.20250426045002.187"><vh>tog.Dict</vh></v>
<v t="ekr.20250426045002.188"><vh>tog.DictComp</vh></v>
<v t="ekr.20250426045002.189"><vh>tog.Ellipsis</vh></v>
<v t="ekr.20250426045002.190"><vh>tog.ExtSlice</vh></v>
<v t="ekr.20250426045002.191"><vh>tog.FormattedValue</vh></v>
<v t="ekr.20250426045002.192"><vh>tog.Index</vh></v>
<v t="ekr.20250426045002.193"><vh>tog.JoinedStr</vh></v>
<v t="ekr.20250426045002.194"><vh>tog.List</vh></v>
<v t="ekr.20250426045002.195"><vh>tog.ListComp</vh></v>
<v t="ekr.20250426045002.196"><vh>tog.Name</vh></v>
<v t="ekr.20250426045002.197"><vh>tog.Set</vh></v>
<v t="ekr.20250426045002.198"><vh>tog.SetComp</vh></v>
<v t="ekr.20250426045002.199"><vh>tog.Slice</vh></v>
<v t="ekr.20250426045002.200"><vh>tog.Str (deprecated)</vh></v>
<v t="ekr.20250426045002.201"><vh>tog.Subscript</vh></v>
<v t="ekr.20250426045002.202"><vh>tog.Tuple</vh></v>
</v>
<v t="ekr.20250426045002.203"><vh>tog: Operators</vh>
<v t="ekr.20250426045002.204"><vh>tog.BinOp</vh></v>
<v t="ekr.20250426045002.205"><vh>tog.BoolOp</vh></v>
<v t="ekr.20250426045002.206"><vh>tog.Compare</vh></v>
<v t="ekr.20250426045002.207"><vh>tog.UnaryOp</vh></v>
<v t="ekr.20250426045002.208"><vh>tog.IfExp (ternary operator)</vh></v>
</v>
<v t="ekr.20250426045002.209"><vh>tog: Statements</vh>
<v t="ekr.20250426045002.210"><vh> tog.Starred</vh></v>
<v t="ekr.20250426045002.211"><vh>tog.AnnAssign</vh></v>
<v t="ekr.20250426045002.212"><vh>tog.Assert</vh></v>
<v t="ekr.20250426045002.213"><vh>tog.Assign</vh></v>
<v t="ekr.20250426045002.214"><vh>tog.AsyncFor</vh></v>
<v t="ekr.20250426045002.215"><vh>tog.AsyncWith</vh></v>
<v t="ekr.20250426045002.216"><vh>tog.AugAssign</vh></v>
<v t="ekr.20250426045002.217"><vh>tog.Await</vh></v>
<v t="ekr.20250426045002.218"><vh>tog.Break</vh></v>
<v t="ekr.20250426045002.219"><vh>tog.Call &amp; helpers</vh>
<v t="ekr.20250426045002.220"><vh>tog.arg_helper</vh></v>
<v t="ekr.20250426045002.221"><vh>tog.handle_call_arguments</vh></v>
</v>
<v t="ekr.20250426045002.222"><vh>tog.Continue</vh></v>
<v t="ekr.20250426045002.223"><vh>tog.Delete</vh></v>
<v t="ekr.20250426045002.224"><vh>tog.ExceptHandler</vh></v>
<v t="ekr.20250426045002.225"><vh>tog.For</vh></v>
<v t="ekr.20250426045002.226"><vh>tog.Global</vh></v>
<v t="ekr.20250426045002.227"><vh>tog.If &amp; helpers</vh>
<v t="ekr.20250426045002.228"><vh>&lt;&lt; do_If docstring &gt;&gt;</vh></v>
</v>
<v t="ekr.20250426045002.229"><vh>tog.Import &amp; helper</vh></v>
<v t="ekr.20250426045002.230"><vh>tog.ImportFrom</vh></v>
<v t="ekr.20250426045002.231"><vh>tog.Match* (Python 3.10+)</vh>
<v t="ekr.20250426045002.232"><vh>tog.match_case</vh></v>
<v t="ekr.20250426045002.233"><vh>tog.MatchAs</vh></v>
<v t="ekr.20250426045002.234"><vh>tog.MatchClass</vh></v>
<v t="ekr.20250426045002.235"><vh>tog.MatchMapping</vh></v>
<v t="ekr.20250426045002.236"><vh>tog.MatchOr</vh></v>
<v t="ekr.20250426045002.237"><vh>tog.MatchSequence</vh></v>
<v t="ekr.20250426045002.238"><vh>tog.MatchSingleton</vh></v>
<v t="ekr.20250426045002.239"><vh>tog.MatchStar</vh></v>
<v t="ekr.20250426045002.240"><vh>tog.MatchValue</vh></v>
</v>
<v t="ekr.20250426045002.241"><vh>tog.Nonlocal</vh></v>
<v t="ekr.20250426045002.242"><vh>tog.Pass</vh></v>
<v t="ekr.20250426045002.243"><vh>tog.Raise</vh></v>
<v t="ekr.20250426045002.244"><vh>tog.Return</vh></v>
<v t="ekr.20250426045002.245"><vh>tog.Try</vh></v>
<v t="ekr.20250426045002.246"><vh>tog.TryStar</vh></v>
<v t="ekr.20250426045002.247"><vh>tog.While</vh></v>
<v t="ekr.20250426045002.248"><vh>tog.With</vh></v>
<v t="ekr.20250426045002.249"><vh>tog.Yield</vh></v>
<v t="ekr.20250426045002.250"><vh>tog.YieldFrom</vh></v>
</v>
<v t="ekr.20250426045002.251"><vh>tog: Types</vh>
<v t="ekr.20250426045002.252"><vh>tog.ParamSpec</vh></v>
<v t="ekr.20250426045002.253"><vh>tog.TypeAlias</vh></v>
<v t="ekr.20250426045002.254"><vh>tog.TypeVar</vh></v>
<v t="ekr.20250426045002.255"><vh>tog.TypeVarTuple</vh></v>
</v>
</v>
</v>
</v>
<v t="ekr.20250426045002.256"><vh>function: main (leoAst.py) &amp; helper</vh>
<v t="ekr.20250426045002.257"><vh>function: scan_ast_args</vh></v>
</v>
</v>
<v t="ekr.20250426045416.1"><vh>COPY: leoTokens.py</vh>
<v t="ekr.20250426045416.2"><vh>&lt;&lt; leoTokens.py: docstring &gt;&gt;</vh></v>
<v t="ekr.20250426045416.3"><vh>&lt;&lt; leoTokens.py: imports &amp; annotations &gt;&gt;</vh></v>
<v t="ekr.20250426045416.4"><vh>top-level functions (leoTokens.py)</vh>
<v t="ekr.20250426045416.5"><vh>function: dump_contents</vh></v>
<v t="ekr.20250426045416.6"><vh>function: dump_lines</vh></v>
<v t="ekr.20250426045416.7"><vh>function: dump_results</vh></v>
<v t="ekr.20250426045416.8"><vh>function: dump_tokens</vh></v>
<v t="ekr.20250426045416.9"><vh>function: input_tokens_to_string</vh></v>
<v t="ekr.20250426045416.10"><vh>function: beautify_file (leoTokens.py)</vh></v>
<v t="ekr.20250426045416.11"><vh>function: main (leoTokens.py)</vh></v>
<v t="ekr.20250426045416.12"><vh>function: orange_command (leoTokens.py)</vh></v>
<v t="ekr.20250426045416.13"><vh>function: scan_args (leoTokens.py)</vh></v>
</v>
<v t="ekr.20250426045416.14"><vh>Classes</vh>
<v t="ekr.20250426045416.15"><vh>class InternalBeautifierError(Exception)</vh></v>
<v t="ekr.20250426045416.16"><vh>class InputToken</vh>
<v t="ekr.20250426045416.17"><vh>itoken.brief_dump</vh></v>
<v t="ekr.20250426045416.18"><vh>itoken.dump</vh></v>
<v t="ekr.20250426045416.19"><vh>itoken.dump_header</vh></v>
<v t="ekr.20250426045416.20"><vh>itoken.error_dump</vh></v>
<v t="ekr.20250426045416.21"><vh>itoken.show_val</vh></v>
</v>
<v t="ekr.20250426045416.22"><vh>class Tokenizer</vh>
<v t="ekr.20250426045416.23"><vh>Tokenizer.add_token</vh></v>
<v t="ekr.20250426045416.24"><vh>Tokenizer.check_results</vh></v>
<v t="ekr.20250426045416.25"><vh>Tokenizer.check_round_trip</vh></v>
<v t="ekr.20250426045416.26"><vh>Tokenizer.create_input_tokens</vh></v>
<v t="ekr.20250426045416.27"><vh>Tokenizer.do_token (the gem)</vh></v>
<v t="ekr.20250426045416.28"><vh>Tokenizer.make_input_tokens (entry)</vh></v>
<v t="ekr.20250426045416.29"><vh>Tokenizer.tokens_to_string</vh></v>
</v>
<v t="ekr.20250426045416.30"><vh>class ParseState</vh></v>
<v t="ekr.20250426045416.31"><vh>class ScanState</vh></v>
<v t="ekr.20250426045416.32"><vh>class TokenBasedOrange</vh>
<v t="ekr.20250426045416.33"><vh>&lt;&lt; TokenBasedOrange: docstring &gt;&gt;</vh></v>
<v t="ekr.20250426045416.34"><vh>&lt;&lt; TokenBasedOrange: __slots__ &gt;&gt;</vh></v>
<v t="ekr.20250426045416.35"><vh>&lt;&lt; TokenBasedOrange: python-related constants &gt;&gt;</vh></v>
<v t="ekr.20250426045416.36"><vh>tbo.ctor</vh></v>
<v t="ekr.20250426045416.37"><vh>tbo: Checking &amp; dumping</vh>
<v t="ekr.20250426045416.38"><vh>tbo.dump_token_range</vh></v>
<v t="ekr.20250426045416.39"><vh>tbo.internal_error_message</vh></v>
<v t="ekr.20250426045416.40"><vh>tbo.user_error_message</vh></v>
<v t="ekr.20250426045416.41"><vh>tbo.oops</vh></v>
</v>
<v t="ekr.20250426045416.42"><vh>tbo: Entries &amp; helpers</vh>
<v t="ekr.20250426045416.43"><vh>tbo.beautify (main token loop)</vh>
<v t="ekr.20250426045416.44"><vh>&lt;&lt; tbo.beautify: init ivars &gt;&gt;</vh></v>
</v>
<v t="ekr.20250426045416.45"><vh>tbo.beautify_file (entry) (stats &amp; diffs)</vh></v>
<v t="ekr.20250426045416.46"><vh>tbo.init_tokens_from_file</vh></v>
<v t="ekr.20250426045416.47"><vh>tbo.regularize_newlines</vh></v>
<v t="ekr.20250426045416.48"><vh>tbo.write_file</vh></v>
<v t="ekr.20250426045416.49"><vh>tbo.show_diffs</vh></v>
</v>
<v t="ekr.20250426045416.50"><vh>tbo: Visitors &amp; generators</vh>
<v t="ekr.20250426045416.51"><vh>tbo.do_comment</vh>
<v t="ekr.20250426045416.52"><vh>&lt;&lt; do_comment: update comment-related state &gt;&gt;</vh></v>
</v>
<v t="ekr.20250426045416.53"><vh>tbo.do_dedent</vh></v>
<v t="ekr.20250426045416.54"><vh>tbo.do_encoding</vh></v>
<v t="ekr.20250426045416.55"><vh>tbo.do_endmarker</vh></v>
<v t="ekr.20250426045416.56"><vh>tbo.do_indent</vh></v>
<v t="ekr.20250426045416.57"><vh>tbo.do_name &amp; generators</vh>
<v t="ekr.20250426045416.58"><vh>tbo.do_name</vh></v>
<v t="ekr.20250426045416.59"><vh>tbo.gen_word</vh></v>
<v t="ekr.20250426045416.60"><vh>tbo.gen_word_op</vh></v>
</v>
<v t="ekr.20250426045416.61"><vh>tbo.do_newline, do_nl &amp; generators</vh>
<v t="ekr.20250426045416.62"><vh>tbo.do_newline</vh></v>
<v t="ekr.20250426045416.63"><vh>tbo.do_nl</vh></v>
</v>
<v t="ekr.20250426045416.64"><vh>tbo.do_number</vh></v>
<v t="ekr.20250426045416.65"><vh>tbo.do_op &amp; generators</vh>
<v t="ekr.20250426045416.66"><vh>tbo.do_op</vh></v>
<v t="ekr.20250426045416.67"><vh>tbo.gen_colon &amp; helper</vh></v>
<v t="ekr.20250426045416.68"><vh>tbo.gen_dot_op &amp; _next</vh>
<v t="ekr.20250426045416.69"><vh>tbo._next</vh></v>
</v>
<v t="ekr.20250426045416.70"><vh>tbo.gen_equal_op</vh></v>
<v t="ekr.20250426045416.71"><vh>tbo.gen_lt</vh></v>
<v t="ekr.20250426045416.72"><vh>tbo.gen_possible_unary_op &amp; helper</vh>
<v t="ekr.20250426045416.73"><vh>tbo.is_unary_op &amp; _prev</vh>
<v t="ekr.20250426045416.74"><vh>tbo._prev</vh></v>
</v>
</v>
<v t="ekr.20250426045416.75"><vh>tbo.gen_rt</vh></v>
<v t="ekr.20250426045416.76"><vh>tbo.gen_star_op</vh></v>
<v t="ekr.20250426045416.77"><vh>tbo.gen_star_star_op</vh></v>
<v t="ekr.20250426045416.78"><vh>tbo.push_state</vh></v>
</v>
<v t="ekr.20250426045416.79"><vh>tbo.do_string</vh></v>
<v t="ekr.20250426045416.80"><vh>tbo.do_verbatim</vh></v>
<v t="ekr.20250426045416.81"><vh>tbo.do_ws</vh></v>
<v t="ekr.20250426045416.82"><vh>tbo.gen_blank</vh></v>
<v t="ekr.20250426045416.83"><vh>tbo.gen_token</vh></v>
</v>
<v t="ekr.20250426045416.84"><vh>tbo: Scanning</vh>
<v t="ekr.20250426045416.85"><vh>tbo.pre_scan &amp; helpers</vh>
<v t="ekr.20250426045416.86"><vh>&lt;&lt; pre-scan 'newline' tokens &gt;&gt;</vh></v>
<v t="ekr.20250426045416.87"><vh>&lt;&lt; pre-scan 'op' tokens &gt;&gt;</vh></v>
<v t="ekr.20250426045416.88"><vh>&lt;&lt; pre-scan 'name' tokens &gt;&gt;</vh></v>
<v t="ekr.20250426045416.89"><vh>tbo.finish_arg</vh></v>
<v t="ekr.20250426045416.90"><vh>tbo.finish_slice</vh></v>
<v t="ekr.20250426045416.91"><vh>tbo.finish_dict</vh></v>
</v>
<v t="ekr.20250426045416.92"><vh>tbo.is_unary_op_with_prev</vh></v>
<v t="ekr.20250426045416.93"><vh>tbo.is_python_keyword</vh></v>
<v t="ekr.20250426045416.94"><vh>tbo.set_context</vh></v>
</v>
</v>
</v>
</v>
<v t="ekr.20250430053636.2"><vh>COPY: pyflakes</vh>
<v t="ekr.20250430053636.5"><vh>@@file api.py</vh>
<v t="ekr.20250430053636.6"><vh>function: check</vh></v>
<v t="ekr.20250430053636.7"><vh>function: checkPath</vh></v>
<v t="ekr.20250430053636.8"><vh>function: isPythonFile</vh></v>
<v t="ekr.20250430053636.9"><vh>function: iterSourceCode</vh></v>
<v t="ekr.20250430053636.10"><vh>function: checkRecursive</vh></v>
<v t="ekr.20250430053636.11"><vh>function: _exitOnSignal</vh></v>
<v t="ekr.20250430053636.12"><vh>function: _get_version</vh></v>
<v t="ekr.20250430053636.13"><vh>function: main</vh></v>
</v>
<v t="ekr.20250430053636.14"><vh>@@file checker.py</vh>
<v t="ekr.20250430053636.15"><vh>function: getAlternatives</vh></v>
<v t="ekr.20250430053636.16"><vh>function: _is_singleton</vh></v>
<v t="ekr.20250430053636.17"><vh>function: _is_tuple_constant</vh></v>
<v t="ekr.20250430053636.18"><vh>function: _is_constant</vh></v>
<v t="ekr.20250430053636.19"><vh>function: _is_const_non_singleton</vh></v>
<v t="ekr.20250430053636.20"><vh>function: _is_name_or_attr</vh></v>
<v t="ekr.20250430053636.21"><vh>function: _must_match</vh></v>
<v t="ekr.20250430053636.22"><vh>function: parse_percent_format</vh></v>
<v t="ekr.20250430053636.23"><vh>class _FieldsOrder</vh>
<v t="ekr.20250430053636.62"><vh>_FieldsOrder._get_fields</vh></v>
<v t="ekr.20250430053636.63"><vh>_FieldsOrder.__missing__</vh></v>
</v>
<v t="ekr.20250430053636.24"><vh>function: counter</vh></v>
<v t="ekr.20250430053636.25"><vh>function: iter_child_nodes</vh></v>
<v t="ekr.20250430053636.26"><vh>function: convert_to_value</vh></v>
<v t="ekr.20250430053636.27"><vh>function: is_notimplemented_name_node</vh></v>
<v t="ekr.20250430053636.28"><vh>class Binding</vh>
<v t="ekr.20250430053636.64"><vh>Binding.__init__</vh></v>
<v t="ekr.20250430053636.65"><vh>Binding.__str__</vh></v>
<v t="ekr.20250430053636.66"><vh>Binding.__repr__</vh></v>
<v t="ekr.20250430053636.67"><vh>Binding.redefines</vh></v>
</v>
<v t="ekr.20250430053636.29"><vh>class Definition</vh>
<v t="ekr.20250430053636.68"><vh>Definition.redefines</vh></v>
</v>
<v t="ekr.20250430053636.30"><vh>class Builtin</vh>
<v t="ekr.20250430053636.69"><vh>Builtin.__init__</vh></v>
<v t="ekr.20250430053636.70"><vh>Builtin.__repr__</vh></v>
</v>
<v t="ekr.20250430053636.31"><vh>class UnhandledKeyType</vh></v>
<v t="ekr.20250430053636.32"><vh>class VariableKey</vh>
<v t="ekr.20250430053636.71"><vh>VariableKey.__init__</vh></v>
<v t="ekr.20250430053636.72"><vh>VariableKey.__eq__</vh></v>
<v t="ekr.20250430053636.73"><vh>VariableKey.__hash__</vh></v>
</v>
<v t="ekr.20250430053636.33"><vh>class Importation</vh>
<v t="ekr.20250430053636.74"><vh>Importation.__init__</vh></v>
<v t="ekr.20250430053636.75"><vh>Importation.redefines</vh></v>
<v t="ekr.20250430053636.76"><vh>Importation._has_alias</vh></v>
<v t="ekr.20250430053636.77"><vh>Importation.source_statement</vh></v>
<v t="ekr.20250430053636.78"><vh>Importation.__str__</vh></v>
</v>
<v t="ekr.20250430053636.34"><vh>class SubmoduleImportation</vh>
<v t="ekr.20250430053636.79"><vh>SubmoduleImportation.__init__</vh></v>
<v t="ekr.20250430053636.80"><vh>SubmoduleImportation.redefines</vh></v>
<v t="ekr.20250430053636.81"><vh>SubmoduleImportation.__str__</vh></v>
<v t="ekr.20250430053636.82"><vh>SubmoduleImportation.source_statement</vh></v>
</v>
<v t="ekr.20250430053636.35"><vh>class ImportationFrom</vh>
<v t="ekr.20250430053636.83"><vh>ImportationFrom.__init__</vh></v>
<v t="ekr.20250430053636.84"><vh>ImportationFrom.__str__</vh></v>
<v t="ekr.20250430053636.85"><vh>ImportationFrom.source_statement</vh></v>
</v>
<v t="ekr.20250430053636.36"><vh>class StarImportation</vh>
<v t="ekr.20250430053636.86"><vh>StarImportation.__init__</vh></v>
<v t="ekr.20250430053636.87"><vh>StarImportation.source_statement</vh></v>
<v t="ekr.20250430053636.88"><vh>StarImportation.__str__</vh></v>
</v>
<v t="ekr.20250430053636.37"><vh>class FutureImportation</vh>
<v t="ekr.20250430053636.89"><vh>FutureImportation.__init__</vh></v>
</v>
<v t="ekr.20250430053636.38"><vh>class Argument</vh></v>
<v t="ekr.20250430053636.39"><vh>class Assignment</vh></v>
<v t="ekr.20250430053636.40"><vh>class NamedExprAssignment</vh></v>
<v t="ekr.20250430053636.41"><vh>class Annotation</vh>
<v t="ekr.20250430053636.90"><vh>Annotation.redefines</vh></v>
</v>
<v t="ekr.20250430053636.42"><vh>class FunctionDefinition</vh></v>
<v t="ekr.20250430053636.43"><vh>class ClassDefinition</vh></v>
<v t="ekr.20250430053636.44"><vh>class ExportBinding</vh>
<v t="ekr.20250430053636.91"><vh>ExportBinding.__init__</vh></v>
</v>
<v t="ekr.20250430053636.45"><vh>class Scope</vh>
<v t="ekr.20250430053636.92"><vh>Scope.__repr__</vh></v>
</v>
<v t="ekr.20250430053636.46"><vh>class ClassScope</vh></v>
<v t="ekr.20250430053636.47"><vh>class FunctionScope</vh>
<v t="ekr.20250430053636.93"><vh>FunctionScope.__init__</vh></v>
<v t="ekr.20250430053636.94"><vh>FunctionScope.unused_assignments</vh></v>
<v t="ekr.20250430053636.95"><vh>FunctionScope.unused_annotations</vh></v>
</v>
<v t="ekr.20250430053636.48"><vh>class TypeScope</vh></v>
<v t="ekr.20250430053636.49"><vh>class GeneratorScope</vh></v>
<v t="ekr.20250430053636.50"><vh>class ModuleScope</vh></v>
<v t="ekr.20250430053636.51"><vh>class DoctestScope</vh></v>
<v t="ekr.20250430053636.52"><vh>class DetectClassScopedMagic</vh></v>
<v t="ekr.20250430053636.53"><vh>function: getNodeName</vh></v>
<v t="ekr.20250430053636.54"><vh>function: _is_typing_helper</vh></v>
<v t="ekr.20250430053636.55"><vh>function: _is_typing</vh></v>
<v t="ekr.20250430053636.56"><vh>function: _is_any_typing_member</vh></v>
<v t="ekr.20250430053636.57"><vh>function: is_typing_overload</vh></v>
<v t="ekr.20250430053636.58"><vh>class AnnotationState</vh></v>
<v t="ekr.20250430053636.59"><vh>function: in_annotation</vh></v>
<v t="ekr.20250430053636.60"><vh>function: in_string_annotation</vh></v>
<v t="ekr.20250430053636.61"><vh>class Checker</vh>
<v t="ekr.20250430053636.96"><vh>Checker.__init__</vh></v>
<v t="ekr.20250430053636.97"><vh>Checker.deferFunction</vh></v>
<v t="ekr.20250430053636.98"><vh>Checker._run_deferred</vh></v>
<v t="ekr.20250430053636.99"><vh>Checker._in_doctest</vh></v>
<v t="ekr.20250430053636.100"><vh>Checker.futuresAllowed</vh></v>
<v t="ekr.20250430053636.101"><vh>Checker.futuresAllowed</vh></v>
<v t="ekr.20250430053636.102"><vh>Checker.annotationsFutureEnabled</vh></v>
<v t="ekr.20250430053636.103"><vh>Checker.annotationsFutureEnabled</vh></v>
<v t="ekr.20250430053636.104"><vh>Checker.scope</vh></v>
<v t="ekr.20250430053636.105"><vh>Checker.in_scope</vh></v>
<v t="ekr.20250430053636.106"><vh>Checker.checkDeadScopes</vh></v>
<v t="ekr.20250430053636.107"><vh>Checker.report</vh></v>
<v t="ekr.20250430053636.108"><vh>Checker.getParent</vh></v>
<v t="ekr.20250430053636.109"><vh>Checker.getCommonAncestor</vh></v>
<v t="ekr.20250430053636.110"><vh>Checker.descendantOf</vh></v>
<v t="ekr.20250430053636.111"><vh>Checker._getAncestor</vh></v>
<v t="ekr.20250430053636.112"><vh>Checker.getScopeNode</vh></v>
<v t="ekr.20250430053636.113"><vh>Checker.differentForks</vh></v>
<v t="ekr.20250430053636.114"><vh>Checker.addBinding</vh></v>
<v t="ekr.20250430053636.115"><vh>Checker._unknown_handler</vh></v>
<v t="ekr.20250430053636.116"><vh>Checker.getNodeHandler</vh></v>
<v t="ekr.20250430053636.117"><vh>Checker.handleNodeLoad</vh></v>
<v t="ekr.20250430053636.118"><vh>Checker.handleNodeStore</vh></v>
<v t="ekr.20250430053636.119"><vh>Checker.handleNodeDelete</vh></v>
<v t="ekr.20250430053636.120"><vh>Checker._enter_annotation</vh></v>
<v t="ekr.20250430053636.121"><vh>Checker._in_postponed_annotation</vh></v>
<v t="ekr.20250430053636.122"><vh>Checker.handleChildren</vh></v>
<v t="ekr.20250430053636.123"><vh>Checker.isLiteralTupleUnpacking</vh></v>
<v t="ekr.20250430053636.124"><vh>Checker.isDocstring</vh></v>
<v t="ekr.20250430053636.125"><vh>Checker.getDocstring</vh></v>
<v t="ekr.20250430053636.126"><vh>Checker.handleNode</vh></v>
<v t="ekr.20250430053636.127"><vh>Checker.handleDoctests</vh></v>
<v t="ekr.20250430053636.128"><vh>Checker.handleStringAnnotation</vh></v>
<v t="ekr.20250430053636.129"><vh>Checker.handle_annotation_always_deferred</vh></v>
<v t="ekr.20250430053636.130"><vh>Checker.handleAnnotation</vh></v>
<v t="ekr.20250430053636.131"><vh>Checker.ignore</vh></v>
<v t="ekr.20250430053636.132"><vh>Checker.SUBSCRIPT</vh></v>
<v t="ekr.20250430053636.133"><vh>Checker._handle_string_dot_format</vh></v>
<v t="ekr.20250430053636.134"><vh>Checker.CALL</vh></v>
<v t="ekr.20250430053636.135"><vh>Checker._handle_percent_format</vh></v>
<v t="ekr.20250430053636.136"><vh>Checker.BINOP</vh></v>
<v t="ekr.20250430053636.137"><vh>Checker.CONSTANT</vh></v>
<v t="ekr.20250430053636.138"><vh>Checker.RAISE</vh></v>
<v t="ekr.20250430053636.139"><vh>Checker.JOINEDSTR</vh></v>
<v t="ekr.20250430053636.140"><vh>Checker.DICT</vh></v>
<v t="ekr.20250430053636.141"><vh>Checker.IF</vh></v>
<v t="ekr.20250430053636.142"><vh>Checker.ASSERT</vh></v>
<v t="ekr.20250430053636.143"><vh>Checker.GLOBAL</vh></v>
<v t="ekr.20250430053636.144"><vh>Checker.GENERATOREXP</vh></v>
<v t="ekr.20250430053636.145"><vh>Checker.NAME</vh></v>
<v t="ekr.20250430053636.146"><vh>Checker.CONTINUE</vh></v>
<v t="ekr.20250430053636.147"><vh>Checker.RETURN</vh></v>
<v t="ekr.20250430053636.148"><vh>Checker.YIELD</vh></v>
<v t="ekr.20250430053636.149"><vh>Checker.FUNCTIONDEF</vh></v>
<v t="ekr.20250430053636.150"><vh>Checker.LAMBDA</vh></v>
<v t="ekr.20250430053636.151"><vh>Checker.ARGUMENTS</vh></v>
<v t="ekr.20250430053636.152"><vh>Checker.ARG</vh></v>
<v t="ekr.20250430053636.153"><vh>Checker.CLASSDEF</vh></v>
<v t="ekr.20250430053636.154"><vh>Checker.AUGASSIGN</vh></v>
<v t="ekr.20250430053636.155"><vh>Checker.TUPLE</vh></v>
<v t="ekr.20250430053636.156"><vh>Checker.IMPORT</vh></v>
<v t="ekr.20250430053636.157"><vh>Checker.IMPORTFROM</vh></v>
<v t="ekr.20250430053636.158"><vh>Checker.TRY</vh></v>
<v t="ekr.20250430053636.159"><vh>Checker.EXCEPTHANDLER</vh></v>
<v t="ekr.20250430053636.160"><vh>Checker.ANNASSIGN</vh></v>
<v t="ekr.20250430053636.161"><vh>Checker.COMPARE</vh></v>
<v t="ekr.20250430053636.162"><vh>Checker._match_target</vh></v>
<v t="ekr.20250430053636.163"><vh>Checker._type_param_scope</vh></v>
<v t="ekr.20250430053636.164"><vh>Checker.TYPEVAR</vh></v>
<v t="ekr.20250430053636.165"><vh>Checker.TYPEALIAS</vh></v>
</v>
</v>
<v t="ekr.20250430053636.166"><vh>@@file messages.py</vh>
<v t="ekr.20250430053636.167"><vh>class Message</vh></v>
<v t="ekr.20250430061458.1"><vh>Messages</vh>
<v t="ekr.20250430053636.168"><vh>class UnusedImport(Message)</vh></v>
<v t="ekr.20250430053636.169"><vh>class RedefinedWhileUnused(Message)</vh></v>
<v t="ekr.20250430053636.170"><vh>class ImportShadowedByLoopVar(Message)</vh>
<v t="ekr.20250430053636.217"><vh>ImportShadowedByLoopVar.__init__</vh></v>
</v>
<v t="ekr.20250430053636.171"><vh>class ImportStarNotPermitted(Message)</vh>
<v t="ekr.20250430053636.218"><vh>ImportStarNotPermitted.__init__</vh></v>
</v>
<v t="ekr.20250430053636.172"><vh>class ImportStarUsed(Message)</vh>
<v t="ekr.20250430053636.219"><vh>ImportStarUsed.__init__</vh></v>
</v>
<v t="ekr.20250430053636.173"><vh>class ImportStarUsage(Message)</vh>
<v t="ekr.20250430053636.220"><vh>ImportStarUsage.__init__</vh></v>
</v>
<v t="ekr.20250430053636.174"><vh>class UndefinedName(Message)</vh>
<v t="ekr.20250430053636.221"><vh>UndefinedName.__init__</vh></v>
</v>
<v t="ekr.20250430053636.175"><vh>class DoctestSyntaxError(Message)</vh>
<v t="ekr.20250430053636.222"><vh>DoctestSyntaxError.__init__</vh></v>
</v>
<v t="ekr.20250430053636.176"><vh>class UndefinedExport(Message)</vh>
<v t="ekr.20250430053636.223"><vh>UndefinedExport.__init__</vh></v>
</v>
<v t="ekr.20250430053636.177"><vh>class UndefinedLocal(Message)</vh>
<v t="ekr.20250430053636.224"><vh>UndefinedLocal.__init__</vh></v>
</v>
<v t="ekr.20250430053636.178"><vh>class DuplicateArgument(Message)</vh>
<v t="ekr.20250430053636.225"><vh>DuplicateArgument.__init__</vh></v>
</v>
<v t="ekr.20250430053636.179"><vh>class MultiValueRepeatedKeyLiteral(Message)</vh>
<v t="ekr.20250430053636.226"><vh>MultiValueRepeatedKeyLiteral.__init__</vh></v>
</v>
<v t="ekr.20250430053636.180"><vh>class MultiValueRepeatedKeyVariable(Message)</vh>
<v t="ekr.20250430053636.227"><vh>MultiValueRepeatedKeyVariable.__init__</vh></v>
</v>
<v t="ekr.20250430053636.181"><vh>class LateFutureImport(Message)</vh></v>
<v t="ekr.20250430053636.182"><vh>class FutureFeatureNotDefined(Message)</vh>
<v t="ekr.20250430053636.228"><vh>FutureFeatureNotDefined.__init__</vh></v>
</v>
<v t="ekr.20250430053636.183"><vh>class UnusedVariable(Message)</vh>
<v t="ekr.20250430053636.229"><vh>UnusedVariable.__init__</vh></v>
</v>
<v t="ekr.20250430053636.184"><vh>class UnusedAnnotation(Message)</vh>
<v t="ekr.20250430053636.230"><vh>UnusedAnnotation.__init__</vh></v>
</v>
<v t="ekr.20250430053636.185"><vh>class ReturnOutsideFunction(Message)</vh></v>
<v t="ekr.20250430053636.186"><vh>class YieldOutsideFunction(Message)</vh></v>
<v t="ekr.20250430053636.187"><vh>class ContinueOutsideLoop(Message)</vh></v>
<v t="ekr.20250430053636.188"><vh>class BreakOutsideLoop(Message)</vh></v>
<v t="ekr.20250430053636.189"><vh>class DefaultExceptNotLast(Message)</vh></v>
<v t="ekr.20250430053636.190"><vh>class TwoStarredExpressions(Message)</vh></v>
<v t="ekr.20250430053636.191"><vh>class TooManyExpressionsInStarredAssignment(Message)</vh></v>
<v t="ekr.20250430053636.192"><vh>class IfTuple(Message)</vh></v>
<v t="ekr.20250430053636.193"><vh>class AssertTuple(Message)</vh></v>
<v t="ekr.20250430053636.194"><vh>class ForwardAnnotationSyntaxError(Message)</vh>
<v t="ekr.20250430053636.231"><vh>ForwardAnnotationSyntaxError.__init__</vh></v>
</v>
<v t="ekr.20250430053636.195"><vh>class RaiseNotImplemented(Message)</vh></v>
<v t="ekr.20250430053636.196"><vh>class InvalidPrintSyntax(Message)</vh></v>
<v t="ekr.20250430053636.197"><vh>class IsLiteral(Message)</vh></v>
<v t="ekr.20250430053636.198"><vh>class FStringMissingPlaceholders(Message)</vh></v>
<v t="ekr.20250430053636.199"><vh>class StringDotFormatExtraPositionalArguments(Message)</vh>
<v t="ekr.20250430053636.232"><vh>StringDotFormatExtraPositionalArguments.__init__</vh></v>
</v>
<v t="ekr.20250430053636.200"><vh>class StringDotFormatExtraNamedArguments(Message)</vh>
<v t="ekr.20250430053636.233"><vh>StringDotFormatExtraNamedArguments.__init__</vh></v>
</v>
<v t="ekr.20250430053636.201"><vh>class StringDotFormatMissingArgument(Message)</vh>
<v t="ekr.20250430053636.234"><vh>StringDotFormatMissingArgument.__init__</vh></v>
</v>
<v t="ekr.20250430053636.202"><vh>class StringDotFormatMixingAutomatic(Message)</vh></v>
<v t="ekr.20250430053636.203"><vh>class StringDotFormatInvalidFormat(Message)</vh>
<v t="ekr.20250430053636.235"><vh>StringDotFormatInvalidFormat.__init__</vh></v>
</v>
<v t="ekr.20250430053636.204"><vh>class PercentFormatInvalidFormat(Message)</vh>
<v t="ekr.20250430053636.236"><vh>PercentFormatInvalidFormat.__init__</vh></v>
</v>
<v t="ekr.20250430053636.205"><vh>class PercentFormatMixedPositionalAndNamed(Message)</vh></v>
<v t="ekr.20250430053636.206"><vh>class PercentFormatUnsupportedFormatCharacter(Message)</vh>
<v t="ekr.20250430053636.237"><vh>PercentFormatUnsupportedFormatCharacter.__init__</vh></v>
</v>
<v t="ekr.20250430053636.207"><vh>class PercentFormatPositionalCountMismatch(Message)</vh>
<v t="ekr.20250430053636.238"><vh>PercentFormatPositionalCountMismatch.__init__</vh></v>
</v>
<v t="ekr.20250430053636.208"><vh>class PercentFormatExtraNamedArguments(Message)</vh>
<v t="ekr.20250430053636.239"><vh>PercentFormatExtraNamedArguments.__init__</vh></v>
</v>
<v t="ekr.20250430053636.209"><vh>class PercentFormatMissingArgument(Message)</vh>
<v t="ekr.20250430053636.240"><vh>PercentFormatMissingArgument.__init__</vh></v>
</v>
<v t="ekr.20250430053636.210"><vh>class PercentFormatExpectedMapping(Message)</vh></v>
<v t="ekr.20250430053636.211"><vh>class PercentFormatExpectedSequence(Message)</vh></v>
<v t="ekr.20250430053636.212"><vh>class PercentFormatStarRequiresSequence(Message)</vh></v>
</v>
</v>
<v t="ekr.20250430053636.241"><vh>@@file reporter.py</vh>
<v t="ekr.20250430053636.242"><vh>class Reporter</vh>
<v t="ekr.20250430053636.244"><vh>Reporter.__init__</vh></v>
<v t="ekr.20250430053636.245"><vh>Reporter.unexpectedError</vh></v>
<v t="ekr.20250430053636.246"><vh>Reporter.syntaxError</vh></v>
<v t="ekr.20250430053636.247"><vh>Reporter.flake</vh></v>
</v>
<v t="ekr.20250430053636.243"><vh>function: _makeDefaultReporter</vh></v>
</v>
<v t="ekr.20250430053636.250"><vh>@@file scripts/pyflakes.py</vh></v>
<v t="ekr.20250430053636.251"><vh>test</vh>
<v t="ekr.20250430053636.252"><vh>@@file test/__init__.py</vh></v>
<v t="ekr.20250430053636.253"><vh>@@file test/harness.py</vh>
<v t="ekr.20250430053636.254"><vh>class TestCase</vh></v>
</v>
<v t="ekr.20250430053637.1"><vh>@@file test/test_api.py</vh>
<v t="ekr.20250430053637.2"><vh>function: withStderrTo</vh></v>
<v t="ekr.20250430053637.3"><vh>class Node</vh>
<v t="ekr.20250430053637.11"><vh>Node.__init__</vh></v>
</v>
<v t="ekr.20250430053637.4"><vh>class SysStreamCapturing</vh>
<v t="ekr.20250430053637.12"><vh>SysStreamCapturing.__init__</vh></v>
<v t="ekr.20250430053637.13"><vh>SysStreamCapturing.__enter__</vh></v>
<v t="ekr.20250430053637.14"><vh>SysStreamCapturing.__exit__</vh></v>
</v>
<v t="ekr.20250430053637.5"><vh>class LoggingReporter</vh>
<v t="ekr.20250430053637.15"><vh>LoggingReporter.__init__</vh></v>
<v t="ekr.20250430053637.16"><vh>LoggingReporter.flake</vh></v>
<v t="ekr.20250430053637.17"><vh>LoggingReporter.unexpectedError</vh></v>
<v t="ekr.20250430053637.18"><vh>LoggingReporter.syntaxError</vh></v>
</v>
<v t="ekr.20250430053637.6"><vh>class TestIterSourceCode</vh>
<v t="ekr.20250430053637.19"><vh>TestIterSourceCode.setUp</vh></v>
<v t="ekr.20250430053637.20"><vh>TestIterSourceCode.tearDown</vh></v>
<v t="ekr.20250430053637.21"><vh>TestIterSourceCode.makeEmptyFile</vh></v>
<v t="ekr.20250430053637.22"><vh>TestIterSourceCode.test_emptyDirectory</vh></v>
<v t="ekr.20250430053637.23"><vh>TestIterSourceCode.test_singleFile</vh></v>
<v t="ekr.20250430053637.24"><vh>TestIterSourceCode.test_onlyPythonSource</vh></v>
<v t="ekr.20250430053637.25"><vh>TestIterSourceCode.test_recurses</vh></v>
<v t="ekr.20250430053637.26"><vh>TestIterSourceCode.test_shebang</vh></v>
<v t="ekr.20250430053637.27"><vh>TestIterSourceCode.test_multipleDirectories</vh></v>
<v t="ekr.20250430053637.28"><vh>TestIterSourceCode.test_explicitFiles</vh></v>
</v>
<v t="ekr.20250430053637.7"><vh>class TestReporter</vh>
<v t="ekr.20250430053637.29"><vh>TestReporter.test_syntaxError</vh></v>
<v t="ekr.20250430053637.30"><vh>TestReporter.test_syntaxErrorNoOffset</vh></v>
<v t="ekr.20250430053637.31"><vh>TestReporter.test_syntaxErrorNoText</vh></v>
<v t="ekr.20250430053637.32"><vh>TestReporter.test_multiLineSyntaxError</vh></v>
<v t="ekr.20250430053637.33"><vh>TestReporter.test_unexpectedError</vh></v>
<v t="ekr.20250430053637.34"><vh>TestReporter.test_flake</vh></v>
</v>
<v t="ekr.20250430053637.8"><vh>class CheckTests</vh>
<v t="ekr.20250430053637.35"><vh>CheckTests.makeTempFile</vh></v>
<v t="ekr.20250430053637.36"><vh>CheckTests.assertHasErrors</vh></v>
<v t="ekr.20250430053637.37"><vh>CheckTests.getErrors</vh></v>
<v t="ekr.20250430053637.38"><vh>CheckTests.test_legacyScript</vh></v>
<v t="ekr.20250430053637.39"><vh>CheckTests.test_missingTrailingNewline</vh></v>
<v t="ekr.20250430053637.40"><vh>CheckTests.test_checkPathNonExisting</vh></v>
<v t="ekr.20250430053637.41"><vh>CheckTests.test_multilineSyntaxError</vh></v>
<v t="ekr.20250430053637.42"><vh>CheckTests.test_eofSyntaxError</vh></v>
<v t="ekr.20250430053637.43"><vh>CheckTests.test_eofSyntaxErrorWithTab</vh></v>
<v t="ekr.20250430053637.44"><vh>CheckTests.test_nonDefaultFollowsDefaultSyntaxError</vh></v>
<v t="ekr.20250430053637.45"><vh>CheckTests.test_nonKeywordAfterKeywordSyntaxError</vh></v>
<v t="ekr.20250430053637.46"><vh>CheckTests.test_invalidEscape</vh></v>
<v t="ekr.20250430053637.47"><vh>CheckTests.test_permissionDenied</vh></v>
<v t="ekr.20250430053637.48"><vh>CheckTests.test_pyflakesWarning</vh></v>
<v t="ekr.20250430053637.49"><vh>CheckTests.test_encodedFileUTF8</vh></v>
<v t="ekr.20250430053637.50"><vh>CheckTests.test_CRLFLineEndings</vh></v>
<v t="ekr.20250430053637.51"><vh>CheckTests.test_misencodedFileUTF8</vh></v>
<v t="ekr.20250430053637.52"><vh>CheckTests.test_misencodedFileUTF16</vh></v>
<v t="ekr.20250430053637.53"><vh>CheckTests.test_checkRecursive</vh></v>
<v t="ekr.20250430053637.54"><vh>CheckTests.test_stdinReportsErrors</vh></v>
</v>
<v t="ekr.20250430053637.9"><vh>class IntegrationTests</vh>
<v t="ekr.20250430053637.55"><vh>IntegrationTests.setUp</vh></v>
<v t="ekr.20250430053637.56"><vh>IntegrationTests.tearDown</vh></v>
<v t="ekr.20250430053637.57"><vh>IntegrationTests.getPyflakesBinary</vh></v>
<v t="ekr.20250430053637.58"><vh>IntegrationTests.runPyflakes</vh></v>
<v t="ekr.20250430053637.59"><vh>IntegrationTests.test_goodFile</vh></v>
<v t="ekr.20250430053637.60"><vh>IntegrationTests.test_fileWithFlakes</vh></v>
<v t="ekr.20250430053637.61"><vh>IntegrationTests.test_errors_io</vh></v>
<v t="ekr.20250430053637.62"><vh>IntegrationTests.test_errors_syntax</vh></v>
<v t="ekr.20250430053637.63"><vh>IntegrationTests.test_readFromStdin</vh></v>
</v>
<v t="ekr.20250430053637.10"><vh>class TestMain</vh>
<v t="ekr.20250430053637.64"><vh>TestMain.runPyflakes</vh></v>
</v>
</v>
<v t="ekr.20250430053637.65"><vh>@@file test/test_builtin.py</vh>
<v t="ekr.20250430053637.66"><vh>class TestBuiltins</vh>
<v t="ekr.20250430053637.67"><vh>TestBuiltins.test_builtin_unbound_local</vh></v>
<v t="ekr.20250430053637.68"><vh>TestBuiltins.test_global_shadowing_builtin</vh></v>
</v>
</v>
<v t="ekr.20250430053637.69"><vh>@@file test/test_code_segment.py</vh>
<v t="ekr.20250430053637.70"><vh>class TestCodeSegments</vh>
<v t="ekr.20250430053637.71"><vh>TestCodeSegments.test_function_segment</vh></v>
<v t="ekr.20250430053637.72"><vh>TestCodeSegments.test_class_segment</vh></v>
<v t="ekr.20250430053637.73"><vh>TestCodeSegments.test_scope_class</vh></v>
<v t="ekr.20250430053637.74"><vh>TestCodeSegments.test_scope_function</vh></v>
<v t="ekr.20250430053637.75"><vh>TestCodeSegments.test_scope_async_function</vh></v>
</v>
</v>
<v t="ekr.20250430053637.76"><vh>@@file test/test_dict.py</vh>
<v t="ekr.20250430053637.77"><vh>class Test</vh>
<v t="ekr.20250430053637.78"><vh>Test.test_duplicate_keys</vh></v>
<v t="ekr.20250430053637.79"><vh>Test.test_duplicate_keys_bytes_vs_unicode_py3</vh></v>
<v t="ekr.20250430053637.80"><vh>Test.test_duplicate_values_bytes_vs_unicode_py3</vh></v>
<v t="ekr.20250430053637.81"><vh>Test.test_multiple_duplicate_keys</vh></v>
<v t="ekr.20250430053637.82"><vh>Test.test_duplicate_keys_in_function</vh></v>
<v t="ekr.20250430053637.83"><vh>Test.test_duplicate_keys_in_lambda</vh></v>
<v t="ekr.20250430053637.84"><vh>Test.test_duplicate_keys_tuples</vh></v>
<v t="ekr.20250430053637.85"><vh>Test.test_duplicate_keys_tuples_int_and_float</vh></v>
<v t="ekr.20250430053637.86"><vh>Test.test_duplicate_keys_ints</vh></v>
<v t="ekr.20250430053637.87"><vh>Test.test_duplicate_keys_bools</vh></v>
<v t="ekr.20250430053637.88"><vh>Test.test_duplicate_keys_bools_false</vh></v>
<v t="ekr.20250430053637.89"><vh>Test.test_duplicate_keys_none</vh></v>
<v t="ekr.20250430053637.90"><vh>Test.test_duplicate_variable_keys</vh></v>
<v t="ekr.20250430053637.91"><vh>Test.test_duplicate_variable_values</vh></v>
<v t="ekr.20250430053637.92"><vh>Test.test_duplicate_variable_values_same_value</vh></v>
<v t="ekr.20250430053637.93"><vh>Test.test_duplicate_key_float_and_int</vh></v>
<v t="ekr.20250430053637.94"><vh>Test.test_no_duplicate_key_error_same_value</vh></v>
<v t="ekr.20250430053637.95"><vh>Test.test_no_duplicate_key_errors</vh></v>
<v t="ekr.20250430053637.96"><vh>Test.test_no_duplicate_keys_tuples_same_first_element</vh></v>
<v t="ekr.20250430053637.97"><vh>Test.test_no_duplicate_key_errors_func_call</vh></v>
<v t="ekr.20250430053637.98"><vh>Test.test_no_duplicate_key_errors_bool_or_none</vh></v>
<v t="ekr.20250430053637.99"><vh>Test.test_no_duplicate_key_errors_ints</vh></v>
<v t="ekr.20250430053637.100"><vh>Test.test_no_duplicate_key_errors_vars</vh></v>
<v t="ekr.20250430053637.101"><vh>Test.test_no_duplicate_key_errors_tuples</vh></v>
<v t="ekr.20250430053637.102"><vh>Test.test_no_duplicate_key_errors_instance_attributes</vh></v>
</v>
</v>
<v t="ekr.20250430053637.103"><vh>@@file test/test_doctests.py</vh>
<v t="ekr.20250430053637.104"><vh>class _DoctestMixin</vh>
<v t="ekr.20250430053637.109"><vh>_DoctestMixin.doctestify</vh></v>
<v t="ekr.20250430053637.110"><vh>_DoctestMixin.flakes</vh></v>
</v>
<v t="ekr.20250430053637.105"><vh>class Test</vh>
<v t="ekr.20250430053637.111"><vh>Test.test_scope_class</vh></v>
<v t="ekr.20250430053637.112"><vh>Test.test_nested_doctest_ignored</vh></v>
<v t="ekr.20250430053637.113"><vh>Test.test_global_module_scope_pollution</vh></v>
<v t="ekr.20250430053637.114"><vh>Test.test_global_undefined</vh></v>
<v t="ekr.20250430053637.115"><vh>Test.test_nested_class</vh></v>
<v t="ekr.20250430053637.116"><vh>Test.test_ignore_nested_function</vh></v>
<v t="ekr.20250430053637.117"><vh>Test.test_inaccessible_scope_class</vh></v>
<v t="ekr.20250430053637.118"><vh>Test.test_importBeforeDoctest</vh></v>
<v t="ekr.20250430053637.119"><vh>Test.test_importBeforeAndInDoctest</vh></v>
<v t="ekr.20250430053637.120"><vh>Test.test_importInDoctestAndAfter</vh></v>
<v t="ekr.20250430053637.121"><vh>Test.test_offsetInDoctests</vh></v>
<v t="ekr.20250430053637.122"><vh>Test.test_offsetInLambdasInDoctests</vh></v>
<v t="ekr.20250430053637.123"><vh>Test.test_offsetAfterDoctests</vh></v>
<v t="ekr.20250430053637.124"><vh>Test.test_syntaxErrorInDoctest</vh></v>
<v t="ekr.20250430053637.125"><vh>Test.test_indentationErrorInDoctest</vh></v>
<v t="ekr.20250430053637.126"><vh>Test.test_offsetWithMultiLineArgs</vh></v>
<v t="ekr.20250430053637.127"><vh>Test.test_doctestCanReferToFunction</vh></v>
<v t="ekr.20250430053637.128"><vh>Test.test_doctestCanReferToClass</vh></v>
<v t="ekr.20250430053637.129"><vh>Test.test_noOffsetSyntaxErrorInDoctest</vh></v>
<v t="ekr.20250430053637.130"><vh>Test.test_singleUnderscoreInDoctest</vh></v>
<v t="ekr.20250430053637.131"><vh>Test.test_globalUnderscoreInDoctest</vh></v>
</v>
<v t="ekr.20250430053637.106"><vh>class TestOther</vh></v>
<v t="ekr.20250430053637.107"><vh>class TestImports</vh></v>
<v t="ekr.20250430053637.108"><vh>class TestUndefinedNames</vh></v>
</v>
<v t="ekr.20250430053637.132"><vh>@@file test/test_imports.py</vh>
<v t="ekr.20250430053637.133"><vh>class TestImportationObject</vh>
<v t="ekr.20250430053637.136"><vh>TestImportationObject.test_import_basic</vh></v>
<v t="ekr.20250430053637.137"><vh>TestImportationObject.test_import_as</vh></v>
<v t="ekr.20250430053637.138"><vh>TestImportationObject.test_import_submodule</vh></v>
<v t="ekr.20250430053637.139"><vh>TestImportationObject.test_import_submodule_as</vh></v>
<v t="ekr.20250430053637.140"><vh>TestImportationObject.test_import_submodule_as_source_name</vh></v>
<v t="ekr.20250430053637.141"><vh>TestImportationObject.test_importfrom_relative</vh></v>
<v t="ekr.20250430053637.142"><vh>TestImportationObject.test_importfrom_relative_parent</vh></v>
<v t="ekr.20250430053637.143"><vh>TestImportationObject.test_importfrom_relative_with_module</vh></v>
<v t="ekr.20250430053637.144"><vh>TestImportationObject.test_importfrom_relative_with_module_as</vh></v>
<v t="ekr.20250430053637.145"><vh>TestImportationObject.test_importfrom_member</vh></v>
<v t="ekr.20250430053637.146"><vh>TestImportationObject.test_importfrom_submodule_member</vh></v>
<v t="ekr.20250430053637.147"><vh>TestImportationObject.test_importfrom_member_as</vh></v>
<v t="ekr.20250430053637.148"><vh>TestImportationObject.test_importfrom_submodule_member_as</vh></v>
<v t="ekr.20250430053637.149"><vh>TestImportationObject.test_importfrom_star</vh></v>
<v t="ekr.20250430053637.150"><vh>TestImportationObject.test_importfrom_star_relative</vh></v>
<v t="ekr.20250430053637.151"><vh>TestImportationObject.test_importfrom_future</vh></v>
<v t="ekr.20250430053637.152"><vh>TestImportationObject.test_unusedImport_underscore</vh></v>
</v>
<v t="ekr.20250430053637.134"><vh>class Test</vh>
<v t="ekr.20250430053637.153"><vh>Test.test_unusedImport</vh></v>
<v t="ekr.20250430053637.154"><vh>Test.test_unusedImport_relative</vh></v>
<v t="ekr.20250430053637.155"><vh>Test.test_aliasedImport</vh></v>
<v t="ekr.20250430053637.156"><vh>Test.test_aliasedImportShadowModule</vh></v>
<v t="ekr.20250430053637.157"><vh>Test.test_usedImport</vh></v>
<v t="ekr.20250430053637.158"><vh>Test.test_usedImport_relative</vh></v>
<v t="ekr.20250430053637.159"><vh>Test.test_redefinedWhileUnused</vh></v>
<v t="ekr.20250430053637.160"><vh>Test.test_redefinedIf</vh></v>
<v t="ekr.20250430053637.161"><vh>Test.test_redefinedIfElse</vh></v>
<v t="ekr.20250430053637.162"><vh>Test.test_redefinedTry</vh></v>
<v t="ekr.20250430053637.163"><vh>Test.test_redefinedTryExcept</vh></v>
<v t="ekr.20250430053637.164"><vh>Test.test_redefinedTryNested</vh></v>
<v t="ekr.20250430053637.165"><vh>Test.test_redefinedTryExceptMulti</vh></v>
<v t="ekr.20250430053637.166"><vh>Test.test_redefinedTryElse</vh></v>
<v t="ekr.20250430053637.167"><vh>Test.test_redefinedTryExceptElse</vh></v>
<v t="ekr.20250430053637.168"><vh>Test.test_redefinedTryExceptFinally</vh></v>
<v t="ekr.20250430053637.169"><vh>Test.test_redefinedTryExceptElseFinally</vh></v>
<v t="ekr.20250430053637.170"><vh>Test.test_redefinedByFunction</vh></v>
<v t="ekr.20250430053637.171"><vh>Test.test_redefinedInNestedFunction</vh></v>
<v t="ekr.20250430053637.172"><vh>Test.test_redefinedInNestedFunctionTwice</vh></v>
<v t="ekr.20250430053637.173"><vh>Test.test_redefinedButUsedLater</vh></v>
<v t="ekr.20250430053637.174"><vh>Test.test_redefinedByClass</vh></v>
<v t="ekr.20250430053637.175"><vh>Test.test_redefinedBySubclass</vh></v>
<v t="ekr.20250430053637.176"><vh>Test.test_redefinedInClass</vh></v>
<v t="ekr.20250430053637.177"><vh>Test.test_importInClass</vh></v>
<v t="ekr.20250430053637.178"><vh>Test.test_usedInFunction</vh></v>
<v t="ekr.20250430053637.179"><vh>Test.test_shadowedByParameter</vh></v>
<v t="ekr.20250430053637.180"><vh>Test.test_newAssignment</vh></v>
<v t="ekr.20250430053637.181"><vh>Test.test_usedInGetattr</vh></v>
<v t="ekr.20250430053637.182"><vh>Test.test_usedInSlice</vh></v>
<v t="ekr.20250430053637.183"><vh>Test.test_usedInIfBody</vh></v>
<v t="ekr.20250430053637.184"><vh>Test.test_usedInIfConditional</vh></v>
<v t="ekr.20250430053637.185"><vh>Test.test_usedInElifConditional</vh></v>
<v t="ekr.20250430053637.186"><vh>Test.test_usedInElse</vh></v>
<v t="ekr.20250430053637.187"><vh>Test.test_usedInCall</vh></v>
<v t="ekr.20250430053637.188"><vh>Test.test_usedInClass</vh></v>
<v t="ekr.20250430053637.189"><vh>Test.test_usedInClassBase</vh></v>
<v t="ekr.20250430053637.190"><vh>Test.test_notUsedInNestedScope</vh></v>
<v t="ekr.20250430053637.191"><vh>Test.test_usedInFor</vh></v>
<v t="ekr.20250430053637.192"><vh>Test.test_usedInForElse</vh></v>
<v t="ekr.20250430053637.193"><vh>Test.test_redefinedByFor</vh></v>
<v t="ekr.20250430053637.194"><vh>Test.test_shadowedByFor</vh></v>
<v t="ekr.20250430053637.195"><vh>Test.test_shadowedByForDeep</vh></v>
<v t="ekr.20250430053637.196"><vh>Test.test_usedInReturn</vh></v>
<v t="ekr.20250430053637.197"><vh>Test.test_usedInOperators</vh></v>
<v t="ekr.20250430053637.198"><vh>Test.test_usedInAssert</vh></v>
<v t="ekr.20250430053637.199"><vh>Test.test_usedInSubscript</vh></v>
<v t="ekr.20250430053637.200"><vh>Test.test_usedInLogic</vh></v>
<v t="ekr.20250430053637.201"><vh>Test.test_usedInList</vh></v>
<v t="ekr.20250430053637.202"><vh>Test.test_usedInTuple</vh></v>
<v t="ekr.20250430053637.203"><vh>Test.test_usedInTry</vh></v>
<v t="ekr.20250430053637.204"><vh>Test.test_usedInExcept</vh></v>
<v t="ekr.20250430053637.205"><vh>Test.test_redefinedByExcept</vh></v>
<v t="ekr.20250430053637.206"><vh>Test.test_usedInRaise</vh></v>
<v t="ekr.20250430053637.207"><vh>Test.test_usedInYield</vh></v>
<v t="ekr.20250430053637.208"><vh>Test.test_usedInDict</vh></v>
<v t="ekr.20250430053637.209"><vh>Test.test_usedInParameterDefault</vh></v>
<v t="ekr.20250430053637.210"><vh>Test.test_usedInAttributeAssign</vh></v>
<v t="ekr.20250430053637.211"><vh>Test.test_usedInKeywordArg</vh></v>
<v t="ekr.20250430053637.212"><vh>Test.test_usedInAssignment</vh></v>
<v t="ekr.20250430053637.213"><vh>Test.test_usedInListComp</vh></v>
<v t="ekr.20250430053637.214"><vh>Test.test_usedInTryFinally</vh></v>
<v t="ekr.20250430053637.215"><vh>Test.test_usedInWhile</vh></v>
<v t="ekr.20250430053637.216"><vh>Test.test_usedInGlobal</vh></v>
<v t="ekr.20250430053637.217"><vh>Test.test_usedAndGlobal</vh></v>
<v t="ekr.20250430053637.218"><vh>Test.test_assignedToGlobal</vh></v>
<v t="ekr.20250430053637.219"><vh>Test.test_usedInExec</vh></v>
<v t="ekr.20250430053637.220"><vh>Test.test_usedInLambda</vh></v>
<v t="ekr.20250430053637.221"><vh>Test.test_shadowedByLambda</vh></v>
<v t="ekr.20250430053637.222"><vh>Test.test_usedInSliceObj</vh></v>
<v t="ekr.20250430053637.223"><vh>Test.test_unusedInNestedScope</vh></v>
<v t="ekr.20250430053637.224"><vh>Test.test_methodsDontUseClassScope</vh></v>
<v t="ekr.20250430053637.225"><vh>Test.test_nestedFunctionsNestScope</vh></v>
<v t="ekr.20250430053637.226"><vh>Test.test_nestedClassAndFunctionScope</vh></v>
<v t="ekr.20250430053637.227"><vh>Test.test_importStar</vh></v>
<v t="ekr.20250430053637.228"><vh>Test.test_importStar_relative</vh></v>
<v t="ekr.20250430053637.229"><vh>Test.test_localImportStar</vh></v>
<v t="ekr.20250430053637.230"><vh>Test.test_packageImport</vh></v>
<v t="ekr.20250430053637.231"><vh>Test.test_unusedPackageImport</vh></v>
<v t="ekr.20250430053637.232"><vh>Test.test_duplicateSubmoduleImport</vh></v>
<v t="ekr.20250430053637.233"><vh>Test.test_differentSubmoduleImport</vh></v>
<v t="ekr.20250430053637.234"><vh>Test.test_used_package_with_submodule_import</vh></v>
<v t="ekr.20250430053637.235"><vh>Test.test_used_package_with_submodule_import_of_alias</vh></v>
<v t="ekr.20250430053637.236"><vh>Test.test_unused_package_with_submodule_import</vh></v>
<v t="ekr.20250430053637.237"><vh>Test.test_assignRHSFirst</vh></v>
<v t="ekr.20250430053637.238"><vh>Test.test_tryingMultipleImports</vh></v>
<v t="ekr.20250430053637.239"><vh>Test.test_nonGlobalDoesNotRedefine</vh></v>
<v t="ekr.20250430053637.240"><vh>Test.test_functionsRunLater</vh></v>
<v t="ekr.20250430053637.241"><vh>Test.test_functionNamesAreBoundNow</vh></v>
<v t="ekr.20250430053637.242"><vh>Test.test_ignoreNonImportRedefinitions</vh></v>
<v t="ekr.20250430053637.243"><vh>Test.test_importingForImportError</vh></v>
<v t="ekr.20250430053637.244"><vh>Test.test_importedInClass</vh></v>
<v t="ekr.20250430053637.245"><vh>Test.test_importUsedInMethodDefinition</vh></v>
<v t="ekr.20250430053637.246"><vh>Test.test_futureImport</vh></v>
<v t="ekr.20250430053637.247"><vh>Test.test_futureImportFirst</vh></v>
<v t="ekr.20250430053637.248"><vh>Test.test_futureImportUsed</vh></v>
<v t="ekr.20250430053637.249"><vh>Test.test_futureImportUndefined</vh></v>
<v t="ekr.20250430053637.250"><vh>Test.test_futureImportStar</vh></v>
</v>
<v t="ekr.20250430053637.135"><vh>class TestSpecialAll</vh>
<v t="ekr.20250430053637.251"><vh>TestSpecialAll.test_ignoredInFunction</vh></v>
<v t="ekr.20250430053637.252"><vh>TestSpecialAll.test_ignoredInClass</vh></v>
<v t="ekr.20250430053637.253"><vh>TestSpecialAll.test_ignored_when_not_directly_assigned</vh></v>
<v t="ekr.20250430053637.254"><vh>TestSpecialAll.test_warningSuppressed</vh></v>
<v t="ekr.20250430053637.255"><vh>TestSpecialAll.test_augmentedAssignment</vh></v>
<v t="ekr.20250430053637.256"><vh>TestSpecialAll.test_list_concatenation_assignment</vh></v>
<v t="ekr.20250430053637.257"><vh>TestSpecialAll.test_tuple_concatenation_assignment</vh></v>
<v t="ekr.20250430053637.258"><vh>TestSpecialAll.test_all_with_attributes</vh></v>
<v t="ekr.20250430053637.259"><vh>TestSpecialAll.test_all_with_names</vh></v>
<v t="ekr.20250430053637.260"><vh>TestSpecialAll.test_all_with_attributes_added</vh></v>
<v t="ekr.20250430053637.261"><vh>TestSpecialAll.test_all_mixed_attributes_and_strings</vh></v>
<v t="ekr.20250430053637.262"><vh>TestSpecialAll.test_unboundExported</vh></v>
<v t="ekr.20250430053637.263"><vh>TestSpecialAll.test_importStarExported</vh></v>
<v t="ekr.20250430053637.264"><vh>TestSpecialAll.test_importStarNotExported</vh></v>
<v t="ekr.20250430053637.265"><vh>TestSpecialAll.test_usedInGenExp</vh></v>
<v t="ekr.20250430053637.266"><vh>TestSpecialAll.test_redefinedByGenExp</vh></v>
<v t="ekr.20250430053637.267"><vh>TestSpecialAll.test_usedAsDecorator</vh></v>
<v t="ekr.20250430053637.268"><vh>TestSpecialAll.test_usedAsClassDecorator</vh></v>
</v>
</v>
<v t="ekr.20250430053637.269"><vh>@@file test/test_is_literal.py</vh>
<v t="ekr.20250430053637.270"><vh>class Test</vh>
<v t="ekr.20250430053637.271"><vh>Test.test_is_str</vh></v>
<v t="ekr.20250430053637.272"><vh>Test.test_is_bytes</vh></v>
<v t="ekr.20250430053637.273"><vh>Test.test_is_unicode</vh></v>
<v t="ekr.20250430053637.274"><vh>Test.test_is_int</vh></v>
<v t="ekr.20250430053637.275"><vh>Test.test_is_true</vh></v>
<v t="ekr.20250430053637.276"><vh>Test.test_is_false</vh></v>
<v t="ekr.20250430053637.277"><vh>Test.test_is_not_str</vh></v>
<v t="ekr.20250430053637.278"><vh>Test.test_is_not_bytes</vh></v>
<v t="ekr.20250430053637.279"><vh>Test.test_is_not_unicode</vh></v>
<v t="ekr.20250430053637.280"><vh>Test.test_is_not_int</vh></v>
<v t="ekr.20250430053637.281"><vh>Test.test_is_not_true</vh></v>
<v t="ekr.20250430053637.282"><vh>Test.test_is_not_false</vh></v>
<v t="ekr.20250430053637.283"><vh>Test.test_left_is_str</vh></v>
<v t="ekr.20250430053637.284"><vh>Test.test_left_is_bytes</vh></v>
<v t="ekr.20250430053637.285"><vh>Test.test_left_is_unicode</vh></v>
<v t="ekr.20250430053637.286"><vh>Test.test_left_is_int</vh></v>
<v t="ekr.20250430053637.287"><vh>Test.test_left_is_true</vh></v>
<v t="ekr.20250430053637.288"><vh>Test.test_left_is_false</vh></v>
<v t="ekr.20250430053637.289"><vh>Test.test_left_is_not_str</vh></v>
<v t="ekr.20250430053637.290"><vh>Test.test_left_is_not_bytes</vh></v>
<v t="ekr.20250430053637.291"><vh>Test.test_left_is_not_unicode</vh></v>
<v t="ekr.20250430053637.292"><vh>Test.test_left_is_not_int</vh></v>
<v t="ekr.20250430053637.293"><vh>Test.test_left_is_not_true</vh></v>
<v t="ekr.20250430053637.294"><vh>Test.test_left_is_not_false</vh></v>
<v t="ekr.20250430053637.295"><vh>Test.test_chained_operators_is_true</vh></v>
<v t="ekr.20250430053637.296"><vh>Test.test_chained_operators_is_str</vh></v>
<v t="ekr.20250430053637.297"><vh>Test.test_chained_operators_is_true_end</vh></v>
<v t="ekr.20250430053637.298"><vh>Test.test_chained_operators_is_str_end</vh></v>
<v t="ekr.20250430053637.299"><vh>Test.test_is_tuple_constant</vh></v>
<v t="ekr.20250430053637.300"><vh>Test.test_is_tuple_constant_containing_constants</vh></v>
<v t="ekr.20250430053637.301"><vh>Test.test_is_tuple_containing_variables_ok</vh></v>
</v>
</v>
<v t="ekr.20250430053637.302"><vh>@@file test/test_match.py</vh>
<v t="ekr.20250430053637.303"><vh>class TestMatch</vh>
<v t="ekr.20250430053637.304"><vh>TestMatch.test_match_bindings</vh></v>
<v t="ekr.20250430053637.305"><vh>TestMatch.test_match_pattern_matched_class</vh></v>
<v t="ekr.20250430053637.306"><vh>TestMatch.test_match_placeholder</vh></v>
<v t="ekr.20250430053637.307"><vh>TestMatch.test_match_singleton</vh></v>
<v t="ekr.20250430053637.308"><vh>TestMatch.test_match_or_pattern</vh></v>
<v t="ekr.20250430053637.309"><vh>TestMatch.test_match_star</vh></v>
<v t="ekr.20250430053637.310"><vh>TestMatch.test_match_double_star</vh></v>
<v t="ekr.20250430053637.311"><vh>TestMatch.test_defined_in_different_branches</vh></v>
</v>
</v>
<v t="ekr.20250430053637.312"><vh>@@file test/test_other.py</vh>
<v t="ekr.20250430053637.313"><vh>class Test</vh>
<v t="ekr.20250430053637.318"><vh>Test.test_duplicateArgs</vh></v>
<v t="ekr.20250430053637.319"><vh>Test.test_localReferencedBeforeAssignment</vh></v>
<v t="ekr.20250430053637.320"><vh>Test.test_redefinedInGenerator</vh></v>
<v t="ekr.20250430053637.321"><vh>Test.test_redefinedInSetComprehension</vh></v>
<v t="ekr.20250430053637.322"><vh>Test.test_redefinedInDictComprehension</vh></v>
<v t="ekr.20250430053637.323"><vh>Test.test_redefinedFunction</vh></v>
<v t="ekr.20250430053637.324"><vh>Test.test_redefined_function_shadows_variable</vh></v>
<v t="ekr.20250430053637.325"><vh>Test.test_redefinedUnderscoreFunction</vh></v>
<v t="ekr.20250430053637.326"><vh>Test.test_redefinedUnderscoreImportation</vh></v>
<v t="ekr.20250430053637.327"><vh>Test.test_redefinedClassFunction</vh></v>
<v t="ekr.20250430053637.328"><vh>Test.test_redefinedIfElseFunction</vh></v>
<v t="ekr.20250430053637.329"><vh>Test.test_redefinedIfFunction</vh></v>
<v t="ekr.20250430053637.330"><vh>Test.test_redefinedTryExceptFunction</vh></v>
<v t="ekr.20250430053637.331"><vh>Test.test_redefinedTryFunction</vh></v>
<v t="ekr.20250430053637.332"><vh>Test.test_redefinedIfElseInListComp</vh></v>
<v t="ekr.20250430053637.333"><vh>Test.test_functionDecorator</vh></v>
<v t="ekr.20250430053637.334"><vh>Test.test_classFunctionDecorator</vh></v>
<v t="ekr.20250430053637.335"><vh>Test.test_modernProperty</vh></v>
<v t="ekr.20250430053637.336"><vh>Test.test_unaryPlus</vh></v>
<v t="ekr.20250430053637.337"><vh>Test.test_undefinedBaseClass</vh></v>
<v t="ekr.20250430053637.338"><vh>Test.test_classNameUndefinedInClassBody</vh></v>
<v t="ekr.20250430053637.339"><vh>Test.test_classNameDefinedPreviously</vh></v>
<v t="ekr.20250430053637.340"><vh>Test.test_classRedefinition</vh></v>
<v t="ekr.20250430053637.341"><vh>Test.test_functionRedefinedAsClass</vh></v>
<v t="ekr.20250430053637.342"><vh>Test.test_classRedefinedAsFunction</vh></v>
<v t="ekr.20250430053637.343"><vh>Test.test_classWithReturn</vh></v>
<v t="ekr.20250430053637.344"><vh>Test.test_moduleWithReturn</vh></v>
<v t="ekr.20250430053637.345"><vh>Test.test_classWithYield</vh></v>
<v t="ekr.20250430053637.346"><vh>Test.test_moduleWithYield</vh></v>
<v t="ekr.20250430053637.347"><vh>Test.test_classWithYieldFrom</vh></v>
<v t="ekr.20250430053637.348"><vh>Test.test_moduleWithYieldFrom</vh></v>
<v t="ekr.20250430053637.349"><vh>Test.test_continueOutsideLoop</vh></v>
<v t="ekr.20250430053637.350"><vh>Test.test_continueInsideLoop</vh></v>
<v t="ekr.20250430053637.351"><vh>Test.test_breakOutsideLoop</vh></v>
<v t="ekr.20250430053637.352"><vh>Test.test_breakInsideLoop</vh></v>
<v t="ekr.20250430053637.353"><vh>Test.test_defaultExceptLast</vh></v>
<v t="ekr.20250430053637.354"><vh>Test.test_defaultExceptNotLast</vh></v>
<v t="ekr.20250430053637.355"><vh>Test.test_starredAssignmentNoError</vh></v>
<v t="ekr.20250430053637.356"><vh>Test.test_starredAssignmentErrors</vh></v>
<v t="ekr.20250430053637.357"><vh>Test.test_doubleAssignment</vh></v>
<v t="ekr.20250430053637.358"><vh>Test.test_doubleAssignmentConditionally</vh></v>
<v t="ekr.20250430053637.359"><vh>Test.test_doubleAssignmentWithUse</vh></v>
<v t="ekr.20250430053637.360"><vh>Test.test_comparison</vh></v>
<v t="ekr.20250430053637.361"><vh>Test.test_identity</vh></v>
<v t="ekr.20250430053637.362"><vh>Test.test_containment</vh></v>
<v t="ekr.20250430053637.363"><vh>Test.test_loopControl</vh></v>
<v t="ekr.20250430053637.364"><vh>Test.test_ellipsis</vh></v>
<v t="ekr.20250430053637.365"><vh>Test.test_extendedSlice</vh></v>
<v t="ekr.20250430053637.366"><vh>Test.test_varAugmentedAssignment</vh></v>
<v t="ekr.20250430053637.367"><vh>Test.test_attrAugmentedAssignment</vh></v>
<v t="ekr.20250430053637.368"><vh>Test.test_globalDeclaredInDifferentScope</vh></v>
<v t="ekr.20250430053637.369"><vh>Test.test_function_arguments</vh></v>
<v t="ekr.20250430053637.370"><vh>Test.test_function_arguments_python3</vh></v>
</v>
<v t="ekr.20250430053637.314"><vh>class TestUnusedAssignment</vh>
<v t="ekr.20250430053637.371"><vh>TestUnusedAssignment.test_unusedVariable</vh></v>
<v t="ekr.20250430053637.372"><vh>TestUnusedAssignment.test_unusedUnderscoreVariable</vh></v>
<v t="ekr.20250430053637.373"><vh>TestUnusedAssignment.test_unusedVariableAsLocals</vh></v>
<v t="ekr.20250430053637.374"><vh>TestUnusedAssignment.test_unusedVariableNoLocals</vh></v>
<v t="ekr.20250430053637.375"><vh>TestUnusedAssignment.test_unusedReassignedVariable</vh></v>
<v t="ekr.20250430053637.376"><vh>TestUnusedAssignment.test_variableUsedInLoop</vh></v>
<v t="ekr.20250430053637.377"><vh>TestUnusedAssignment.test_assignToGlobal</vh></v>
<v t="ekr.20250430053637.378"><vh>TestUnusedAssignment.test_assignToNonlocal</vh></v>
<v t="ekr.20250430053637.379"><vh>TestUnusedAssignment.test_assignToMember</vh></v>
<v t="ekr.20250430053637.380"><vh>TestUnusedAssignment.test_assignInForLoop</vh></v>
<v t="ekr.20250430053637.381"><vh>TestUnusedAssignment.test_assignInListComprehension</vh></v>
<v t="ekr.20250430053637.382"><vh>TestUnusedAssignment.test_generatorExpression</vh></v>
<v t="ekr.20250430053637.383"><vh>TestUnusedAssignment.test_assignmentInsideLoop</vh></v>
<v t="ekr.20250430053637.384"><vh>TestUnusedAssignment.test_tupleUnpacking</vh></v>
<v t="ekr.20250430053637.385"><vh>TestUnusedAssignment.test_listUnpacking</vh></v>
<v t="ekr.20250430053637.386"><vh>TestUnusedAssignment.test_closedOver</vh></v>
<v t="ekr.20250430053637.387"><vh>TestUnusedAssignment.test_doubleClosedOver</vh></v>
<v t="ekr.20250430053637.388"><vh>TestUnusedAssignment.test_tracebackhideSpecialVariable</vh></v>
<v t="ekr.20250430053637.389"><vh>TestUnusedAssignment.test_ifexp</vh></v>
<v t="ekr.20250430053637.390"><vh>TestUnusedAssignment.test_if_tuple</vh></v>
<v t="ekr.20250430053637.391"><vh>TestUnusedAssignment.test_withStatementNoNames</vh></v>
<v t="ekr.20250430053637.392"><vh>TestUnusedAssignment.test_withStatementSingleName</vh></v>
<v t="ekr.20250430053637.393"><vh>TestUnusedAssignment.test_withStatementAttributeName</vh></v>
<v t="ekr.20250430053637.394"><vh>TestUnusedAssignment.test_withStatementSubscript</vh></v>
<v t="ekr.20250430053637.395"><vh>TestUnusedAssignment.test_withStatementSubscriptUndefined</vh></v>
<v t="ekr.20250430053637.396"><vh>TestUnusedAssignment.test_withStatementTupleNames</vh></v>
<v t="ekr.20250430053637.397"><vh>TestUnusedAssignment.test_withStatementListNames</vh></v>
<v t="ekr.20250430053637.398"><vh>TestUnusedAssignment.test_withStatementComplicatedTarget</vh></v>
<v t="ekr.20250430053637.399"><vh>TestUnusedAssignment.test_withStatementSingleNameUndefined</vh></v>
<v t="ekr.20250430053637.400"><vh>TestUnusedAssignment.test_withStatementTupleNamesUndefined</vh></v>
<v t="ekr.20250430053637.401"><vh>TestUnusedAssignment.test_withStatementSingleNameRedefined</vh></v>
<v t="ekr.20250430053637.402"><vh>TestUnusedAssignment.test_withStatementTupleNamesRedefined</vh></v>
<v t="ekr.20250430053637.403"><vh>TestUnusedAssignment.test_withStatementUndefinedInside</vh></v>
<v t="ekr.20250430053637.404"><vh>TestUnusedAssignment.test_withStatementNameDefinedInBody</vh></v>
<v t="ekr.20250430053637.405"><vh>TestUnusedAssignment.test_withStatementUndefinedInExpression</vh></v>
<v t="ekr.20250430053637.406"><vh>TestUnusedAssignment.test_dictComprehension</vh></v>
<v t="ekr.20250430053637.407"><vh>TestUnusedAssignment.test_setComprehensionAndLiteral</vh></v>
<v t="ekr.20250430053637.408"><vh>TestUnusedAssignment.test_exceptionUsedInExcept</vh></v>
<v t="ekr.20250430053637.409"><vh>TestUnusedAssignment.test_exceptionUnusedInExcept</vh></v>
<v t="ekr.20250430053637.410"><vh>TestUnusedAssignment.test_exception_unused_in_except_star</vh></v>
<v t="ekr.20250430053637.411"><vh>TestUnusedAssignment.test_exceptionUnusedInExceptInFunction</vh></v>
<v t="ekr.20250430053637.412"><vh>TestUnusedAssignment.test_exceptWithoutNameInFunction</vh></v>
<v t="ekr.20250430053637.413"><vh>TestUnusedAssignment.test_exceptWithoutNameInFunctionTuple</vh></v>
<v t="ekr.20250430053637.414"><vh>TestUnusedAssignment.test_augmentedAssignmentImportedFunctionCall</vh></v>
<v t="ekr.20250430053637.415"><vh>TestUnusedAssignment.test_assert_without_message</vh></v>
<v t="ekr.20250430053637.416"><vh>TestUnusedAssignment.test_assert_with_message</vh></v>
<v t="ekr.20250430053637.417"><vh>TestUnusedAssignment.test_assert_tuple</vh></v>
<v t="ekr.20250430053637.418"><vh>TestUnusedAssignment.test_assert_tuple_empty</vh></v>
<v t="ekr.20250430053637.419"><vh>TestUnusedAssignment.test_assert_static</vh></v>
<v t="ekr.20250430053637.420"><vh>TestUnusedAssignment.test_yieldFromUndefined</vh></v>
<v t="ekr.20250430053637.421"><vh>TestUnusedAssignment.test_f_string</vh></v>
<v t="ekr.20250430053637.422"><vh>TestUnusedAssignment.test_assign_expr</vh></v>
<v t="ekr.20250430053637.423"><vh>TestUnusedAssignment.test_assign_expr_generator_scope</vh></v>
<v t="ekr.20250430053637.424"><vh>TestUnusedAssignment.test_assign_expr_nested</vh></v>
</v>
<v t="ekr.20250430053637.315"><vh>class TestStringFormatting</vh>
<v t="ekr.20250430053637.425"><vh>TestStringFormatting.test_f_string_without_placeholders</vh></v>
<v t="ekr.20250430053637.426"><vh>TestStringFormatting.test_invalid_dot_format_calls</vh></v>
<v t="ekr.20250430053637.427"><vh>TestStringFormatting.test_invalid_percent_format_calls</vh></v>
<v t="ekr.20250430053637.428"><vh>TestStringFormatting.test_ok_percent_format_cannot_determine_element_count</vh></v>
</v>
<v t="ekr.20250430053637.316"><vh>class TestAsyncStatements</vh>
<v t="ekr.20250430053637.429"><vh>TestAsyncStatements.test_asyncDef</vh></v>
<v t="ekr.20250430053637.430"><vh>TestAsyncStatements.test_asyncDefAwait</vh></v>
<v t="ekr.20250430053637.431"><vh>TestAsyncStatements.test_asyncDefUndefined</vh></v>
<v t="ekr.20250430053637.432"><vh>TestAsyncStatements.test_asyncFor</vh></v>
<v t="ekr.20250430053637.433"><vh>TestAsyncStatements.test_asyncForUnderscoreLoopVar</vh></v>
<v t="ekr.20250430053637.434"><vh>TestAsyncStatements.test_loopControlInAsyncFor</vh></v>
<v t="ekr.20250430053637.435"><vh>TestAsyncStatements.test_loopControlInAsyncForElse</vh></v>
<v t="ekr.20250430053637.436"><vh>TestAsyncStatements.test_asyncWith</vh></v>
<v t="ekr.20250430053637.437"><vh>TestAsyncStatements.test_asyncWithItem</vh></v>
<v t="ekr.20250430053637.438"><vh>TestAsyncStatements.test_matmul</vh></v>
<v t="ekr.20250430053637.439"><vh>TestAsyncStatements.test_formatstring</vh></v>
<v t="ekr.20250430053637.440"><vh>TestAsyncStatements.test_raise_notimplemented</vh></v>
</v>
<v t="ekr.20250430053637.317"><vh>class TestIncompatiblePrintOperator</vh>
<v t="ekr.20250430053637.441"><vh>TestIncompatiblePrintOperator.test_valid_print</vh></v>
<v t="ekr.20250430053637.442"><vh>TestIncompatiblePrintOperator.test_invalid_print_when_imported_from_future</vh></v>
<v t="ekr.20250430053637.443"><vh>TestIncompatiblePrintOperator.test_print_augmented_assign</vh></v>
<v t="ekr.20250430053637.444"><vh>TestIncompatiblePrintOperator.test_print_function_assignment</vh></v>
<v t="ekr.20250430053637.445"><vh>TestIncompatiblePrintOperator.test_print_in_lambda</vh></v>
<v t="ekr.20250430053637.446"><vh>TestIncompatiblePrintOperator.test_print_returned_in_function</vh></v>
<v t="ekr.20250430053637.447"><vh>TestIncompatiblePrintOperator.test_print_as_condition_test</vh></v>
</v>
</v>
<v t="ekr.20250430053637.448"><vh>@@file test/test_type_annotations.py</vh>
<v t="ekr.20250430053637.449"><vh>class TestTypeAnnotations</vh>
<v t="ekr.20250430053637.450"><vh>TestTypeAnnotations.test_typingOverload</vh></v>
<v t="ekr.20250430053637.451"><vh>TestTypeAnnotations.test_typingExtensionsOverload</vh></v>
<v t="ekr.20250430053637.452"><vh>TestTypeAnnotations.test_typingOverloadAsync</vh></v>
<v t="ekr.20250430053637.453"><vh>TestTypeAnnotations.test_overload_with_multiple_decorators</vh></v>
<v t="ekr.20250430053637.454"><vh>TestTypeAnnotations.test_overload_in_class</vh></v>
<v t="ekr.20250430053637.455"><vh>TestTypeAnnotations.test_aliased_import</vh></v>
<v t="ekr.20250430053637.456"><vh>TestTypeAnnotations.test_not_a_typing_overload</vh></v>
<v t="ekr.20250430053637.457"><vh>TestTypeAnnotations.test_variable_annotations</vh></v>
<v t="ekr.20250430053637.458"><vh>TestTypeAnnotations.test_variable_annotation_references_self_name_undefined</vh></v>
<v t="ekr.20250430053637.459"><vh>TestTypeAnnotations.test_TypeAlias_annotations</vh></v>
<v t="ekr.20250430053637.460"><vh>TestTypeAnnotations.test_annotating_an_import</vh></v>
<v t="ekr.20250430053637.461"><vh>TestTypeAnnotations.test_unused_annotation</vh></v>
<v t="ekr.20250430053637.462"><vh>TestTypeAnnotations.test_unused_annotation_in_outer_scope_reassigned_in_local_scope</vh></v>
<v t="ekr.20250430053637.463"><vh>TestTypeAnnotations.test_unassigned_annotation_is_undefined</vh></v>
<v t="ekr.20250430053637.464"><vh>TestTypeAnnotations.test_annotated_async_def</vh></v>
<v t="ekr.20250430053637.465"><vh>TestTypeAnnotations.test_postponed_annotations</vh></v>
<v t="ekr.20250430053637.466"><vh>TestTypeAnnotations.test_type_annotation_clobbers_all</vh></v>
<v t="ekr.20250430053637.467"><vh>TestTypeAnnotations.test_return_annotation_is_class_scope_variable</vh></v>
<v t="ekr.20250430053637.468"><vh>TestTypeAnnotations.test_return_annotation_is_function_body_variable</vh></v>
<v t="ekr.20250430053637.469"><vh>TestTypeAnnotations.test_positional_only_argument_annotations</vh></v>
<v t="ekr.20250430053637.470"><vh>TestTypeAnnotations.test_partially_quoted_type_annotation</vh></v>
<v t="ekr.20250430053637.471"><vh>TestTypeAnnotations.test_partially_quoted_type_assignment</vh></v>
<v t="ekr.20250430053637.472"><vh>TestTypeAnnotations.test_nested_partially_quoted_type_assignment</vh></v>
<v t="ekr.20250430053637.473"><vh>TestTypeAnnotations.test_quoted_type_cast</vh></v>
<v t="ekr.20250430053637.474"><vh>TestTypeAnnotations.test_type_cast_literal_str_to_str</vh></v>
<v t="ekr.20250430053637.475"><vh>TestTypeAnnotations.test_quoted_type_cast_renamed_import</vh></v>
<v t="ekr.20250430053637.476"><vh>TestTypeAnnotations.test_quoted_TypeVar_constraints</vh></v>
<v t="ekr.20250430053637.477"><vh>TestTypeAnnotations.test_quoted_TypeVar_bound</vh></v>
<v t="ekr.20250430053637.478"><vh>TestTypeAnnotations.test_literal_type_typing</vh></v>
<v t="ekr.20250430053637.479"><vh>TestTypeAnnotations.test_literal_type_typing_extensions</vh></v>
<v t="ekr.20250430053637.480"><vh>TestTypeAnnotations.test_annotated_type_typing_missing_forward_type</vh></v>
<v t="ekr.20250430053637.481"><vh>TestTypeAnnotations.test_annotated_type_typing_missing_forward_type_multiple_args</vh></v>
<v t="ekr.20250430053637.482"><vh>TestTypeAnnotations.test_annotated_type_typing_with_string_args</vh></v>
<v t="ekr.20250430053637.483"><vh>TestTypeAnnotations.test_annotated_type_typing_with_string_args_in_union</vh></v>
<v t="ekr.20250430053637.484"><vh>TestTypeAnnotations.test_literal_type_some_other_module</vh></v>
<v t="ekr.20250430053637.485"><vh>TestTypeAnnotations.test_literal_union_type_typing</vh></v>
<v t="ekr.20250430053637.486"><vh>TestTypeAnnotations.test_deferred_twice_annotation</vh></v>
<v t="ekr.20250430053637.487"><vh>TestTypeAnnotations.test_partial_string_annotations_with_future_annotations</vh></v>
<v t="ekr.20250430053637.488"><vh>TestTypeAnnotations.test_forward_annotations_for_classes_in_scope</vh></v>
<v t="ekr.20250430053637.489"><vh>TestTypeAnnotations.test_idomiatic_typing_guards</vh></v>
<v t="ekr.20250430053637.490"><vh>TestTypeAnnotations.test_typing_guard_for_protocol</vh></v>
<v t="ekr.20250430053637.491"><vh>TestTypeAnnotations.test_typednames_correct_forward_ref</vh></v>
<v t="ekr.20250430053637.492"><vh>TestTypeAnnotations.test_namedtypes_classes</vh></v>
<v t="ekr.20250430053637.493"><vh>TestTypeAnnotations.test_variadic_generics</vh></v>
<v t="ekr.20250430053637.494"><vh>TestTypeAnnotations.test_type_statements</vh></v>
<v t="ekr.20250430053637.495"><vh>TestTypeAnnotations.test_type_parameters_functions</vh></v>
<v t="ekr.20250430053637.496"><vh>TestTypeAnnotations.test_type_parameters_do_not_escape_function_scopes</vh></v>
<v t="ekr.20250430053637.497"><vh>TestTypeAnnotations.test_type_parameters_classes</vh></v>
<v t="ekr.20250430053637.498"><vh>TestTypeAnnotations.test_type_parameters_do_not_escape_class_scopes</vh></v>
<v t="ekr.20250430053637.499"><vh>TestTypeAnnotations.test_type_parameters_TypeVarTuple</vh></v>
<v t="ekr.20250430053637.500"><vh>TestTypeAnnotations.test_type_parameters_ParamSpec</vh></v>
</v>
</v>
<v t="ekr.20250430053637.501"><vh>@@file test/test_undefined_names.py</vh>
<v t="ekr.20250430053637.502"><vh>class Test</vh>
<v t="ekr.20250430053637.504"><vh>Test.test_undefined</vh></v>
<v t="ekr.20250430053637.505"><vh>Test.test_definedInListComp</vh></v>
<v t="ekr.20250430053637.506"><vh>Test.test_undefinedInListComp</vh></v>
<v t="ekr.20250430053637.507"><vh>Test.test_undefinedExceptionName</vh></v>
<v t="ekr.20250430053637.508"><vh>Test.test_namesDeclaredInExceptBlocks</vh></v>
<v t="ekr.20250430053637.509"><vh>Test.test_undefinedExceptionNameObscuringLocalVariable</vh></v>
<v t="ekr.20250430053637.510"><vh>Test.test_undefinedExceptionNameObscuringLocalVariable2</vh></v>
<v t="ekr.20250430053637.511"><vh>Test.test_undefinedExceptionNameObscuringLocalVariableFalsePositive1</vh></v>
<v t="ekr.20250430053637.512"><vh>Test.test_delExceptionInExcept</vh></v>
<v t="ekr.20250430053637.513"><vh>Test.test_undefinedExceptionNameObscuringLocalVariableFalsePositive2</vh></v>
<v t="ekr.20250430053637.514"><vh>Test.test_undefinedExceptionNameObscuringGlobalVariable</vh></v>
<v t="ekr.20250430053637.515"><vh>Test.test_undefinedExceptionNameObscuringGlobalVariable2</vh></v>
<v t="ekr.20250430053637.516"><vh>Test.test_undefinedExceptionNameObscuringGlobalVariableFalsePositive1</vh></v>
<v t="ekr.20250430053637.517"><vh>Test.test_undefinedExceptionNameObscuringGlobalVariableFalsePositive2</vh></v>
<v t="ekr.20250430053637.518"><vh>Test.test_functionsNeedGlobalScope</vh></v>
<v t="ekr.20250430053637.519"><vh>Test.test_builtins</vh></v>
<v t="ekr.20250430053637.520"><vh>Test.test_builtinWindowsError</vh></v>
<v t="ekr.20250430053637.521"><vh>Test.test_moduleAnnotations</vh></v>
<v t="ekr.20250430053637.522"><vh>Test.test_magicGlobalsFile</vh></v>
<v t="ekr.20250430053637.523"><vh>Test.test_magicGlobalsBuiltins</vh></v>
<v t="ekr.20250430053637.524"><vh>Test.test_magicGlobalsName</vh></v>
<v t="ekr.20250430053637.525"><vh>Test.test_magicGlobalsPath</vh></v>
<v t="ekr.20250430053637.526"><vh>Test.test_magicModuleInClassScope</vh></v>
<v t="ekr.20250430053637.527"><vh>Test.test_magicQualnameInClassScope</vh></v>
<v t="ekr.20250430053637.528"><vh>Test.test_globalImportStar</vh></v>
<v t="ekr.20250430053637.529"><vh>Test.test_definedByGlobal</vh></v>
<v t="ekr.20250430053637.530"><vh>Test.test_definedByGlobalMultipleNames</vh></v>
<v t="ekr.20250430053637.531"><vh>Test.test_globalInGlobalScope</vh></v>
<v t="ekr.20250430053637.532"><vh>Test.test_global_reset_name_only</vh></v>
<v t="ekr.20250430053637.533"><vh>Test.test_unused_global</vh></v>
<v t="ekr.20250430053637.534"><vh>Test.test_del</vh></v>
<v t="ekr.20250430053637.535"><vh>Test.test_delGlobal</vh></v>
<v t="ekr.20250430053637.536"><vh>Test.test_delUndefined</vh></v>
<v t="ekr.20250430053637.537"><vh>Test.test_delConditional</vh></v>
<v t="ekr.20250430053637.538"><vh>Test.test_delConditionalNested</vh></v>
<v t="ekr.20250430053637.539"><vh>Test.test_delWhile</vh></v>
<v t="ekr.20250430053637.540"><vh>Test.test_delWhileTestUsage</vh></v>
<v t="ekr.20250430053637.541"><vh>Test.test_delWhileNested</vh></v>
<v t="ekr.20250430053637.542"><vh>Test.test_globalFromNestedScope</vh></v>
<v t="ekr.20250430053637.543"><vh>Test.test_laterRedefinedGlobalFromNestedScope</vh></v>
<v t="ekr.20250430053637.544"><vh>Test.test_laterRedefinedGlobalFromNestedScope2</vh></v>
<v t="ekr.20250430053637.545"><vh>Test.test_intermediateClassScopeIgnored</vh></v>
<v t="ekr.20250430053637.546"><vh>Test.test_doubleNestingReportsClosestName</vh></v>
<v t="ekr.20250430053637.547"><vh>Test.test_laterRedefinedGlobalFromNestedScope3</vh></v>
<v t="ekr.20250430053637.548"><vh>Test.test_undefinedAugmentedAssignment</vh></v>
<v t="ekr.20250430053637.549"><vh>Test.test_nestedClass</vh></v>
<v t="ekr.20250430053637.550"><vh>Test.test_badNestedClass</vh></v>
<v t="ekr.20250430053637.551"><vh>Test.test_definedAsStarArgs</vh></v>
<v t="ekr.20250430053637.552"><vh>Test.test_definedAsStarUnpack</vh></v>
<v t="ekr.20250430053637.553"><vh>Test.test_usedAsStarUnpack</vh></v>
<v t="ekr.20250430053637.554"><vh>Test.test_unusedAsStarUnpack</vh></v>
<v t="ekr.20250430053637.555"><vh>Test.test_keywordOnlyArgs</vh></v>
<v t="ekr.20250430053637.556"><vh>Test.test_keywordOnlyArgsUndefined</vh></v>
<v t="ekr.20250430053637.557"><vh>Test.test_annotationUndefined</vh></v>
<v t="ekr.20250430053637.558"><vh>Test.test_metaClassUndefined</vh></v>
<v t="ekr.20250430053637.559"><vh>Test.test_definedInGenExp</vh></v>
<v t="ekr.20250430053637.560"><vh>Test.test_undefinedInGenExpNested</vh></v>
<v t="ekr.20250430053637.561"><vh>Test.test_undefinedWithErrorHandler</vh></v>
<v t="ekr.20250430053637.562"><vh>Test.test_definedInClass</vh></v>
<v t="ekr.20250430053637.563"><vh>Test.test_definedInClassNested</vh></v>
<v t="ekr.20250430053637.564"><vh>Test.test_undefinedInLoop</vh></v>
<v t="ekr.20250430053637.565"><vh>Test.test_definedFromLambdaInDictionaryComprehension</vh></v>
<v t="ekr.20250430053637.566"><vh>Test.test_definedFromLambdaInGenerator</vh></v>
<v t="ekr.20250430053637.567"><vh>Test.test_undefinedFromLambdaInDictionaryComprehension</vh></v>
<v t="ekr.20250430053637.568"><vh>Test.test_undefinedFromLambdaInComprehension</vh></v>
<v t="ekr.20250430053637.569"><vh>Test.test_dunderClass</vh></v>
</v>
<v t="ekr.20250430053637.503"><vh>class NameTests</vh>
<v t="ekr.20250430053637.570"><vh>NameTests.test_impossibleContext</vh></v>
</v>
</v>
</v>
</v>
</v>
</v>
<v t="ekr.20250426050131.1"><vh>--- files</vh>
<v t="ekr.20250512055230.1"><vh>@clean setup.cfg </vh></v>
<v t="ekr.20250426050140.1"><vh>@file src/semantic_cache.py</vh></v>
<v t="ekr.20250512061658.1"><vh>@file tests/test.py</vh></v>
</v>
<v t="ekr.20250426191746.1"><vh>*** To do</vh>
<v t="ekr.20250501074230.1"><vh>--- messages.py</vh>
<v t="ekr.20250430053636.166"></v>
<v t="ekr.20250430053636.217"></v>
<v t="ekr.20250430132623.1"><vh>strange classes</vh>
<v t="ekr.20250430053636.177"></v>
<v t="ekr.20250430053636.182"></v>
</v>
<v t="ekr.20250430062819.1"><vh>script: simplify message classes</vh>
<v t="ekr.20250430122722.1"><vh>function: replace_body</vh></v>
<v t="ekr.20250430111617.1"><vh>function: update_body</vh></v>
<v t="ekr.20250430121603.1"><vh>function: update_ctor</vh></v>
<v t="ekr.20250430114925.1"><vh>function: update_message</vh></v>
</v>
</v>
</v>
<v t="ekr.20250501074906.1"><vh>--- classes</vh>
<v t="ekr.20250427190248.1"><vh>class CacheController</vh>
<v t="ekr.20250428033750.1"><vh>CacheController.__init__</vh></v>
<v t="ekr.20250427200712.1"><vh>CacheController.commit &amp; close</vh></v>
<v t="ekr.20250428100117.1"><vh>CacheController.compute_diffs</vh></v>
<v t="ekr.20250428034510.1"><vh>CacheController.dump</vh></v>
<v t="ekr.20250427190307.1"><vh>CacheController.get_changed_files</vh></v>
<v t="ekr.20250428071526.1"><vh>CacheController.print_stats</vh></v>
<v t="ekr.20250427194628.1"><vh>CacheController.write_cache</vh></v>
</v>
<v t="ekr.20250430053636.61"></v>
<v t="ekr.20250427052951.1"><vh>class SemanticCache(SqlitePickleShare)</vh>
<v t="ekr.20250427053445.1"><vh>SemanticCache.__init__</vh></v>
</v>
</v>
<v t="ekr.20250512075040.1"><vh>--- testing</vh>
<v t="ekr.20250512073231.1"><vh>class CacheTests</vh>
<v t="ekr.20250512062255.1"><vh>CacheTests.test_import</vh></v>
</v>
<v t="ekr.20250512062255.1"></v>
<v t="ekr.20250512061621.1"></v>
</v>
<v t="ekr.20250426052508.1"><vh>function: main</vh></v>
</vnodes>
<tnodes>
<t tx="ekr.20250426044347.1"></t>
<t tx="ekr.20250426044445.1">"""
Back up this .leo file.

os.environ['LEO_BACKUP'] must be the path to an existing (writable) directory.
"""
c.backup_helper(sub_dir='ekr-cache')
</t>
<t tx="ekr.20250426044713.1">@language python
"""Recursively import all python files in a directory and clean the result."""
@tabwidth -4 # For a better match.
g.cls()

dir_ = r'C:\Python\Python3.13\Lib\site-packages\pyflakes'

c.recursiveImport(
    dir_= dir_,
    kind = '@file', # '@auto', '@clean', '@nosent','@file',
    recursive = True,
    safe_at_file = True,
    theTypes = ['.py',],
    verbose = True,
)
if 1:
    last = c.lastTopLevel()
    last.expand()
    if last.hasChildren():
        last.firstChild().expand()
    c.redraw(last)
print('Done')</t>
<t tx="ekr.20250426045002.1"># This file is part of Leo: https://leo-editor.github.io/leo-editor
# Leo's copyright notice is based on the MIT license:
# https://leo-editor.github.io/leo-editor/license.html

# Don't pollute searches with matches from this file!
### @nosearch

&lt;&lt; leoAst docstring &gt;&gt;
&lt;&lt; leoAst imports &amp; annotations &gt;&gt;

v1, v2, junk1, junk2, junk3 = sys.version_info
if (v1, v2) &lt; (3, 9):  # pragma: no cover
    raise ImportError('The commands in leoAst.py require Python 3.9 or above')

@others

if __name__ == '__main__':
    main()  # pragma: no cover

@language python
@tabwidth -4
@pagewidth 70
</t>
<t tx="ekr.20250426045002.10">def check_g() -&gt; bool:
    """print an error message if g is None"""
    if not g:
        print('This statement failed: `from leo.core import leoGlobals as g`')
        print('Please adjust your Python path accordingly')
    return bool(g)
</t>
<t tx="ekr.20250426045002.100">def do_op(self) -&gt; None:
    """Handle an op token."""
    val = self.val
    if val == '.':
        self.clean('blank')
        prev = self.code_list[-1]
        # #2495 &amp; #2533: Special case for 'from .'
        if prev.kind == 'word' and prev.value == 'from':
            self.blank()
        self.add_token('op-no-blanks', val)
    elif val == '@':
        if self.black_mode:  # pragma: no cover (black)
            if not self.decorator_seen:
                self.blank_lines(1)
                self.decorator_seen = True
        self.clean('blank')
        self.add_token('op-no-blanks', val)
        self.push_state('decorator')
    elif val == ':':
        # Treat slices differently.
        self.colon(val)
    elif val in ',;':
        # Pep 8: Avoid extraneous whitespace immediately before
        # comma, semicolon, or colon.
        self.clean('blank')
        self.add_token('op', val)
        self.blank()
    elif val in '([{':
        # Pep 8: Avoid extraneous whitespace immediately inside
        # parentheses, brackets or braces.
        self.lt(val)
    elif val in ')]}':
        # Ditto.
        self.rt(val)
    elif val == '=':
        self.do_equal_op(val)
    elif val in '~+-':
        self.possible_unary_op(val)
    elif val == '*':
        self.star_op()
    elif val == '**':
        self.star_star_op()
    else:
        # Pep 8: always surround binary operators with a single space.
        # '==','+=','-=','*=','**=','/=','//=','%=','!=','&lt;=','&gt;=','&lt;','&gt;',
        # '^','~','*','**','&amp;','|','/','//',
        # Pep 8: If operators with different priorities are used,
        # consider adding whitespace around the operators with the lowest priorities.
        self.blank()
        self.add_token('op', val)
        self.blank()
</t>
<t tx="ekr.20250426045002.101"># Keys: token.index of '=' token. Values: count of ???s
arg_dict: dict[int, int] = {}

dump_flag = True

def do_equal_op(self, val: str) -&gt; None:

    if 0:
        token = self.token
        g.trace(
            f"token.index: {token.index:2} paren_level: {self.paren_level} "
            f"token.equal_sign_spaces: {int(token.equal_sign_spaces)} "
            # f"{token.node.__class__.__name__}"
        )
        # dump_tree(self.tokens, self.tree)
    if self.token.equal_sign_spaces:
        self.blank()
        self.add_token('op', val)
        self.blank()
    else:
        # Pep 8: Don't use spaces around the = sign when used to indicate
        #        a keyword argument or a default parameter value.
        #        However, hen combining an argument annotation with a default value,
        #        *do* use spaces around the = sign
        self.clean('blank')
        self.add_token('op-no-blanks', val)
</t>
<t tx="ekr.20250426045002.102">def do_string(self) -&gt; None:
    """Handle a 'string' token."""
    # Careful: continued strings may contain '\r'
    val = regularize_nls(self.val)
    self.add_token('string', val)
    self.blank()
</t>
<t tx="ekr.20250426045002.103">beautify_pat = re.compile(
    r'#\s*pragma:\s*beautify\b|#\s*@@beautify|#\s*@\+node|#\s*@[+-]others|#\s*@[+-]&lt;&lt;')

def do_verbatim(self) -&gt; None:
    """
    Handle one token in verbatim mode.
    End verbatim mode when the appropriate comment is seen.
    """
    kind = self.kind
    #
    # Careful: tokens may contain '\r'
    val = regularize_nls(self.val)
    if kind == 'comment':
        if self.beautify_pat.match(val):
            self.verbatim = False
        val = val.rstrip()
        self.add_token('comment', val)
        return
    if kind == 'indent':
        self.level += 1
        self.lws = self.level * self.tab_width * ' '
    if kind == 'dedent':
        self.level -= 1
        self.lws = self.level * self.tab_width * ' '
    self.add_token('verbatim', val)
</t>
<t tx="ekr.20250426045002.104">def do_ws(self) -&gt; None:
    """
    Handle the "ws" pseudo-token.

    Put the whitespace only if if ends with backslash-newline.
    """
    val = self.val
    # Handle backslash-newline.
    if '\\\n' in val:
        self.clean('blank')
        self.add_token('op-no-blanks', val)
        return
    # Handle start-of-line whitespace.
    prev = self.code_list[-1]
    inner = self.paren_level or self.square_brackets_stack or self.curly_brackets_level
    if prev.kind == 'line-indent' and inner:
        # Retain the indent that won't be cleaned away.
        self.clean('line-indent')
        self.add_token('hard-blank', val)
</t>
<t tx="ekr.20250426045002.105"></t>
<t tx="ekr.20250426045002.106">def add_line_end(self) -&gt; OutputToken:
    """Add a line-end request to the code list."""
    # This may be called from do_name as well as do_newline and do_nl.
    assert self.token.kind in ('newline', 'nl'), self.token.kind
    self.clean('blank')  # Important!
    self.clean('line-indent')
    t = self.add_token('line-end', '\n')
    # Distinguish between kinds of 'line-end' tokens.
    t.newline_kind = self.token.kind
    return t
</t>
<t tx="ekr.20250426045002.107">def add_token(self, kind: str, value: Value) -&gt; OutputToken:
    """Add an output token to the code list."""
    tok = OutputToken(kind, value)
    tok.index = len(self.code_list)
    self.code_list.append(tok)
    return tok
</t>
<t tx="ekr.20250426045002.108">def blank(self) -&gt; None:
    """Add a blank request to the code list."""
    prev = self.code_list[-1]
    if prev.kind not in (
        'blank',
        'blank-lines',
        'file-start',
        'hard-blank',  # Unique to orange.
        'line-end',
        'line-indent',
        'lt',
        'op-no-blanks',
        'unary-op',
    ):
        self.add_token('blank', ' ')
</t>
<t tx="ekr.20250426045002.109">def blank_lines(self, n: int) -&gt; None:  # pragma: no cover (black)
    """
    Add a request for n blank lines to the code list.
    Multiple blank-lines request yield at least the maximum of all requests.
    """
    self.clean_blank_lines()
    prev = self.code_list[-1]
    if prev.kind == 'file-start':
        self.add_token('blank-lines', n)
        return
    for _i in range(0, n + 1):
        self.add_token('line-end', '\n')
    # Retain the token (intention) for debugging.
    self.add_token('blank-lines', n)
    self.line_indent()
</t>
<t tx="ekr.20250426045002.11">def regularize_nls(s: str) -&gt; str:
    """Regularize newlines within s."""
    return s.replace('\r\n', '\n').replace('\r', '\n')
</t>
<t tx="ekr.20250426045002.110">def clean(self, kind: str) -&gt; None:
    """Remove the last item of token list if it has the given kind."""
    prev = self.code_list[-1]
    if prev.kind == kind:
        self.code_list.pop()
</t>
<t tx="ekr.20250426045002.111">def clean_blank_lines(self) -&gt; bool:
    """
    Remove all vestiges of previous blank lines.

    Return True if any of the cleaned 'line-end' tokens represented "hard" newlines.
    """
    cleaned_newline = False
    table = ('blank-lines', 'line-end', 'line-indent')
    while self.code_list[-1].kind in table:
        t = self.code_list.pop()
        if t.kind == 'line-end' and getattr(t, 'newline_kind', None) != 'nl':
            cleaned_newline = True
    return cleaned_newline
</t>
<t tx="ekr.20250426045002.112">def colon(self, val: str) -&gt; None:
    """Handle a colon."""

    def is_expr(node: Node) -&gt; bool:
        """True if node is any expression other than += number."""
        if isinstance(node, (ast.BinOp, ast.Call, ast.IfExp)):
            return True
        num_node = ast.Num if g.python_version_tuple &lt; (3, 12, 0) else ast.Constant
        return (
            isinstance(node, ast.UnaryOp)
            and not isinstance(node.operand, num_node)
        )

    node = self.token.node
    self.clean('blank')
    if not isinstance(node, ast.Slice):
        self.add_token('op', val)
        self.blank()
        return
    # A slice.
    lower = getattr(node, 'lower', None)
    upper = getattr(node, 'upper', None)
    step = getattr(node, 'step', None)
    if any(is_expr(z) for z in (lower, upper, step)):
        prev = self.code_list[-1]
        if prev.value not in '[:':
            self.blank()
        self.add_token('op', val)
        self.blank()
    else:
        self.add_token('op-no-blanks', val)
</t>
<t tx="ekr.20250426045002.113">def line_end(self) -&gt; None:
    """Add a line-end request to the code list."""
    # This should be called only be do_newline and do_nl.
    node, token = self.token.statement_node, self.token
    assert token.kind in ('newline', 'nl'), (token.kind, g.callers())
    # Create the 'line-end' output token.
    self.add_line_end()
    # Attempt to split the line.
    was_split = self.split_line(node, token)
    # Attempt to join the line only if it has not just been split.
    if not was_split and self.max_join_line_length &gt; 0:
        self.join_lines(node, token)
    # Add the indentation for all lines
    # until the next indent or unindent token.
    self.line_indent()
</t>
<t tx="ekr.20250426045002.114">def line_indent(self) -&gt; None:
    """Add a line-indent token."""
    self.clean('line-indent')  # Defensive. Should never happen.
    self.add_token('line-indent', self.lws)
</t>
<t tx="ekr.20250426045002.115"></t>
<t tx="ekr.20250426045002.116">def lt(self, val: str) -&gt; None:
    """Generate code for a left paren or curly/square bracket."""
    assert val in '([{', repr(val)
    if val == '(':
        self.paren_level += 1
    elif val == '[':
        self.square_brackets_stack.append(False)
    else:
        self.curly_brackets_level += 1
    self.clean('blank')
    prev = self.code_list[-1]
    if prev.kind in ('op', 'word-op'):
        self.blank()
        self.add_token('lt', val)
    elif prev.kind == 'word':
        # Only suppress blanks before '(' or '[' for non-keywords.
        if val == '{' or prev.value in ('if', 'else', 'return', 'for'):
            self.blank()
        elif val == '(':
            self.in_arg_list += 1
        self.add_token('lt', val)
    else:
        self.clean('blank')
        self.add_token('op-no-blanks', val)
</t>
<t tx="ekr.20250426045002.117">def rt(self, val: str) -&gt; None:
    """Generate code for a right paren or curly/square bracket."""
    assert val in ')]}', repr(val)
    if val == ')':
        self.paren_level -= 1
        self.in_arg_list = max(0, self.in_arg_list - 1)
    elif val == ']':
        self.square_brackets_stack.pop()
    else:
        self.curly_brackets_level -= 1
    self.clean('blank')
    self.add_token('rt', val)
</t>
<t tx="ekr.20250426045002.118">def possible_unary_op(self, s: str) -&gt; None:
    """Add a unary or binary op to the token list."""
    node = self.token.node
    self.clean('blank')
    if isinstance(node, ast.UnaryOp):
        self.unary_op(s)
    else:
        self.blank()
        self.add_token('op', s)
        self.blank()

def unary_op(self, s: str) -&gt; None:
    """Add an operator request to the code list."""
    assert s and isinstance(s, str), repr(s)
    self.clean('blank')
    prev = self.code_list[-1]
    if prev.kind == 'lt':
        self.add_token('unary-op', s)
    else:
        self.blank()
        self.add_token('unary-op', s)
</t>
<t tx="ekr.20250426045002.119">def star_op(self) -&gt; None:
    """Put a '*' op, with special cases for *args."""
    val = '*'
    node = self.token.node
    self.clean('blank')
    if isinstance(node, ast.arguments):
        self.blank()
        self.add_token('op', val)
        return  # #2533
    if self.paren_level &gt; 0:
        prev = self.code_list[-1]
        if prev.kind == 'lt' or (prev.kind, prev.value) == ('op', ','):
            self.blank()
            self.add_token('op', val)
            return
    self.blank()
    self.add_token('op', val)
    self.blank()
</t>
<t tx="ekr.20250426045002.12">def write_file(filename: str, contents: str, encoding: str = 'utf-8') -&gt; None:
    """
    Write the string s to the file whose name is given.

    Handle all exceptions.

    Before calling this function, the caller should ensure
    that the file actually has been changed.
    """
    try:
        # Do the conversion first, so errors do not destroy the file.
        byte_contents = g.toEncodedString(contents, encoding=encoding)
        # 'wb' preserves line endings.
        with open(filename, 'wb') as f:
            f.write(byte_contents)
    except Exception as e:
        print(f"exception writing: {filename}:\n{e}")
</t>
<t tx="ekr.20250426045002.120">def star_star_op(self) -&gt; None:
    """Put a ** operator, with a special case for **kwargs."""
    val = '**'
    node = self.token.node
    self.clean('blank')
    if isinstance(node, ast.arguments):
        self.blank()
        self.add_token('op', val)
        return  # #2533
    if self.paren_level &gt; 0:
        prev = self.code_list[-1]
        if prev.kind == 'lt' or (prev.kind, prev.value) == ('op', ','):
            self.blank()
            self.add_token('op', val)
            return
    self.blank()
    self.add_token('op', val)
    self.blank()
</t>
<t tx="ekr.20250426045002.121">def word(self, s: str) -&gt; None:
    """Add a word request to the code list."""
    assert s and isinstance(s, str), repr(s)
    node = self.token.node
    if isinstance(node, ast.ImportFrom) and s == 'import':  # #2533
        self.clean('blank')
        self.add_token('blank', ' ')
        self.add_token('word', s)
    elif self.square_brackets_stack:
        # A previous 'op-no-blanks' token may cancel this blank.
        self.blank()
        self.add_token('word', s)
    elif self.in_arg_list &gt; 0:
        self.add_token('word', s)
        self.blank()
    else:
        self.blank()
        self.add_token('word', s)
        self.blank()

def word_op(self, s: str) -&gt; None:
    """Add a word-op request to the code list."""
    assert s and isinstance(s, str), repr(s)
    self.blank()
    self.add_token('word-op', s)
    self.blank()
</t>
<t tx="ekr.20250426045002.122"></t>
<t tx="ekr.20250426045002.123">def split_line(self, node: Node, token: Token) -&gt; bool:
    """
    Split token's line, if possible and enabled.

    Return True if the line was broken into two or more lines.
    """
    assert token.kind in ('newline', 'nl'), repr(token)
    # Return if splitting is disabled:
    if self.max_split_line_length &lt;= 0:  # pragma: no cover (user option)
        return False
    # Return if the node can't be split.
    if not is_long_statement(node):
        return False
    # Find the *output* tokens of the previous lines.
    line_tokens = self.find_prev_line()
    line_s = ''.join([z.to_string() for z in line_tokens])
    # Do nothing for short lines.
    if len(line_s) &lt; self.max_split_line_length:
        return False
    # Return if the previous line has no opening delim: (, [ or {.
    if not any(z.kind == 'lt' for z in line_tokens):  # pragma: no cover (defensive)
        return False
    prefix = self.find_line_prefix(line_tokens)
    # Calculate the tail before cleaning the prefix.
    tail = line_tokens[len(prefix) :]
    # Cut back the token list: subtract 1 for the trailing line-end.
    self.code_list = self.code_list[: len(self.code_list) - len(line_tokens) - 1]
    # Append the tail, splitting it further, as needed.
    self.append_tail(prefix, tail)
    # Add the line-end token deleted by find_line_prefix.
    self.add_token('line-end', '\n')
    return True
</t>
<t tx="ekr.20250426045002.124">def append_tail(self, prefix: list[OutputToken], tail: list[OutputToken]) -&gt; None:
    """Append the tail tokens, splitting the line further as necessary."""
    tail_s = ''.join([z.to_string() for z in tail])
    if len(tail_s) &lt; self.max_split_line_length:
        # Add the prefix.
        self.code_list.extend(prefix)
        # Start a new line and increase the indentation.
        self.add_token('line-end', '\n')
        self.add_token('line-indent', self.lws + ' ' * 4)
        self.code_list.extend(tail)
        return
    # Still too long.  Split the line at commas.
    self.code_list.extend(prefix)
    # Start a new line and increase the indentation.
    self.add_token('line-end', '\n')
    self.add_token('line-indent', self.lws + ' ' * 4)
    open_delim = Token(kind='lt', value=prefix[-1].value)
    value = open_delim.value.replace('(', ')').replace('[', ']').replace('{', '}')
    close_delim = Token(kind='rt', value=value)
    delim_count = 1
    lws = self.lws + ' ' * 4
    for i, t in enumerate(tail):
        if t.kind == 'op' and t.value == ',':
            if delim_count == 1:
                # Start a new line.
                self.add_token('op-no-blanks', ',')
                self.add_token('line-end', '\n')
                self.add_token('line-indent', lws)
                # Kill a following blank.
                if i + 1 &lt; len(tail):
                    next_t = tail[i + 1]
                    if next_t.kind == 'blank':
                        next_t.kind = 'no-op'
                        next_t.value = ''
            else:
                self.code_list.append(t)
        elif t.kind == close_delim.kind and t.value == close_delim.value:
            # Done if the delims match.
            delim_count -= 1
            if delim_count == 0:
                # Start a new line
                self.add_token('op-no-blanks', ',')
                self.add_token('line-end', '\n')
                self.add_token('line-indent', self.lws)
                self.code_list.extend(tail[i:])
                return
            lws = lws[:-4]
            self.code_list.append(t)
        elif t.kind == open_delim.kind and t.value == open_delim.value:
            delim_count += 1
            lws = lws + ' ' * 4
            self.code_list.append(t)
        else:
            self.code_list.append(t)
    g.trace('BAD DELIMS', delim_count)  # pragma: no cover
</t>
<t tx="ekr.20250426045002.125">def find_prev_line(self) -&gt; list[OutputToken]:
    """Return the previous line, as a list of tokens."""
    line = []
    for t in reversed(self.code_list[:-1]):
        if t.kind in ('hard-newline', 'line-end'):
            break
        line.append(t)
    return list(reversed(line))
</t>
<t tx="ekr.20250426045002.126">def find_line_prefix(self, token_list: list[OutputToken]) -&gt; list[OutputToken]:
    """
    Return all tokens up to and including the first lt token.
    Also add all lt tokens directly following the first lt token.
    """
    result = []
    for t in token_list:
        result.append(t)
        if t.kind == 'lt':
            break
    return result
</t>
<t tx="ekr.20250426045002.127">def join_lines(self, node: Node, token: Token) -&gt; None:
    """
    Join preceding lines, if possible and enabled.
    token is a line_end token. node is the corresponding ast node.
    """
    if self.max_join_line_length &lt;= 0:  # pragma: no cover (user option)
        return
    assert token.kind in ('newline', 'nl'), repr(token)
    if token.kind == 'nl':
        return
    # Scan backward in the *code* list,
    # looking for 'line-end' tokens with tok.newline_kind == 'nl'
    nls = 0
    i = len(self.code_list) - 1
    t = self.code_list[i]
    assert t.kind == 'line-end', repr(t)
    # Not all tokens have a newline_kind ivar.
    assert t.newline_kind == 'newline'
    i -= 1
    while i &gt;= 0:
        t = self.code_list[i]
        if t.kind == 'comment':
            # Can't join.
            return
        if t.kind == 'string' and not self.allow_joined_strings:
            # An EKR preference: don't join strings, no matter what black does.
            # This allows "short" f-strings to be aligned.
            return
        if t.kind == 'line-end':
            if getattr(t, 'newline_kind', None) == 'nl':
                nls += 1
            else:
                break  # pragma: no cover
        i -= 1
    # Retain at the file-start token.
    if i &lt;= 0:
        i = 1
    if nls &lt;= 0:  # pragma: no cover (rare)
        return
    # Retain line-end and and any following line-indent.
    # Required, so that the regex below won't eat too much.
    while True:
        t = self.code_list[i]
        if t.kind == 'line-end':
            if getattr(t, 'newline_kind', None) == 'nl':  # pragma: no cover (rare)
                nls -= 1
            i += 1
        elif self.code_list[i].kind == 'line-indent':
            i += 1
        else:
            break  # pragma: no cover (defensive)
    if nls &lt;= 0:  # pragma: no cover (defensive)
        return
    # Calculate the joined line.
    tail = self.code_list[i:]
    tail_s = output_tokens_to_string(tail)
    tail_s = re.sub(r'\n\s*', ' ', tail_s)
    tail_s = tail_s.replace('( ', '(').replace(' )', ')')
    tail_s = tail_s.rstrip()
    # Don't join the lines if they would be too long.
    if len(tail_s) &gt; self.max_join_line_length:  # pragma: no cover (defensive)
        return
    # Cut back the code list.
    self.code_list = self.code_list[:i]
    # Add the new output tokens.
    self.add_token('string', tail_s)
    self.add_token('line-end', '\n')
</t>
<t tx="ekr.20250426045002.128">class OutputToken:
    """
    A class representing an Orange output token.
    """

    def __init__(self, kind: str, value: str):

        self.kind = kind
        self.value = value

    def __repr__(self) -&gt; str:  # pragma: no cover
        return f"OutputToken: {self.show_val(20)}"

    __str__ = __repr__


    def to_string(self) -&gt; str:
        """Return the contribution of the token to the source file."""
        return self.value if isinstance(self.value, str) else ''

    @others
</t>
<t tx="ekr.20250426045002.129">def show_val(self, truncate_n: int) -&gt; str:  # pragma: no cover
    """Return the token.value field."""
    if self.kind in ('ws', 'indent'):
        val = str(len(self.value))
    elif self.kind == 'string' or self.kind.startswith('fstring'):
        # repr would be confusing.
        val = g.truncate(self.value, truncate_n)
    else:
        val = g.truncate(repr(self.value), truncate_n)
    return val
</t>
<t tx="ekr.20250426045002.13"></t>
<t tx="ekr.20250426045002.130">class ParseState:
    """
    A class representing items in the parse state stack.

    The present states:

    'file-start': Ensures the stack stack is never empty.

    'decorator': The last '@' was a decorator.

        do_op():    push_state('decorator')
        do_name():  pops the stack if state.kind == 'decorator'.

    'indent': The indentation level for 'class' and 'def' names.

        do_name():      push_state('indent', self.level)
        do_dendent():   pops the stack once or twice if state.value == self.level.

    """

    def __init__(self, kind: str, value: Union[int, str]) -&gt; None:
        self.kind = kind
        self.value = value

    def __repr__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover

    __str__ = __repr__
</t>
<t tx="ekr.20250426045002.131">class ReassignTokens:
    """A class that reassigns tokens to more appropriate ast nodes."""
    @others
</t>
<t tx="ekr.20250426045002.132">def reassign(self, filename: str, tokens: list[Token], tree: Node) -&gt; None:
    """The main entry point."""
    self.filename = filename
    self.tokens = tokens
    # Just handle Call nodes.
    for node in ast.walk(tree):
        if isinstance(node, ast.Call):
            self.visit_call(node)
</t>
<t tx="ekr.20250426045002.133">def visit_call(self, node: Node) -&gt; None:
    """ReassignTokens.visit_call"""
    tokens = tokens_for_node(self.filename, node, self.tokens)
    node0, node9 = tokens[0].node, tokens[-1].node
    nca = nearest_common_ancestor(node0, node9)
    if not nca:
        return
    # Associate () with the call node.
    i = tokens[-1].index
    j = find_paren_token(i + 1, self.tokens)
    if j is None:
        return  # pragma: no cover
    k = find_paren_token(j + 1, self.tokens)
    if k is None:
        return  # pragma: no cover
    self.tokens[j].node = nca
    self.tokens[k].node = nca
    add_token_to_token_list(self.tokens[j], nca)
    add_token_to_token_list(self.tokens[k], nca)
</t>
<t tx="ekr.20250426045002.134">class Token:
    """
    A class representing a *general* token.

    The TOG makes no distinction between input and output tokens.
    """

    def __init__(self, kind: str, value: str):

        self.kind = kind
        self.value = value
        # Injected by Tokenizer.add_token.
        self.five_tuple: tuple = None
        self.index = 0
        # The entire line containing the token.
        # Same as five_tuple.line.
        self.line = ''
        # The line number, for errors and dumps.
        # Same as five_tuple.start[0]
        self.line_number = 0
        self.level = 0
        self.node: Optional[Node] = None

    def __repr__(self) -&gt; str:  # pragma: no cover
        s = f"{self.index:&lt;3} {self.kind}"
        return f"Token {s}: {self.show_val(20)}"

    __str__ = __repr__


    def to_string(self) -&gt; str:
        """Return the contribution of the token to the source file."""
        return self.value if isinstance(self.value, str) else ''
    @others
</t>
<t tx="ekr.20250426045002.135">def brief_dump(self) -&gt; str:  # pragma: no cover
    """Dump a token."""
    return (
        f"{self.index:&gt;3} line: {self.line_number:&lt;2} "
        f"{self.kind:&gt;15} {self.show_val(100)}")
</t>
<t tx="ekr.20250426045002.136">def dump(self) -&gt; str:  # pragma: no cover
    """Dump a token and related links."""
    # Let block.
    node_id = self.node.node_index if self.node else ''
    node_cn = self.node.__class__.__name__ if self.node else ''
    return (
        f"{self.line_number:4} "
        f"{node_id:5} {node_cn:16} "
        f"{self.index:&gt;5} {self.kind:&gt;15} "
        f"{self.show_val(100)}")
</t>
<t tx="ekr.20250426045002.137">def dump_header(self) -&gt; None:  # pragma: no cover
    """Print the header for token.dump"""
    print(
        f"\n"
        f"         node    {'':10} token {'':10}   token\n"
        f"line index class {'':10} index {'':10} kind value\n"
        f"==== ===== ===== {'':10} ===== {'':10} ==== =====\n")
</t>
<t tx="ekr.20250426045002.138">def error_dump(self) -&gt; str:  # pragma: no cover
    """Dump a token or result node for error message."""
    if self.node:
        node_id = obj_id(self.node)
        node_s = f"{node_id} {self.node.__class__.__name__}"
    else:
        node_s = "None"
    return (
        f"index: {self.index:&lt;3} {self.kind:&gt;12} {self.show_val(20):&lt;20} "
        f"{node_s}")
</t>
<t tx="ekr.20250426045002.139">def show_val(self, truncate_n: int) -&gt; str:  # pragma: no cover
    """Return the token.value field."""
    if self.kind in ('ws', 'indent'):
        val = str(len(self.value))
    elif self.kind == 'string' or self.kind.startswith('fstring'):
        # repr would be confusing.
        val = g.truncate(self.value, truncate_n)
    else:
        val = g.truncate(repr(self.value), truncate_n)
    return val
</t>
<t tx="ekr.20250426045002.14">def dump_ast(ast: Node, tag: str = 'dump_ast') -&gt; None:
    """Utility to dump an ast tree."""
    g.printObj(AstDumper().dump_ast(ast), tag=tag)
</t>
<t tx="ekr.20250426045002.140">class Tokenizer:

    """Create a list of Tokens from contents."""

    token_kind: str
    results: list[AnyToken] = []  # A list of Tokens or InputTokens.

    @others
</t>
<t tx="ekr.20250426045002.141">token_index = 0
prev_line_token = None

def add_token(self, kind: str, five_tuple: tuple, line: str, s_row: int, value: str) -&gt; None:
    """
    Add an InputToken to the results list.

    Subclasses could override this method to filter out specific tokens.
    """
    assert self.token_kind in ('Token', 'InputToken'), repr(self.token_kind)
    tok: Union[Token, InputToken]
    if self.token_kind == 'Token':
        tok = Token(kind, value)
    else:
        tok = InputToken(kind, value)
    tok.five_tuple = five_tuple
    tok.index = self.token_index
    # Bump the token index.
    self.token_index += 1
    tok.line = line
    tok.line_number = s_row
    self.results.append(tok)
</t>
<t tx="ekr.20250426045002.142">def check_results(self, contents: str) -&gt; None:

    # Split the results into lines.
    result = ''.join([z.to_string() for z in self.results])
    result_lines = g.splitLines(result)
    # Check.
    ok = result == contents and result_lines == self.lines
    assert ok, (
        f"\n"
        f"      result: {result!r}\n"
        f"    contents: {contents!r}\n"
        f"result_lines: {result_lines}\n"
        f"       lines: {self.lines}"
    )
</t>
<t tx="ekr.20250426045002.143">def create_input_tokens(self, contents: str, tokens: Generator) -&gt; list[InputToken]:
    """
    Generate a list of Token's from tokens, a list of 5-tuples.
    """
    self.token_kind = 'InputToken'  # For add_token.
    # Create the physical lines.
    self.lines = contents.splitlines(True)
    # Create the list of character offsets of the start of each physical line.
    last_offset, self.offsets = 0, [0]
    for line in self.lines:
        last_offset += len(line)
        self.offsets.append(last_offset)
    # Handle each token, appending tokens and between-token whitespace to results.
    self.prev_offset, self.results = -1, []
    for token in tokens:
        self.do_token(contents, token)
    # Print results when tracing.
    self.check_results(contents)
    # Return results, as a list.
    return self.results
</t>
<t tx="ekr.20250426045002.144">def create_tokens(self, contents: str, tokens: Generator) -&gt; list[Token]:
    """
    Generate a list of Token's from tokens, a list of 5-tuples.
    """
    self.token_kind = 'Token'  # For add_token.
    # Create the physical lines.
    self.lines = contents.splitlines(True)
    # Create the list of character offsets of the start of each physical line.
    last_offset, self.offsets = 0, [0]
    for line in self.lines:
        last_offset += len(line)
        self.offsets.append(last_offset)
    # Handle each token, appending tokens and between-token whitespace to results.
    self.prev_offset, self.results = -1, []
    for token in tokens:
        self.do_token(contents, token)
    # Print results when tracing.
    self.check_results(contents)
    # Return results, as a list.
    return self.results
</t>
<t tx="ekr.20250426045002.145">header_has_been_shown = False

def do_token(self, contents: str, five_tuple: tuple) -&gt; None:
    """
    Handle the given token, optionally including between-token whitespace.

    This is part of the "gem".

    Links:

    - 11/13/19: ENB: A much better untokenizer
      https://groups.google.com/forum/#!msg/leo-editor/DpZ2cMS03WE/VPqtB9lTEAAJ

    - Untokenize does not round-trip ws before bs-nl
      https://bugs.python.org/issue38663
    """
    import token as token_module
    # Unpack..
    tok_type, val, start, end, line = five_tuple
    s_row, s_col = start  # row/col offsets of start of token.
    e_row, e_col = end  # row/col offsets of end of token.
    kind = token_module.tok_name[tok_type].lower()
    # Calculate the token's start/end offsets: character offsets into contents.
    s_offset = self.offsets[max(0, s_row - 1)] + s_col
    e_offset = self.offsets[max(0, e_row - 1)] + e_col
    # tok_s is corresponding string in the line.
    tok_s = contents[s_offset:e_offset]
    # Add any preceding between-token whitespace.
    ws = contents[self.prev_offset:s_offset]
    if ws:
        # No need for a hook.
        self.add_token('ws', five_tuple, line, s_row, ws)
    # Always add token, even if it contributes no text!
    self.add_token(kind, five_tuple, line, s_row, tok_s)
    # Update the ending offset.
    self.prev_offset = e_offset
</t>
<t tx="ekr.20250426045002.146">class TokenOrderGenerator:
    """
    A class that traverses ast (parse) trees in token order.

    Requires Python 3.9+.

    Overview: https://github.com/leo-editor/leo-editor/issues/1440#issue-522090981

    Theory of operation:
    - https://github.com/leo-editor/leo-editor/issues/1440#issuecomment-573661883
    - https://leo-editor.github.io/leo-editor/appendices.html#tokenorder-classes-theory-of-operation

    How to: https://leo-editor.github.io/leo-editor/appendices.html#tokenorder-class-how-to

    Project history: https://github.com/leo-editor/leo-editor/issues/1440#issuecomment-574145510
    """

    begin_end_stack: list[str] = []
    debug_flag: bool = False  # Set by 'debug' in trace_list kwarg.
    equal_sign_spaces = True  # A flag for orange.do_equal_op
    n_nodes = 0  # The number of nodes that have been visited.
    node_index = 0  # The index into the node_stack.
    node_stack: list[ast.AST] = []  # The stack of parent nodes.
    try_stack: list[str] = []  # A stack of either '' (Try) or '*' (TryStar)
    trace_token_method: bool = False  # True: trace the token method

    @others
</t>
<t tx="ekr.20250426045002.147"></t>
<t tx="ekr.20250426045002.148">def balance_tokens(self, tokens: list[Token]) -&gt; int:
    """
    TOG.balance_tokens.

    Insert two-way links between matching paren tokens.
    """
    count, stack = 0, []
    for token in tokens:
        if token.kind == 'op':
            if token.value == '(':
                count += 1
                stack.append(token.index)
            if token.value == ')':
                if stack:
                    index = stack.pop()
                    tokens[index].matching_paren = token.index
                    tokens[token.index].matching_paren = index
                else:  # pragma: no cover
                    g.trace(f"unmatched ')' at index {token.index}")
    if stack:  # pragma: no cover
        g.trace("unmatched '(' at {','.join(stack)}")
    return count
</t>
<t tx="ekr.20250426045002.149">def create_links(self, tokens: list[Token], tree: Node, file_name: str = '') -&gt; None:
    """
    A generator creates two-way links between the given tokens and ast-tree.

    Callers should call this generator with list(tog.create_links(...))

    The sync_tokens method creates the links and verifies that the resulting
    tree traversal generates exactly the given tokens in exact order.

    tokens: the list of Token instances for the input.
            Created by make_tokens().
    tree:   the ast tree for the input.
            Created by parse_ast().
    """
    # Init all ivars.
    self.equal_sign_spaces = True  # For a special case in set_links().
    self.file_name = file_name  # For tests.
    self.level = 0  # Python indentation level.
    self.node = None  # The node being visited.
    self.tokens = tokens  # The immutable list of input tokens.
    self.tree = tree  # The tree of ast.AST nodes.
    # Traverse the tree.
    self.visit(tree)
    # Ensure that all tokens are patched.
    self.node = tree
    self.token('endmarker', '')
</t>
<t tx="ekr.20250426045002.15">def dump_contents(contents: str, tag: str = 'Contents') -&gt; None:
    print('')
    print(f"{tag}...\n")
    for i, z in enumerate(g.splitLines(contents)):
        print(f"{i+1:&lt;3} ", z.rstrip())
    print('')
</t>
<t tx="ekr.20250426045002.150">def init_from_file(self, filename: str) -&gt; tuple[str, str, list[Token], Node]:  # pragma: no cover
    """
    Create the tokens and ast tree for the given file.
    Create links between tokens and the parse tree.
    Return (contents, encoding, tokens, tree).
    """
    self.level = 0
    self.filename = filename
    contents, encoding = g.readFileIntoString(filename)
    if not contents:
        return None, None, None, None
    self.tokens = tokens = self.make_tokens(contents)
    self.tree = tree = parse_ast(contents)
    self.create_links(tokens, tree)
    return contents, encoding, tokens, tree
</t>
<t tx="ekr.20250426045002.151">def init_from_string(self, contents: str, filename: str) -&gt; tuple[list[Token], Node]:  # pragma: no cover
    """
    Tokenize, parse and create links in the contents string.

    Return (tokens, tree).
    """
    self.filename = filename
    self.level = 0
    self.tokens = tokens = self.make_tokens(contents)
    self.tree = tree = parse_ast(contents)
    self.create_links(tokens, tree)
    return tokens, tree
</t>
<t tx="ekr.20250426045002.152">def make_tokens(self, contents: str) -&gt; list[Token]:
    """
    Return a list (not a generator) of Token objects corresponding to the
    list of 5-tuples generated by tokenize.tokenize.

    Perform consistency checks and handle all exceptions.
    """

    def check(contents: str, tokens: list[Token]) -&gt; bool:
        result = tokens_to_string(tokens)
        ok = result == contents
        if not ok:
            print('\nRound-trip check FAILS')
            print('Contents...\n')
            g.printObj(contents)
            print('\nResult...\n')
            g.printObj(result)
        return ok

    try:
        five_tuples = tokenize.tokenize(
            io.BytesIO(contents.encode('utf-8')).readline)
    except Exception:
        print('make_tokens: exception in tokenize.tokenize')
        g.es_exception()
        return None
    tokens = Tokenizer().create_tokens(contents, five_tuples)
    assert check(contents, tokens)
    return tokens
</t>
<t tx="ekr.20250426045002.153"># The synchronizer sync tokens to nodes.
</t>
<t tx="ekr.20250426045002.154">def find_next_significant_token(self) -&gt; Optional[Token]:
    """
    Scan from *after* self.tokens[px] looking for the next significant
    token.

    Return the token, or None. Never change self.px.
    """
    px = self.px + 1
    while px &lt; len(self.tokens):
        token = self.tokens[px]
        px += 1
        if is_significant_token(token):
            return token
    # This will never happen, because endtoken is significant.
    return None  # pragma: no cover
</t>
<t tx="ekr.20250426045002.155">last_statement_node = None

def set_links(self, node: Node, token: Token) -&gt; None:
    """Make two-way links between token and the given node."""
    # Don't bother assigning comment, comma, parens, ws and endtoken tokens.
    if token.kind == 'comment':
        # Append the comment to node.comment_list.
        comment_list: list[Token] = getattr(node, 'comment_list', [])
        node.comment_list = comment_list + [token]
        return
    if token.kind in ('endmarker', 'ws'):
        return
    if token.kind == 'op' and token.value in ',()':
        return
    # *Always* remember the last statement.
    statement = find_statement_node(node)
    if statement:
        self.last_statement_node = statement
        assert not isinstance(self.last_statement_node, ast.Module)
    if token.node is not None:  # pragma: no cover
        line_s = f"line {token.line_number}:"
        raise AssignLinksError(
            'set_links\n'
            f"       file: {self.filename}\n"
            f"{line_s:&gt;12} {token.line.strip()}\n"
            f"token index: {self.px}\n"
            f"token.node is not None\n"
            f" token.node: {token.node.__class__.__name__}\n"
            f"    callers: {g.callers()}"
        )
    # Assign newlines to the previous statement node, if any.
    if token.kind in ('newline', 'nl'):
        # Set an *auxiliary* link for the split/join logic.
        # Do *not* set token.node!
        token.statement_node = self.last_statement_node
        return
    if is_significant_token(token):
        # Link the token to the ast node.
        token.node = node
        # Add the token to node's token_list.
        add_token_to_token_list(token, node)
        # Special case. Inject equal_sign_spaces into '=' tokens.
        if token.kind == 'op' and token.value == '=':
            token.equal_sign_spaces = self.equal_sign_spaces
</t>
<t tx="ekr.20250426045002.156">def name(self, val: str) -&gt; None:
    """Sync to the given name token."""
    aList = val.split('.')
    if len(aList) == 1:
        self.token('name', val)
    else:
        for i, part in enumerate(aList):
            self.token('name', part)
            if i &lt; len(aList) - 1:
                self.op('.')
</t>
<t tx="ekr.20250426045002.157">def op(self, val: str) -&gt; None:
    """
    Sync to the given operator.

    val may be '(' or ')' *only* if the parens *will* actually exist in the
    token list.
    """
    self.token('op', val)
</t>
<t tx="ekr.20250426045002.158">px = -1  # Index of the previously synced token.

def token(self, kind: str, val: str) -&gt; None:
    """
    Sync to a token whose kind &amp; value are given. The token need not be
    significant, but it must be guaranteed to exist in the token list.

    The checks in this method constitute a strong, ever-present, unit test.

    Scan the tokens *after* px, looking for a token T matching (kind, val).
    raise AssignLinksError if a significant token is found that doesn't match T.
    Otherwise:
    - Create two-way links between all assignable tokens between px and T.
    - Create two-way links between T and self.node.
    - Advance by updating self.px to point to T.
    """
    node, tokens = self.node, self.tokens
    assert isinstance(node, ast.AST), repr(node)

    if self.trace_token_method:  # A Superb trace.
        g.trace(
            f"px: {self.px:4} "
            f"node: {node.__class__.__name__:&lt;14} "
            f"significant? {int(is_significant(kind, val))} "
            f"{kind:&gt;10}: {val!r}")
    #
    # Step one: Look for token T.
    old_px = px = self.px + 1
    while px &lt; len(self.tokens):
        token = tokens[px]
        if (kind, val) == (token.kind, token.value):
            break  # Success.
        if kind == token.kind == 'number':
            val = token.value
            break  # Benign: use the token's value, a string, instead of a number.
        if is_significant_token(token):  # pragma: no cover
            line_s = f"line {token.line_number}:"
            val = str(val)  # for g.truncate.
            raise AssignLinksError(
                'tog.token\n'
                f"       file: {self.filename}\n"
                f"{line_s:&gt;12} {g.truncate(token.line.strip(), 40)!r}\n"
                f"Looking for: {kind}.{g.truncate(val, 40)!r}\n"
                f"      found: {token.kind}.{g.truncate(token.value, 40)!r}\n"
                f"token.index: {token.index}\n"
            )
        # Skip the insignificant token.
        px += 1
    else:  # pragma: no cover
        val = str(val)  # for g.truncate.
        raise AssignLinksError(
            'tog.token 2\n'
             f"       file: {self.filename}\n"
             f"Looking for: {kind}.{g.truncate(val, 40)}\n"
             f"      found: end of token list"
        )
    #
    # Step two: Assign *secondary* links only for newline tokens.
    #           Ignore all other non-significant tokens.
    while old_px &lt; px:
        token = tokens[old_px]
        old_px += 1
        if token.kind in ('comment', 'newline', 'nl'):
            self.set_links(node, token)
    #
    # Step three: Set links in the found token.
    token = tokens[px]
    self.set_links(node, token)
    #
    # Step four: Advance.
    self.px = px
</t>
<t tx="ekr.20250426045002.159">def string_helper(self, node: Node) -&gt; None:
    """
    Common string and f-string handling for Constant, JoinedStr and Str nodes.

    Handle all concatenated strings, that is, strings separated only by whitespace.
    """

    # The next significant token must be a string or f-string.
    message1 = f"Old token: self.px: {self.px} token @ px: {self.tokens[self.px]}\n"
    token = self.find_next_significant_token()
    message2 = f"New token: self.px: {self.px} token @ px: {self.tokens[self.px]}\n"
    fail_s = f"tog.string_helper: no string!\n{message1}{message2}"
    assert token and token.kind in ('string', 'fstring_start'), fail_s

    # Handle all adjacent strings.
    while token and token.kind in ('string', 'fstring_start'):
        if token.kind == 'string':
            self.token(token.kind, token.value)
        else:
            self.token(token.kind, token.value)
            self.sync_to_kind('fstring_end')
        # Check for concatenated strings.
        token = self.find_next_non_ws_token()
</t>
<t tx="ekr.20250426045002.16">def dump_lines(tokens: list[Token], tag: str = 'Token lines') -&gt; None:
    print('')
    print(f"{tag}...\n")
    for z in tokens:
        if z.line.strip():
            print(z.line.rstrip())
        else:
            print(repr(z.line))
    print('')
</t>
<t tx="ekr.20250426045002.160">def sync_to_kind(self, kind: str) -&gt; None:
    """Sync to the next significant token of the given kind."""
    assert is_significant_kind(kind), repr(kind)
    while next_token := self.find_next_significant_token():
        self.token(next_token.kind, next_token.value)
        if next_token.kind in (kind, 'endtoken'):
            break

</t>
<t tx="ekr.20250426045002.161">def find_next_non_ws_token(self) -&gt; Optional[Token]:
    """
    Scan from *after* self.tokens[px] looking for the next token that isn't
    whitespace.

    Return the token, or None. Never change self.px.
    """
    px = self.px + 1
    while px &lt; len(self.tokens):
        token = self.tokens[px]
        px += 1
        if token.kind not in ('comment', 'encoding', 'indent', 'newline', 'nl', 'ws'):
            return token

    # This should never happen: endtoken isn't whitespace.
    return None  # pragma: no cover
</t>
<t tx="ekr.20250426045002.162"></t>
<t tx="ekr.20250426045002.163">def enter_node(self, node: Node) -&gt; None:
    """Enter a node."""
    # Update the stats.
    self.n_nodes += 1
    # Do this first, *before* updating self.node.
    node.parent = self.node
    if self.node:
        children: list[Node] = getattr(self.node, 'children', [])
        children.append(node)
        self.node.children = children
    # Inject the node_index field.
    assert not hasattr(node, 'node_index'), g.callers()
    node.node_index = self.node_index
    self.node_index += 1
    # begin_visitor and end_visitor must be paired.
    self.begin_end_stack.append(node.__class__.__name__)
    # Push the previous node.
    self.node_stack.append(self.node)
    # Update self.node *last*.
    self.node = node
</t>
<t tx="ekr.20250426045002.164">def leave_node(self, node: Node) -&gt; None:
    """Leave a visitor."""
    # begin_visitor and end_visitor must be paired.
    entry_name = self.begin_end_stack.pop()
    assert entry_name == node.__class__.__name__, f"{entry_name!r} {node.__class__.__name__}"
    assert self.node == node, (repr(self.node), repr(node))
    # Restore self.node.
    self.node = self.node_stack.pop()
</t>
<t tx="ekr.20250426045002.165">def visit(self, node: Node) -&gt; None:
    """Given an ast node, return a *generator* from its visitor."""
    # This saves a lot of tests.
    if node is None:
        return
    if 0:  # pragma: no cover
        # Keep this trace!
        cn = node.__class__.__name__ if node else ' '
        caller1, caller2 = g.callers(2).split(',')
        g.trace(f"{caller1:&gt;15} {caller2:&lt;14} {cn}")
    # More general, more convenient.
    if isinstance(node, (list, tuple)):
        for z in node or []:
            if isinstance(z, ast.AST):
                self.visit(z)
            else:  # pragma: no cover
                # Some fields may contain ints or strings.
                assert isinstance(z, (int, str)), z.__class__.__name__
        return
    # We *do* want to crash if the visitor doesn't exist.
    method = getattr(self, 'do_' + node.__class__.__name__)
    # Don't even *think* about removing the parent/child links.
    # The nearest_common_ancestor function depends upon them.
    self.enter_node(node)
    method(node)
    self.leave_node(node)
</t>
<t tx="ekr.20250426045002.166"></t>
<t tx="ekr.20250426045002.167"># keyword arguments supplied to call (NULL identifier for **kwargs)

# keyword = (identifier? arg, expr value)

def do_keyword(self, node: Node) -&gt; None:  # pragma: no cover
    """A keyword arg in an ast.Call."""
    # This should never be called.
    # tog.handle_call_arguments calls self.visit(kwarg_arg.value) instead.
    filename = getattr(self, 'filename', '&lt;no file&gt;')
    raise AssignLinksError(
        f"do_keyword called: {g.callers(8)}\n"
        f"file: {filename}\n"
    )
</t>
<t tx="ekr.20250426045002.168"></t>
<t tx="ekr.20250426045002.169"># arg = (identifier arg, expr? annotation)

def do_arg(self, node: Node) -&gt; None:
    """This is one argument of a list of ast.Function or ast.Lambda arguments."""
    self.name(node.arg)
    annotation = getattr(node, 'annotation', None)
    if annotation is not None:
        self.op(':')
        self.visit(node.annotation)
</t>
<t tx="ekr.20250426045002.17">def dump_results(tokens: list[Token], tag: str = 'Results') -&gt; None:
    print('')
    print(f"{tag}...\n")
    print(tokens_to_string(tokens))
    print('')
</t>
<t tx="ekr.20250426045002.170"># arguments = (
#       arg* posonlyargs, arg* args, arg? vararg, arg* kwonlyargs,
#       expr* kw_defaults, arg? kwarg, expr* defaults
# )

sync_equal_flag = False  # A small hack.

def do_arguments(self, node: Node) -&gt; None:
    """Arguments to ast.Function or ast.Lambda, **not** ast.Call."""
    #
    # No need to generate commas anywhere below.
    #
    # Let block. Some fields may not exist pre Python 3.8.
    n_plain = len(node.args) - len(node.defaults)
    posonlyargs = getattr(node, 'posonlyargs', [])
    vararg = getattr(node, 'vararg', None)
    kwonlyargs = getattr(node, 'kwonlyargs', [])
    kw_defaults = getattr(node, 'kw_defaults', [])
    kwarg = getattr(node, 'kwarg', None)
    # 1. Sync the position-only args.
    if posonlyargs:
        for z in posonlyargs:
            self.visit(z)
        self.op('/')
    # 2. Sync all args.
    for i, z in enumerate(node.args):
        assert isinstance(z, ast.arg)
        self.visit(z)
        if i &gt;= n_plain:
            old = self.equal_sign_spaces
            try:
                self.equal_sign_spaces = getattr(z, 'annotation', None) is not None
                self.op('=')
            finally:
                self.equal_sign_spaces = old
            self.visit(node.defaults[i - n_plain])
    # 3. Sync the vararg.
    if vararg:
        self.op('*')
        self.visit(vararg)
    # 4. Sync the keyword-only args.
    if kwonlyargs:
        if not vararg:
            self.op('*')
        for n, z in enumerate(kwonlyargs):
            self.visit(z)
            val = kw_defaults[n]
            if val is not None:
                self.op('=')
                self.visit(val)
    # 5. Sync the kwarg.
    if kwarg:
        self.op('**')
        self.visit(kwarg)


</t>
<t tx="ekr.20250426045002.171"># AsyncFunctionDef(identifier name, arguments args, stmt* body, expr* decorator_list,
#                expr? returns)

def do_AsyncFunctionDef(self, node: Node) -&gt; None:

    if node.decorator_list:
        for z in node.decorator_list:
            # '@%s\n'
            self.op('@')
            self.visit(z)
    # 'asynch def (%s): -&gt; %s\n'
    # 'asynch def %s(%s):\n'
    self.token('name', 'async')
    self.name('def')
    self.name(node.name)  # A string
    self.op('(')
    self.visit(node.args)
    self.op(')')
    returns = getattr(node, 'returns', None)
    if returns is not None:
        self.op('-&gt;')
        self.visit(node.returns)
    self.op(':')
    self.level += 1
    self.visit(node.body)
    self.level -= 1
</t>
<t tx="ekr.20250426045002.172">def do_ClassDef(self, node: Node) -&gt; None:

    for z in node.decorator_list or []:
        # @{z}\n
        self.op('@')
        self.visit(z)
    # class name(bases):\n
    self.name('class')
    self.name(node.name)  # A string.
    if node.bases:
        self.op('(')
        self.visit(node.bases)
        self.op(')')
    self.op(':')
    # Body...
    self.level += 1
    self.visit(node.body)
    self.level -= 1
</t>
<t tx="ekr.20250426045002.173"># FunctionDef(
#   identifier name,
#   arguments args,
#   stmt* body,
#   expr* decorator_list,
#   expr? returns,
#   string? type_comment)

def do_FunctionDef(self, node: Node) -&gt; None:

    # Guards...
    returns = getattr(node, 'returns', None)
    # Decorators...
        # @{z}\n
    for z in node.decorator_list or []:
        self.op('@')
        self.visit(z)
    # Signature...
        # def name(args): -&gt; returns\n
        # def name(args):\n
    self.name('def')
    self.name(node.name)  # A string.
    self.op('(')
    self.visit(node.args)
    self.op(')')
    if returns is not None:
        self.op('-&gt;')
        self.visit(node.returns)
    self.op(':')
    # Body...
    self.level += 1
    self.visit(node.body)
    self.level -= 1
</t>
<t tx="ekr.20250426045002.174">def do_Interactive(self, node: Node) -&gt; None:  # pragma: no cover

    self.visit(node.body)
</t>
<t tx="ekr.20250426045002.175">def do_Lambda(self, node: Node) -&gt; None:

    self.name('lambda')
    self.visit(node.args)
    self.op(':')
    self.visit(node.body)
</t>
<t tx="ekr.20250426045002.176">def do_Module(self, node: Node) -&gt; None:

    # Encoding is a non-syncing statement.
    self.visit(node.body)
</t>
<t tx="ekr.20250426045002.177"></t>
<t tx="ekr.20250426045002.178">def do_Expr(self, node: Node) -&gt; None:
    """An outer expression."""
    # No need to put parentheses.
    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.179">def do_Expression(self, node: Node) -&gt; None:  # pragma: no cover
    """An inner expression."""
    # No need to put parentheses.
    self.visit(node.body)
</t>
<t tx="ekr.20250426045002.18">def dump_tokens(tokens: list[Token], tag: str = 'Tokens') -&gt; None:
    print('')
    print(f"{tag}...\n")
    if not tokens:
        return
    print("Note: values shown are repr(value) *except* for 'string' and 'fstring*' tokens.")
    tokens[0].dump_header()
    for z in tokens:
        print(z.dump())
    print('')
</t>
<t tx="ekr.20250426045002.180">def do_GeneratorExp(self, node: Node) -&gt; None:

    # '&lt;gen %s for %s&gt;' % (elt, ','.join(gens))
    # No need to put parentheses or commas.
    self.visit(node.elt)
    self.visit(node.generators)
</t>
<t tx="ekr.20250426045002.181"># NamedExpr(expr target, expr value)

def do_NamedExpr(self, node: Node) -&gt; None:  # Python 3.8+

    self.visit(node.target)
    self.op(':=')
    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.182"></t>
<t tx="ekr.20250426045002.183"># Attribute(expr value, identifier attr, expr_context ctx)

def do_Attribute(self, node: Node) -&gt; None:

    self.visit(node.value)
    self.op('.')
    self.name(node.attr)  # A string.
</t>
<t tx="ekr.20250426045002.184">def do_Bytes(self, node: Node) -&gt; None:

    """
    It's invalid to mix bytes and non-bytes literals, so just
    advancing to the next 'string' token suffices.
    """
    token = self.find_next_significant_token()
    self.token('string', token.value)
</t>
<t tx="ekr.20250426045002.185"># comprehension = (expr target, expr iter, expr* ifs, int is_async)

def do_comprehension(self, node: Node) -&gt; None:

    # No need to put parentheses.
    self.name('for')  # #1858.
    self.visit(node.target)  # A name
    self.name('in')
    self.visit(node.iter)
    for z in node.ifs or []:
        self.name('if')
        self.visit(z)
</t>
<t tx="ekr.20250426045002.186"># Constant(constant value, string? kind)

def do_Constant(self, node: Node) -&gt; None:
    """
    https://greentreesnakes.readthedocs.io/en/latest/nodes.html

    A constant. The value attribute holds the Python object it represents.
    This can be simple types such as a number, string or None, but also
    immutable container types (tuples and frozensets) if all of their
    elements are constant.
    """
    if node.value == Ellipsis:
        self.op('...')
    elif isinstance(node.value, str):
        self.string_helper(node)
    elif isinstance(node.value, int):
        # Look at the next token to distinguish 0/1 from True/False.
        token = self.find_next_significant_token()
        kind, value = token.kind, token.value
        assert kind in ('name', 'number'), (kind, value, g.callers())
        if kind == 'name':
            self.name(value)
        else:
            self.token(kind, repr(value))
    elif isinstance(node.value, float):
        self.token('number', repr(node.value))
    elif isinstance(node.value, bytes):
        self.do_Bytes(node)
    elif isinstance(node.value, tuple):
        self.do_Tuple(node)
    elif isinstance(node.value, frozenset):
        self.do_Set(node)
    elif node.value is None:
        self.name('None')
    else:
        # Unknown type.
        g.trace('----- Oops -----', repr(node), g.callers())
</t>
<t tx="ekr.20250426045002.187"># Dict(expr* keys, expr* values)

def do_Dict(self, node: Node) -&gt; None:

    assert len(node.keys) == len(node.values)
    self.op('{')
    # No need to put commas.
    for i, key in enumerate(node.keys):
        key, value = node.keys[i], node.values[i]
        self.visit(key)  # a Str node.
        self.op(':')
        if value is not None:
            self.visit(value)
    self.op('}')
</t>
<t tx="ekr.20250426045002.188"># DictComp(expr key, expr value, comprehension* generators)

# d2 = {val: key for key, val in d}

def do_DictComp(self, node: Node) -&gt; None:

    self.token('op', '{')
    self.visit(node.key)
    self.op(':')
    self.visit(node.value)
    for z in node.generators or []:
        self.visit(z)
        self.token('op', '}')
</t>
<t tx="ekr.20250426045002.189">def do_Ellipsis(self, node: Node) -&gt; None:  # pragma: no cover (Does not exist for python 3.8+)

    self.op('...')
</t>
<t tx="ekr.20250426045002.19">def dump_tree(tokens: list[Token], tree: Node, tag: str = 'Tree') -&gt; None:
    print('')
    print(f"{tag}...\n")
    print(AstDumper().dump_tree(tokens, tree))
</t>
<t tx="ekr.20250426045002.190"># https://docs.python.org/3/reference/expressions.html#slicings

# ExtSlice(slice* dims)

def do_ExtSlice(self, node: Node) -&gt; None:  # pragma: no cover (deprecated)

    # ','.join(node.dims)
    for i, z in enumerate(node.dims):
        self.visit(z)
        if i &lt; len(node.dims) - 1:
            self.op(',')
</t>
<t tx="ekr.20250426045002.191"># FormattedValue(expr value, int conversion, expr? format_spec)  Python 3.12+

def do_FormattedValue(self, node: Node) -&gt; None:  # pragma: no cover
    """
    This node represents the *components* of a *single* f-string.

    Happily, JoinedStr nodes *also* represent *all* f-strings.

    JoinedStr does *not* visit the FormattedValue node,
    so the TOG should *never* visit this node!
    """
    raise AssignLinksError(f"do_FormattedValue called: {g.callers()}")
</t>
<t tx="ekr.20250426045002.192">def do_Index(self, node: Node) -&gt; None:  # pragma: no cover (deprecated)

    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.193"># JoinedStr(expr* values)

def do_JoinedStr(self, node: Node) -&gt; None:
    """
    JoinedStr nodes represent at least one f-string and all other strings
    concatenated to it.

    Analyzing JoinedStr.values would be extremely tricky, for reasons that
    need not be explained here.

    Instead, we get the tokens *from the token list itself*!
    """

    # Everything in the JoinedStr tree is a string.
    # Do *not* call self.visit.

    # This works for all versions of Python!
    self.string_helper(node)
</t>
<t tx="ekr.20250426045002.194">def do_List(self, node: Node) -&gt; None:

    # No need to put commas.
    self.op('[')
    self.visit(node.elts)
    self.op(']')
</t>
<t tx="ekr.20250426045002.195"># ListComp(expr elt, comprehension* generators)

def do_ListComp(self, node: Node) -&gt; None:

    self.op('[')
    self.visit(node.elt)
    for z in node.generators:
        self.visit(z)
    self.op(']')
</t>
<t tx="ekr.20250426045002.196">def do_Name(self, node: Node) -&gt; None:

    self.name(node.id)

</t>
<t tx="ekr.20250426045002.197"># Set(expr* elts)

def do_Set(self, node: Node) -&gt; None:

    self.op('{')
    self.visit(node.elts)
    self.op('}')
</t>
<t tx="ekr.20250426045002.198"># SetComp(expr elt, comprehension* generators)

def do_SetComp(self, node: Node) -&gt; None:

    self.op('{')
    self.visit(node.elt)
    for z in node.generators or []:
        self.visit(z)
    self.op('}')
</t>
<t tx="ekr.20250426045002.199"># slice = Slice(expr? lower, expr? upper, expr? step)

def do_Slice(self, node: Node) -&gt; None:

    lower = getattr(node, 'lower', None)
    upper = getattr(node, 'upper', None)
    step = getattr(node, 'step', None)
    if lower is not None:
        self.visit(lower)
    # Always put the colon between upper and lower.
    self.op(':')
    if upper is not None:
        self.visit(upper)
    # Put the second colon if it exists in the token list.
    if step is None:
        token = self.find_next_significant_token()
        if token and token.value == ':':
            self.op(':')
    else:
        self.op(':')
        self.visit(step)
</t>
<t tx="ekr.20250426045002.2">"""
leoAst.py

The classes in this file unify python's token-based and ast-based worlds by
creating two-way links between tokens in the token list and ast nodes in
the parse tree. For more details, see the "Overview" section below.

See also leoTokens.py. It defines a Python beautifier that uses only
Python's tokenize module.

This commands in this file require Python 3.9 or above.

Please help combat the dreaded software rot by reporting any problems to
https://groups.google.com/g/leo-editor

**Stand-alone operation**

usage:
    python -m leo.core.leoAst --help
    python -m leo.core.leoAst --fstringify [ARGS] PATHS
    python -m leo.core.leoAst --fstringify-diff [ARGS] PATHS
    python -m leo.core.leoAst --orange [ARGS] PATHS
    python -m leo.core.leoAst --orange-diff [ARGS] PATHS
    python -m leo.core.leoAst --py-cov [ARGS]
    python -m leo.core.leoAst --pytest [ARGS]
    python -m leo.core.leoAst --unittest [ARGS]

examples:
    python -m leo.core.leoAst --orange --force --verbose PATHS
    python -m leo.core.leoAst --py-cov "-f TestOrange"
    python -m leo.core.leoAst --pytest "-f TestOrange"
    python -m leo.core.leoAst --unittest TestOrange

positional arguments:
  PATHS              directory or list of files

optional arguments:
  -h, --help         show this help message and exit
  --force            operate on all files. Otherwise operate only on modified files
  --fstringify       leonine fstringify
  --fstringify-diff  show fstringify diff
  --orange           leonine text formatter (Orange is the new Black)
  --orange-diff      show orange diff
  --py-cov           run pytest --cov on leoAst.py
  --pytest           run pytest on leoAst.py
  --unittest         run unittest on leoAst.py
  --verbose          verbose output


**Overview**

leoAst.py unifies python's token-oriented and ast-oriented worlds.

leoAst.py defines classes that create two-way links between tokens
created by python's tokenize module and parse tree nodes created by
python's ast module:

The Token Order Generator (TOG) class quickly creates the following
links:

- An *ordered* children array from each ast node to its children.

- A parent link from each ast.node to its parent.

- Two-way links between tokens in the token list, a list of Token
  objects, and the ast nodes in the parse tree:

  - For each token, token.node contains the ast.node "responsible" for
    the token.

  - For each ast node, node.first_i and node.last_i are indices into
    the token list. These indices give the range of tokens that can be
    said to be "generated" by the ast node.

Once the TOG class has inserted parent/child links, the Token Order
Traverser (TOT) class traverses trees annotated with parent/child
links extremely quickly.


**Applicability and importance**

Many python developers will find asttokens meets all their needs.
asttokens is well documented and easy to use. Nevertheless, two-way
links are significant additions to python's tokenize and ast modules:

- Links from tokens to nodes are assigned to the nearest possible ast
  node, not the nearest statement, as in asttokens. Links can easily
  be reassigned, if desired.

- The TOG and TOT classes are intended to be the foundation of tools
  such as fstringify and black.

- The TOG class solves real problems, such as:
  https://stackoverflow.com/questions/16748029/

**Historical note re Python 3.8**

In Python 3.8 *only*, syncing tokens will fail for function calls like:

    f(1, x=2, *[3, 4], y=5)

that is, for calls where keywords appear before non-keyword args.


**Figures of merit**

Simplicity: The code consists primarily of a set of generators, one
for every kind of ast node.

Speed: The TOG creates two-way links between tokens and ast nodes in
roughly the time taken by python's tokenize.tokenize and ast.parse
library methods. This is substantially faster than the asttokens,
black or fstringify tools. The TOT class traverses trees annotated
with parent/child links even more quickly.

Memory: The TOG class makes no significant demands on python's
resources. Generators add nothing to python's call stack.
TOG.node_stack is the only variable-length data. This stack resides in
python's heap, so its length is unimportant. In the worst case, it
might contain a few thousand entries. The TOT class uses no
variable-length data at all.

**Links**

Leo...
Ask for help:       https://groups.google.com/forum/#!forum/leo-editor
Report a bug:       https://github.com/leo-editor/leo-editor/issues
leoAst.py docs:     https://leo-editor.github.io/leo-editor/appendices.html#leoast-py

Other tools...
asttokens:          https://pypi.org/project/asttokens
black:              https://pypi.org/project/black/
fstringify:         https://pypi.org/project/fstringify/

Python modules...
tokenize.py:        https://docs.python.org/3/library/tokenize.html
ast.py              https://docs.python.org/3/library/ast.html

**Studying this file**

I strongly recommend that you use Leo when studying this code so that you
will see the file's intended outline structure.

Without Leo, you will see only special **sentinel comments** that create
Leo's outline structure. These comments have the form::

    `#@&lt;comment-kind&gt;:&lt;user-id&gt;.&lt;timestamp&gt;.&lt;number&gt;: &lt;outline-level&gt; &lt;headline&gt;`
"""
</t>
<t tx="ekr.20250426045002.20">def show_diffs(s1: str, s2: str, filename: str = '') -&gt; None:
    """Print diffs between strings s1 and s2."""
    lines = list(difflib.unified_diff(
        g.splitLines(s1),
        g.splitLines(s2),
        fromfile=f"Old {filename}",
        tofile=f"New {filename}",
    ))
    print('')
    tag = f"Diffs for {filename}" if filename else 'Diffs'
    g.printObj(lines, tag=tag)
</t>
<t tx="ekr.20250426045002.200"># DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14;
# use ast.Constant instead

if g.python_version_tuple &lt; (3, 12, 0):

    def do_Str(self, node: Node) -&gt; None:
        """This node represents a string constant."""
        self.string_helper(node)
</t>
<t tx="ekr.20250426045002.201"># Subscript(expr value, slice slice, expr_context ctx)

def do_Subscript(self, node: Node) -&gt; None:

    self.visit(node.value)
    self.op('[')
    self.visit(node.slice)
    self.op(']')
</t>
<t tx="ekr.20250426045002.202"># Tuple(expr* elts, expr_context ctx)

def do_Tuple(self, node: Node) -&gt; None:

    # Do not call op for parens or commas here.
    # They do not necessarily exist in the token list!
    self.visit(node.elts)
</t>
<t tx="ekr.20250426045002.203"></t>
<t tx="ekr.20250426045002.204">def do_BinOp(self, node: Node) -&gt; None:

    op_name_ = op_name(node.op)
    self.visit(node.left)
    self.op(op_name_)
    self.visit(node.right)
</t>
<t tx="ekr.20250426045002.205"># BoolOp(boolop op, expr* values)

def do_BoolOp(self, node: Node) -&gt; None:

    # op.join(node.values)
    op_name_ = op_name(node.op)
    for i, z in enumerate(node.values):
        self.visit(z)
        if i &lt; len(node.values) - 1:
            self.name(op_name_)
</t>
<t tx="ekr.20250426045002.206"># Compare(expr left, cmpop* ops, expr* comparators)

def do_Compare(self, node: Node) -&gt; None:

    assert len(node.ops) == len(node.comparators)
    self.visit(node.left)
    for i, z in enumerate(node.ops):
        op_name_ = op_name(node.ops[i])
        if op_name_ in ('not in', 'is not'):
            for z in op_name_.split(' '):
                self.name(z)
        elif op_name_.isalpha():
            self.name(op_name_)
        else:
            self.op(op_name_)
        self.visit(node.comparators[i])
</t>
<t tx="ekr.20250426045002.207">def do_UnaryOp(self, node: Node) -&gt; None:

    op_name_ = op_name(node.op)
    if op_name_.isalpha():
        self.name(op_name_)
    else:
        self.op(op_name_)
    self.visit(node.operand)
</t>
<t tx="ekr.20250426045002.208"># IfExp(expr test, expr body, expr orelse)

def do_IfExp(self, node: Node) -&gt; None:

    self.visit(node.body)
    self.name('if')
    self.visit(node.test)
    self.name('else')
    self.visit(node.orelse)
</t>
<t tx="ekr.20250426045002.209"></t>
<t tx="ekr.20250426045002.21"></t>
<t tx="ekr.20250426045002.210"># Starred(expr value, expr_context ctx)

def do_Starred(self, node: Node) -&gt; None:
    """A starred argument to an ast.Call"""
    self.op('*')
    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.211"># AnnAssign(expr target, expr annotation, expr? value, int simple)

def do_AnnAssign(self, node: Node) -&gt; None:

    # {node.target}:{node.annotation}={node.value}\n'
    self.visit(node.target)
    self.op(':')
    self.visit(node.annotation)
    if node.value is not None:  # #1851
        self.op('=')
        self.visit(node.value)
</t>
<t tx="ekr.20250426045002.212"># Assert(expr test, expr? msg)

def do_Assert(self, node: Node) -&gt; None:

    # Guards...
    msg = getattr(node, 'msg', None)
    # No need to put parentheses or commas.
    self.name('assert')
    self.visit(node.test)
    if msg is not None:
        self.visit(node.msg)
</t>
<t tx="ekr.20250426045002.213">def do_Assign(self, node: Node) -&gt; None:

    for z in node.targets:
        self.visit(z)
        self.op('=')
    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.214">def do_AsyncFor(self, node: Node) -&gt; None:

    # The def line...
    self.token('name', 'async')
    self.name('for')
    self.visit(node.target)
    self.name('in')
    self.visit(node.iter)
    self.op(':')
    # Body...
    self.level += 1
    self.visit(node.body)
    # Else clause...
    if node.orelse:
        self.name('else')
        self.op(':')
        self.visit(node.orelse)
    self.level -= 1
</t>
<t tx="ekr.20250426045002.215">def do_AsyncWith(self, node: Node) -&gt; None:

    self.token('name', 'async')
    self.do_With(node)
</t>
<t tx="ekr.20250426045002.216"># AugAssign(expr target, operator op, expr value)

def do_AugAssign(self, node: Node) -&gt; None:

    # %s%s=%s\n'
    op_name_ = op_name(node.op)
    self.visit(node.target)
    self.op(op_name_ + '=')
    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.217"># Await(expr value)

def do_Await(self, node: Node) -&gt; None:

    self.token('name', 'await')
    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.218">def do_Break(self, node: Node) -&gt; None:

    self.name('break')
</t>
<t tx="ekr.20250426045002.219"># Call(expr func, expr* args, keyword* keywords)

# Python 3 ast.Call nodes do not have 'starargs' or 'kwargs' fields.

def do_Call(self, node: Node) -&gt; None:

    # The calls to op(')') and op('(') do nothing by default.
    # Subclasses might handle them in an overridden tog.set_links.
    self.visit(node.func)
    self.op('(')
    # No need to generate any commas.
    self.handle_call_arguments(node)
    self.op(')')
</t>
<t tx="ekr.20250426045002.22">def find_anchor_token(node: Node, global_token_list: list[Token]) -&gt; Optional[Token]:
    """
    Return the anchor_token for node, a token such that token.node == node.

    The search starts at node, and then all the usual child nodes.
    """

    node1 = node

    def anchor_token(node: Node) -&gt; Optional[Token]:
        """Return the anchor token in node.token_list"""
        # Careful: some tokens in the token list may have been killed.
        for token in get_node_token_list(node, global_token_list):
            if is_ancestor(node1, token):
                return token
        return None

    # This table only has to cover fields for ast.Nodes that
    # won't have any associated token.

    fields = (
                    # Common...
        'elt', 'elts', 'body', 'value',  # Less common...
        'dims', 'ifs', 'names', 's',
        'test', 'values', 'targets',
    )
    while node:
        # First, try the node itself.
        token = anchor_token(node)
        if token:
            return token
        # Second, try the most common nodes w/o token_lists:
        if isinstance(node, ast.Call):
            node = node.func
        elif isinstance(node, ast.Tuple):
            node = node.elts  # type:ignore
        # Finally, try all other nodes.
        else:
            # This will be used rarely.
            for field in fields:
                node = getattr(node, field, None)
                if node:
                    token = anchor_token(node)
                    if token:
                        return token
            else:
                break
    return None
</t>
<t tx="ekr.20250426045002.220">def arg_helper(self, node: Union[Node, str]) -&gt; None:
    """
    Yield the node, with a special case for strings.
    """
    if isinstance(node, str):
        self.token('name', node)
    else:
        self.visit(node)
</t>
<t tx="ekr.20250426045002.221">def handle_call_arguments(self, node: Node) -&gt; None:
    """
    Generate arguments in the correct order.

    Call(expr func, expr* args, keyword* keywords)

    https://docs.python.org/3/reference/expressions.html#calls

    Warning: This code will fail on Python 3.8 only for calls
             containing kwargs in unexpected places.
    """
    # *args:    in node.args[]:     Starred(value=Name(id='args'))
    # *[a, 3]:  in node.args[]:     Starred(value=List(elts=[Name(id='a'), Num(n=3)])
    # **kwargs: in node.keywords[]: keyword(arg=None, value=Name(id='kwargs'))
    #
    # Scan args for *name or *List
    args = node.args or []
    keywords = node.keywords or []

    def get_pos(obj: Node) -&gt; tuple[int, int, Node]:
        line1 = getattr(obj, 'lineno', None)
        col1 = getattr(obj, 'col_offset', None)
        return line1, col1, obj

    def sort_key(aTuple: tuple) -&gt; int:
        line, col, obj = aTuple
        return line * 1000 + col

    if 0:  # pragma: no cover
        g.printObj([ast.dump(z) for z in args], tag='args')
        g.printObj([ast.dump(z) for z in keywords], tag='keywords')

    places = [get_pos(z) for z in args + keywords]
    places.sort(key=sort_key)
    ordered_args = [z[2] for z in places]
    for z in ordered_args:
        if isinstance(z, ast.Starred):
            self.op('*')
            self.visit(z.value)
        elif isinstance(z, ast.keyword):
            if getattr(z, 'arg', None) is None:
                self.op('**')
                self.arg_helper(z.value)
            else:
                self.arg_helper(z.arg)
                old = self.equal_sign_spaces
                try:
                    self.equal_sign_spaces = False
                    self.op('=')
                finally:
                    self.equal_sign_spaces = old
                self.arg_helper(z.value)
        else:
            self.arg_helper(z)
</t>
<t tx="ekr.20250426045002.222">def do_Continue(self, node: Node) -&gt; None:

    self.name('continue')
</t>
<t tx="ekr.20250426045002.223">def do_Delete(self, node: Node) -&gt; None:

    # No need to put commas.
    self.name('del')
    self.visit(node.targets)
</t>
<t tx="ekr.20250426045002.224">def do_ExceptHandler(self, node: Node) -&gt; None:

    # Except line...
    self.name('except')
    if self.try_stack[-1] == '*':
        self.op('*')
    if getattr(node, 'type', None):
        self.visit(node.type)
    if getattr(node, 'name', None):
        self.name('as')
        self.name(node.name)
    self.op(':')
    # Body...
    self.level += 1
    self.visit(node.body)
    self.level -= 1
</t>
<t tx="ekr.20250426045002.225">def do_For(self, node: Node) -&gt; None:

    # The def line...
    self.name('for')
    self.visit(node.target)
    self.name('in')
    self.visit(node.iter)
    self.op(':')
    # Body...
    self.level += 1
    self.visit(node.body)
    # Else clause...
    if node.orelse:
        self.name('else')
        self.op(':')
        self.visit(node.orelse)
    self.level -= 1
</t>
<t tx="ekr.20250426045002.226"># Global(identifier* names)

def do_Global(self, node: Node) -&gt; None:

    self.name('global')
    for z in node.names:
        self.name(z)
</t>
<t tx="ekr.20250426045002.227"># If(expr test, stmt* body, stmt* orelse)

def do_If(self, node: Node) -&gt; None:
    &lt;&lt; do_If docstring &gt;&gt;
    # Use the next significant token to distinguish between 'if' and 'elif'.
    token = self.find_next_significant_token()
    self.name(token.value)
    self.visit(node.test)
    self.op(':')
    #
    # Body...
    self.level += 1
    self.visit(node.body)
    self.level -= 1
    #
    # Else and elif clauses...
    if node.orelse:
        self.level += 1
        token = self.find_next_significant_token()
        if token.value == 'else':
            self.name('else')
            self.op(':')
            self.visit(node.orelse)
        else:
            self.visit(node.orelse)
        self.level -= 1
</t>
<t tx="ekr.20250426045002.228">"""
The parse trees for the following are identical!

  if 1:            if 1:
      pass             pass
  else:            elif 2:
      if 2:            pass
          pass

So there is *no* way for the 'if' visitor to disambiguate the above two
cases from the parse tree alone.

Instead, we scan the tokens list for the next 'if', 'else' or 'elif' token.
"""
</t>
<t tx="ekr.20250426045002.229">def do_Import(self, node: Node) -&gt; None:

    self.name('import')
    for alias in node.names:
        self.name(alias.name)
        if alias.asname:
            self.name('as')
            self.name(alias.asname)
</t>
<t tx="ekr.20250426045002.23">def find_paren_token(i: int, global_token_list: list[Token]) -&gt; int:
    """Return i of the next paren token, starting at tokens[i]."""
    while i &lt; len(global_token_list):
        token = global_token_list[i]
        if token.kind == 'op' and token.value in '()':
            return i
        if is_significant_token(token):
            break
        i += 1
    return None
</t>
<t tx="ekr.20250426045002.230"># ImportFrom(identifier? module, alias* names, int? level)

def do_ImportFrom(self, node: Node) -&gt; None:

    self.name('from')
    for _i in range(node.level):
        self.op('.')
    if node.module:
        self.name(node.module)
    self.name('import')
    # No need to put commas.
    for alias in node.names:
        if alias.name == '*':  # #1851.
            self.op('*')
        else:
            self.name(alias.name)
        if alias.asname:
            self.name('as')
            self.name(alias.asname)
</t>
<t tx="ekr.20250426045002.231"># Match(expr subject, match_case* cases)

# match_case = (pattern pattern, expr? guard, stmt* body)

# https://docs.python.org/3/reference/compound_stmts.html#match

def do_Match(self, node: Node) -&gt; None:

    cases = getattr(node, 'cases', [])
    self.name('match')
    self.visit(node.subject)
    self.op(':')
    for case in cases:
        self.visit(case)
</t>
<t tx="ekr.20250426045002.232">#  match_case = (pattern pattern, expr? guard, stmt* body)

def do_match_case(self, node: Node) -&gt; None:

    guard = getattr(node, 'guard', None)
    body = getattr(node, 'body', [])
    self.name('case')
    self.visit(node.pattern)
    if guard:
        self.name('if')
        self.visit(guard)
    self.op(':')
    for statement in body:
        self.visit(statement)
</t>
<t tx="ekr.20250426045002.233"># MatchAs(pattern? pattern, identifier? name)

def do_MatchAs(self, node: Node) -&gt; None:
    pattern = getattr(node, 'pattern', None)
    name = getattr(node, 'name', None)
    if pattern and name:
        self.visit(pattern)
        self.name('as')
        self.name(name)
    elif name:
        self.name(name)
    elif pattern:
        self.visit(pattern)  # pragma: no cover
    else:
        self.token('name', '_')
</t>
<t tx="ekr.20250426045002.234"># MatchClass(expr cls, pattern* patterns, identifier* kwd_attrs, pattern* kwd_patterns)

def do_MatchClass(self, node: Node) -&gt; None:

    patterns = getattr(node, 'patterns', [])
    kwd_attrs = getattr(node, 'kwd_attrs', [])
    kwd_patterns = getattr(node, 'kwd_patterns', [])
    self.visit(node.cls)
    self.op('(')
    for pattern in patterns:
        self.visit(pattern)
    for i, kwd_attr in enumerate(kwd_attrs):
        self.name(kwd_attr)  # a String.
        self.op('=')
        self.visit(kwd_patterns[i])
    self.op(')')
</t>
<t tx="ekr.20250426045002.235"># MatchMapping(expr* keys, pattern* patterns, identifier? rest)

def do_MatchMapping(self, node: Node) -&gt; None:
    keys = getattr(node, 'keys', [])
    patterns = getattr(node, 'patterns', [])
    rest = getattr(node, 'rest', None)
    self.op('{')
    for i, key in enumerate(keys):
        self.visit(key)
        self.op(':')
        self.visit(patterns[i])
    if rest:
        self.op('**')
        self.name(rest)  # A string.
    self.op('}')
</t>
<t tx="ekr.20250426045002.236"># MatchOr(pattern* patterns)

def do_MatchOr(self, node: Node) -&gt; None:
    patterns = getattr(node, 'patterns', [])
    for i, pattern in enumerate(patterns):
        if i &gt; 0:
            self.op('|')
        self.visit(pattern)
</t>
<t tx="ekr.20250426045002.237"># MatchSequence(pattern* patterns)

def do_MatchSequence(self, node: Node) -&gt; None:
    patterns = getattr(node, 'patterns', [])
    # Scan for the next '(' or '[' token, skipping the 'case' token.
    token = None
    for token in self.tokens[self.px + 1 :]:
        if token.kind == 'op' and token.value in '([':
            break
        if is_significant_token(token):
            # An implicit tuple: there is no '(' or '[' token.
            token = None
            break
    else:
        raise AssignLinksError('do_MatchSequence: Ill-formed tuple')  # pragma: no cover
    if token:
        self.op(token.value)
    for pattern in patterns:
        self.visit(pattern)
    if token:
        self.op(']' if token.value == '[' else ')')
</t>
<t tx="ekr.20250426045002.238"># MatchSingleton(constant value)

def do_MatchSingleton(self, node: Node) -&gt; None:
    """Match True, False or None."""
    # g.trace(repr(node.value))
    self.token('name', repr(node.value))
</t>
<t tx="ekr.20250426045002.239"># MatchStar(identifier? name)

def do_MatchStar(self, node: Node) -&gt; None:
    name = getattr(node, 'name', None)
    self.op('*')
    if name:
        self.name(name)
</t>
<t tx="ekr.20250426045002.24">def get_node_token_list(node: Node, global_tokens_list: list[Token]) -&gt; list[Token]:
    """
    tokens_list must be the global tokens list.
    Return the tokens assigned to the node, or [].
    """
    i = getattr(node, 'first_i', None)
    j = getattr(node, 'last_i', None)
    if i is None:
        assert j is None, g.callers()
        return []
    if False:
        name = node.__class__.__name__
        if abs(i - j) &gt; 3:
            tag = f"get_node_token_list: {name} {i}..{j}"
            g.printObj(global_tokens_list[i : j + 1], tag=tag)
        else:
            g.trace(f"{i!r:&gt;3}..{j!r:3} {name} {global_tokens_list[i : j + 1]}")
    return global_tokens_list[i : j + 1]
</t>
<t tx="ekr.20250426045002.240"># MatchValue(expr value)

def do_MatchValue(self, node: Node) -&gt; None:

    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.241"># Nonlocal(identifier* names)

def do_Nonlocal(self, node: Node) -&gt; None:

    # nonlocal %s\n' % ','.join(node.names))
    # No need to put commas.
    self.name('nonlocal')
    for z in node.names:
        self.name(z)
</t>
<t tx="ekr.20250426045002.242">def do_Pass(self, node: Node) -&gt; None:

    self.name('pass')
</t>
<t tx="ekr.20250426045002.243"># Raise(expr? exc, expr? cause)

def do_Raise(self, node: Node) -&gt; None:

    # No need to put commas.
    self.name('raise')
    exc = getattr(node, 'exc', None)
    cause = getattr(node, 'cause', None)
    tback = getattr(node, 'tback', None)
    self.visit(exc)
    if cause:
        self.name('from')  # #2446.
        self.visit(cause)
    self.visit(tback)
</t>
<t tx="ekr.20250426045002.244">def do_Return(self, node: Node) -&gt; None:

    self.name('return')
    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.245"># Try(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)

def do_Try(self, node: Node) -&gt; None:

    # Try line...
    self.name('try')
    self.op(':')
    # Body...
    self.level += 1
    self.visit(node.body)
    self.try_stack.append('')
    self.visit(node.handlers)
    self.try_stack.pop()
    # Else...
    if node.orelse:
        self.name('else')
        self.op(':')
        self.visit(node.orelse)
    # Finally...
    if node.finalbody:
        self.name('finally')
        self.op(':')
        self.visit(node.finalbody)
    self.level -= 1
</t>
<t tx="ekr.20250426045002.246"># TryStar(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)

# Examples:
#   except* SpamError:
#   except* FooError as e:
#   except* (BarError, BazError) as e:

def do_TryStar(self, node: Node) -&gt; None:

    # Try line...
    self.name('try')
    self.op(':')
    # Body...
    self.level += 1
    self.visit(node.body)
    self.try_stack.append('*')
    self.visit(node.handlers)
    self.try_stack.pop()
    # Else...
    if node.orelse:
        self.name('else')
        self.op(':')
        self.visit(node.orelse)
    # Finally...
    if node.finalbody:
        self.name('finally')
        self.op(':')
        self.visit(node.finalbody)
    self.level -= 1
</t>
<t tx="ekr.20250426045002.247">def do_While(self, node: Node) -&gt; None:

    # While line...
        # while %s:\n'
    self.name('while')
    self.visit(node.test)
    self.op(':')
    # Body...
    self.level += 1
    self.visit(node.body)
    # Else clause...
    if node.orelse:
        self.name('else')
        self.op(':')
        self.visit(node.orelse)
    self.level -= 1
</t>
<t tx="ekr.20250426045002.248"># With(withitem* items, stmt* body)

# withitem = (expr context_expr, expr? optional_vars)

def do_With(self, node: Node) -&gt; None:

    expr: Optional[ast.AST] = getattr(node, 'context_expression', None)
    items: list[ast.AST] = getattr(node, 'items', [])
    self.name('with')
    self.visit(expr)
    # No need to put commas.
    for item in items:
        self.visit(item.context_expr)
        optional_vars = getattr(item, 'optional_vars', None)
        if optional_vars is not None:
            self.name('as')
            self.visit(item.optional_vars)
    # End the line.
    self.op(':')
    # Body...
    self.level += 1
    self.visit(node.body)
    self.level -= 1
</t>
<t tx="ekr.20250426045002.249">def do_Yield(self, node: Node) -&gt; None:

    self.name('yield')
    if hasattr(node, 'value'):
        self.visit(node.value)
</t>
<t tx="ekr.20250426045002.25">def is_significant(kind: str, value: str) -&gt; bool:
    """
    Return True if (kind, value) represent a token that can be used for
    syncing generated tokens with the token list.
    """
    # Making 'endmarker' significant ensures that all tokens are synced.
    return (
        kind in ('async', 'await', 'endmarker', 'name', 'number', 'string')
        or kind.startswith('fstring')
        or kind == 'op' and value not in ',;()')

def is_significant_kind(kind: str) -&gt; bool:
    return (
        kind in ('async', 'await', 'endmarker', 'name', 'number', 'string')
        or kind.startswith('fstring')
    )

def is_significant_token(token: Token) -&gt; bool:
    """Return True if the given token is a synchronizing token"""
    return is_significant(token.kind, token.value)
</t>
<t tx="ekr.20250426045002.250"># YieldFrom(expr value)

def do_YieldFrom(self, node: Node) -&gt; None:

    self.name('yield')
    self.name('from')
    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.251"></t>
<t tx="ekr.20250426045002.252"># ParamSpec(identifier name)

def do_ParamSpec(self, node: Node) -&gt; None:

    self.visit(node.name)
</t>
<t tx="ekr.20250426045002.253"># TypeAlias(expr name, type_param* type_params, expr value)

def do_TypeAlias(self, node: Node) -&gt; None:

    params = getattr(node, 'type_params', [])
    self.visit(node.name)
    for param in params:
        self.visit(param)
    self.visit(node.value)
</t>
<t tx="ekr.20250426045002.254">#  TypeVar(identifier name, expr? bound)

def do_TypeVar(self, node: Node) -&gt; None:

    bound = getattr(node, 'bound', None)
    self.visit(node.name)
    if bound:
        self.visit(bound)
</t>
<t tx="ekr.20250426045002.255"># TypeVarTuple(identifier name)

def do_TypeVarTuple(self, node: Node) -&gt; None:

    self.visit(node.name)
</t>
<t tx="ekr.20250426045002.256">def main() -&gt; None:  # pragma: no cover
    """Run commands specified by sys.argv."""
    args, settings_dict, arg_files = scan_ast_args()
    # Finalize arguments.
    cwd = os.getcwd()
    # Calculate requested files.
    requested_files: list[str] = []
    for path in arg_files:
        if path.endswith('.py'):
            requested_files.append(os.path.join(cwd, path))
        else:
            root_dir = os.path.join(cwd, path)
            requested_files.extend(glob.glob(f'{root_dir}**{os.sep}*.py', recursive=True))
    if not requested_files:
        print(f"No files in {arg_files!r}")
        return
    files: list[str]
    if args.force:
        # Handle all requested files.
        files = requested_files
    else:
        # Handle only modified files.
        modified_files = g.getModifiedFiles(cwd)
        files = [z for z in requested_files if os.path.abspath(z) in modified_files]
    if not files:
        return
    if args.verbose:
        kind = (
            'fstringify' if args.f else
            'fstringify-diff' if args.fd else
            'orange' if args.o else
            'orange-diff' if args.od else
            None
        )
        if kind and kind != 'orange':
            n = len(files)
            n_s = f" {n:&gt;3} file" if n == 1 else f"{n:&gt;3} files"
            print(f"{kind}: {n_s} in {', '.join(arg_files)}")
    # Do the command.
    if args.f:
        fstringify_command(files)
    if args.fd:
        fstringify_diff_command(files)
    if args.o:
        orange_command(arg_files, files, settings_dict)
    if args.od:
        orange_diff_command(files, settings_dict)
</t>
<t tx="ekr.20250426045002.257">def scan_ast_args() -&gt; tuple[object, dict[str, object], list[str]]:
    description = textwrap.dedent("""\
        Execute fstringify or beautify commands contained in leoAst.py.
    """)
    parser = argparse.ArgumentParser(
        description=description,
        formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument('PATHS', nargs='*', help='directory or list of files')
    group = parser.add_mutually_exclusive_group(required=False)  # Don't require any args.
    add = group.add_argument
    add('--fstringify', dest='f', action='store_true',
        help='fstringify PATHS')
    add('--fstringify-diff', dest='fd', action='store_true',
        help='fstringify diff PATHS')
    add('--orange', dest='o', action='store_true',
        help='beautify PATHS')
    add('--orange-diff', dest='od', action='store_true',
        help='diff beautify PATHS')
    # New arguments.
    add2 = parser.add_argument
    add2('--allow-joined', dest='allow_joined', action='store_true',
        help='allow joined strings')
    add2('--max-join', dest='max_join', metavar='N', type=int,
        help='max unsplit line length (default 0)')
    add2('--max-split', dest='max_split', metavar='N', type=int,
        help='max unjoined line length (default 0)')
    add2('--tab-width', dest='tab_width', metavar='N', type=int,
        help='tab-width (default -4)')
    # Newer arguments.
    add2('--force', dest='force', action='store_true',
        help='force beautification of all files')
    add2('--verbose', dest='verbose', action='store_true',
        help='verbose (per-file) output')
    # Create the return values, using EKR's prefs as the defaults.
    parser.set_defaults(
        allow_joined=False,
        force=False,
        max_join=0,
        max_split=0,
        recursive=False,
        tab_width=4,
        verbose=False
    )
    args: object = parser.parse_args()
    files = args.PATHS
    # Create the settings dict, ensuring proper values.
    settings_dict: dict[str, object] = {
        'allow_joined_strings': bool(args.allow_joined),
        'force': bool(args.force),
        'max_join_line_length': abs(args.max_join),
        'max_split_line_length': abs(args.max_split),
        'tab_width': abs(args.tab_width),  # Must be positive!
        'verbose': bool(args.verbose),
    }
    return args, settings_dict, files
</t>
<t tx="ekr.20250426045002.26">def match_parens(filename: str, i: int, j: int, tokens: list[Token]) -&gt; int:
    """Match parens in tokens[i:j]. Return the new j."""
    if j &gt;= len(tokens):
        return len(tokens)
    # Calculate paren level...
    level = 0
    for n in range(i, j + 1):
        token = tokens[n]
        if token.kind == 'op' and token.value == '(':
            level += 1
        if token.kind == 'op' and token.value == ')':
            if level == 0:
                break
            level -= 1
    # Find matching ')' tokens *after* j.
    if level &gt; 0:
        while level &gt; 0 and j + 1 &lt; len(tokens):
            token = tokens[j + 1]
            if token.kind == 'op' and token.value == ')':
                level -= 1
            elif token.kind == 'op' and token.value == '(':
                level += 1
            elif is_significant_token(token):
                break
            j += 1
    if level != 0:  # pragma: no cover.
        line_n = tokens[i].line_number
        raise AssignLinksError(
            'In match_parens\n'
            f"Unmatched parens: level={level}\n"
            f"            file: {filename}\n"
            f"            line: {line_n}\n"
        )
    return j
</t>
<t tx="ekr.20250426045002.27">def output_tokens_to_string(tokens: list[OutputToken]) -&gt; str:
    """Return the string represented by the list of tokens."""
    if tokens is None:
        # This indicates an internal error.
        print('')
        g.trace('===== output token list is None ===== ')
        print('')
        return ''
    return ''.join([z.to_string() for z in tokens])
</t>
<t tx="ekr.20250426045002.28">def tokens_for_node(filename: str, node: Node, global_token_list: list[Token]) -&gt; list[Token]:
    """Return the list of all tokens descending from node."""
    # Find any token descending from node.
    token = find_anchor_token(node, global_token_list)
    if not token:
        if 0:  # A good trace for debugging.
            print('')
            g.trace('===== no tokens', node.__class__.__name__)
        return []
    assert is_ancestor(node, token)
    # Scan backward.
    i = first_i = token.index
    while i &gt;= 0:
        token2 = global_token_list[i - 1]
        if getattr(token2, 'node', None):
            if is_ancestor(node, token2):
                first_i = i - 1
            else:
                break
        i -= 1
    # Scan forward.
    j = last_j = token.index
    while j + 1 &lt; len(global_token_list):
        token2 = global_token_list[j + 1]
        if getattr(token2, 'node', None):
            if is_ancestor(node, token2):
                last_j = j + 1
            else:
                break
        j += 1
    last_j = match_parens(filename, first_i, last_j, global_token_list)
    results = global_token_list[first_i : last_j + 1]
    return results
</t>
<t tx="ekr.20250426045002.29">def tokens_to_string(tokens: list[Token]) -&gt; str:
    """Return the string represented by the list of tokens."""
    if tokens is None:
        # This indicates an internal error.
        print('')
        g.trace('===== No tokens ===== ')
        print('')
        return ''
    return ''.join([z.to_string() for z in tokens])
</t>
<t tx="ekr.20250426045002.3">from __future__ import annotations
import argparse
import ast
import difflib
import glob
import io
import os
import re
import sys
import textwrap
import time
import tokenize
from typing import Any, Generator, Optional, Union, TYPE_CHECKING

if TYPE_CHECKING:
    AnyToken = Any
    Node = ast.AST
    Value = Any
    Settings = Optional[dict[str, Value]]

try:
    from leo.core import leoGlobals as g
except Exception:
    # check_g function gives the message.
    g = None

</t>
<t tx="ekr.20250426045002.30">def input_tokens_to_string(tokens: list[InputToken]) -&gt; str:
    """Return the string represented by the list of tokens."""
    if tokens is None:
        # This indicates an internal error.
        print('')
        g.trace('===== input token list is None ===== ')
        print('')
        return ''
    return ''.join([z.to_string() for z in tokens])
</t>
<t tx="ekr.20250426045002.31"># General utility functions on tokens and nodes.
</t>
<t tx="ekr.20250426045002.32">def obj_id(obj: object) -&gt; str:
    """Return the last four digits of id(obj), for dumps &amp; traces."""
    return str(id(obj))[-4:]
</t>
<t tx="ekr.20250426045002.33">@nobeautify

# https://docs.python.org/3/library/ast.html

_op_names = {
    # Binary operators.
    'Add': '+',
    'BitAnd': '&amp;',
    'BitOr': '|',
    'BitXor': '^',
    'Div': '/',
    'FloorDiv': '//',
    'LShift': '&lt;&lt;',
    'MatMult': '@',  # Python 3.5.
    'Mod': '%',
    'Mult': '*',
    'Pow': '**',
    'RShift': '&gt;&gt;',
    'Sub': '-',
    # Boolean operators.
    'And': ' and ',
    'Or': ' or ',
    # Comparison operators
    'Eq': '==',
    'Gt': '&gt;',
    'GtE': '&gt;=',
    'In': ' in ',
    'Is': ' is ',
    'IsNot': ' is not ',
    'Lt': '&lt;',
    'LtE': '&lt;=',
    'NotEq': '!=',
    'NotIn': ' not in ',
    # Context operators.
    'AugLoad': '&lt;AugLoad&gt;',
    'AugStore': '&lt;AugStore&gt;',
    'Del': '&lt;Del&gt;',
    'Load': '&lt;Load&gt;',
    'Param': '&lt;Param&gt;',
    'Store': '&lt;Store&gt;',
    # Unary operators.
    'Invert': '~',
    'Not': ' not ',
    'UAdd': '+',
    'USub': '-',
}

def op_name(node: Node) -&gt; str:
    """Return the print name of an operator node."""
    class_name = node.__class__.__name__
    assert class_name in _op_names, repr(class_name)
    return _op_names[class_name].strip()
</t>
<t tx="ekr.20250426045002.34"></t>
<t tx="ekr.20250426045002.35">def make_tokens(contents: str) -&gt; list[InputToken]:
    """
    Return a list (not a generator) of Token objects corresponding to the
    list of 5-tuples generated by tokenize.tokenize.

    Perform consistency checks and handle all exceptions.

    Called from unit tests.
    """

    def check(contents: str, tokens: list[InputToken]) -&gt; bool:
        result = input_tokens_to_string(tokens)
        ok = result == contents
        if not ok:
            print('\nRound-trip check FAILS')
            print('Contents...\n')
            g.printObj(contents)
            print('\nResult...\n')
            g.printObj(result)
        return ok

    try:
        five_tuples = tokenize.tokenize(
            io.BytesIO(contents.encode('utf-8')).readline)
    except Exception:
        print('make_tokens: exception in tokenize.tokenize')
        g.es_exception()
        return None
    tokens = Tokenizer().create_input_tokens(contents, five_tuples)
    assert check(contents, tokens)
    return tokens
</t>
<t tx="ekr.20250426045002.36">def parse_ast(s: str) -&gt; Optional[Node]:
    """
    Parse string s, catching &amp; reporting all exceptions.
    Return the ast node, or None.
    """

    def oops(message: str) -&gt; None:
        print('')
        print(f"parse_ast: {message}")
        g.printObj(s)
        print('')

    try:
        s1 = g.toEncodedString(s)
        tree = ast.parse(s1, filename='before', mode='exec')
        return tree
    except IndentationError:
        oops('Indentation Error')
    except SyntaxError:
        oops('Syntax Error')
    except Exception:
        oops('Unexpected Exception')
        g.es_exception()
    return None
</t>
<t tx="ekr.20250426045002.37"># Functions that associate tokens with nodes.
</t>
<t tx="ekr.20250426045002.38">def find_statement_node(node: Node) -&gt; Optional[Node]:
    """
    Return the nearest statement node.
    Return None if node has only Module for a parent.
    """
    if isinstance(node, ast.Module):
        return None
    parent = node
    while parent:
        if is_statement_node(parent):
            return parent
        parent = parent.parent
    return None
</t>
<t tx="ekr.20250426045002.39">def is_ancestor(node: Node, token: Token) -&gt; bool:
    """Return True if node is an ancestor of token."""
    t_node = token.node
    if not t_node:
        assert token.kind == 'killed', repr(token)
        return False
    while t_node:
        if t_node == node:
            return True
        t_node = t_node.parent
    return False
</t>
<t tx="ekr.20250426045002.4"># Don't bother covering top-level commands.
if 1:  # pragma: no cover
    @others
</t>
<t tx="ekr.20250426045002.40">def is_long_statement(node: Node) -&gt; bool:
    """
    Return True if node is an instance of a node that might be split into
    shorter lines.
    """
    return isinstance(node, (
        ast.Assign, ast.AnnAssign, ast.AsyncFor, ast.AsyncWith, ast.AugAssign,
        ast.Call, ast.Delete, ast.ExceptHandler, ast.For, ast.Global,
        ast.If, ast.Import, ast.ImportFrom,
        ast.Nonlocal, ast.Return, ast.While, ast.With, ast.Yield, ast.YieldFrom))
</t>
<t tx="ekr.20250426045002.41">def is_statement_node(node: Node) -&gt; bool:
    """Return True if node is a top-level statement."""
    return is_long_statement(node) or isinstance(node, (
        ast.Break, ast.Continue, ast.Pass, ast.Try))
</t>
<t tx="ekr.20250426045002.42">def nearest_common_ancestor(node1: Node, node2: Node) -&gt; Optional[Node]:
    """
    Return the nearest common ancestor node for the given nodes.

    The nodes must have parent links.
    """

    def parents(node: Node) -&gt; list[Node]:
        aList = []
        while node:
            aList.append(node)
            node = node.parent
        return list(reversed(aList))

    result = None
    parents1 = parents(node1)
    parents2 = parents(node2)
    while parents1 and parents2:
        parent1 = parents1.pop(0)
        parent2 = parents2.pop(0)
        if parent1 == parent2:
            result = parent1
        else:
            break
    return result
</t>
<t tx="ekr.20250426045002.43"># Functions that replace tokens or nodes.
</t>
<t tx="ekr.20250426045002.44">def add_token_to_token_list(token: Token, node: Node) -&gt; None:
    """Insert token in the proper location of node.token_list."""

    # Note: get_node_token_list returns global_tokens_list[first_i : last_i + 1]

    if getattr(node, 'first_i', None) is None:
        node.first_i = node.last_i = token.index
    else:
        node.first_i = min(node.first_i, token.index)
        node.last_i = max(node.last_i, token.index)
</t>
<t tx="ekr.20250426045002.45">def replace_node(new_node: Node, old_node: Node) -&gt; None:
    """Replace new_node by old_node in the parse tree."""
    parent = old_node.parent
    new_node.parent = parent
    new_node.node_index = old_node.node_index
    children = parent.children
    i = children.index(old_node)
    children[i] = new_node
    fields = getattr(old_node, '_fields', None)
    if fields:
        for field in fields:
            field = getattr(old_node, field)
            if field == old_node:
                setattr(old_node, field, new_node)
                break
</t>
<t tx="ekr.20250426045002.46">def replace_token(token: Token, kind: str, value: str) -&gt; None:
    """Replace kind and value of the given token."""
    if token.kind in ('endmarker', 'killed'):
        return
    token.kind = kind
    token.value = value
    token.node = None  # Should be filled later.
</t>
<t tx="ekr.20250426045002.47">class AssignLinksError(Exception):
    """Assigning links to ast nodes failed."""

class AstNotEqual(Exception):
    """The two given AST's are not equivalent."""

class BeautifyError(Exception):
    """Leading tabs found."""

class FailFast(Exception):
    """Abort tests in TestRunner class."""
</t>
<t tx="ekr.20250426045002.48"></t>
<t tx="ekr.20250426045002.49">class AstDumper:  # pragma: no cover
    """A class supporting various kinds of dumps of ast nodes."""
    @others
</t>
<t tx="ekr.20250426045002.5">def fstringify_command(files: list[str]) -&gt; None:
    """
    Entry point for --fstringify.

    Fstringify the given file, overwriting the file.
    """
    if not check_g():
        return
    for filename in files:
        if os.path.exists(filename):
            print(f"fstringify {filename}")
            Fstringify().fstringify_file_silent(filename)
        else:
            print(f"file not found: {filename}")
</t>
<t tx="ekr.20250426045002.50">def dump_tree(self, tokens: list[Token], tree: Node) -&gt; str:
    """Briefly show a tree, properly indented."""
    self.tokens = tokens
    result = [self.show_header()]
    self.dump_tree_and_links_helper(tree, 0, result)
    return ''.join(result)
</t>
<t tx="ekr.20250426045002.51">def dump_tree_and_links_helper(self, node: Node, level: int, result: list[str]) -&gt; None:
    """Return the list of lines in result."""
    if node is None:
        return
    # Let block.
    indent = ' ' * 2 * level
    children: list[ast.AST] = getattr(node, 'children', [])
    node_s = self.compute_node_string(node, level)
    # Dump...
    if isinstance(node, (list, tuple)):
        for z in node:
            self.dump_tree_and_links_helper(z, level, result)
    elif isinstance(node, str):
        result.append(f"{indent}{node.__class__.__name__:&gt;8}:{node}\n")
    elif isinstance(node, ast.AST):
        # Node and parent.
        result.append(node_s)
        # Children.
        for z in children:
            self.dump_tree_and_links_helper(z, level + 1, result)
    else:
        result.append(node_s)
</t>
<t tx="ekr.20250426045002.52">def compute_node_string(self, node: Node, level: int) -&gt; str:
    """Return a string summarizing the node."""
    indent = ' ' * 2 * level
    parent = getattr(node, 'parent', None)
    node_id = getattr(node, 'node_index', '??')
    parent_id = getattr(parent, 'node_index', '??')
    parent_s = f"{parent_id:&gt;3}.{parent.__class__.__name__} " if parent else ''
    class_name = node.__class__.__name__
    descriptor_s = f"{node_id}.{class_name}: {self.show_fields(class_name, node, 20)}"
    tokens_s = self.show_tokens(node, 70, 100)
    lines = self.show_line_range(node)
    full_s1 = f"{parent_s:&lt;16} {lines:&lt;10} {indent}{descriptor_s} "
    node_s = f"{full_s1:&lt;62} {tokens_s}\n"
    return node_s
</t>
<t tx="ekr.20250426045002.53">def show_fields(self, class_name: str, node: Node, truncate_n: int) -&gt; str:
    """Return a string showing interesting fields of the node."""
    val = ''
    if class_name == 'JoinedStr':
        values = node.values
        assert isinstance(values, list)
        # Str tokens may represent *concatenated* strings.
        results = []
        fstrings, strings = 0, 0
        for z in values:
            if g.python_version_tuple &lt; (3, 12, 0):
                assert isinstance(z, (ast.FormattedValue, ast.Str))
                if isinstance(z, ast.Str):
                    results.append(z.s)
                    strings += 1
                else:
                    results.append(z.__class__.__name__)
                    fstrings += 1
            else:
                assert isinstance(z, (ast.FormattedValue, ast.Constant))
                if isinstance(z, ast.Constant):
                    results.append(z.value)
                    strings += 1
                else:
                    results.append(z.__class__.__name__)
                    fstrings += 1
        val = f"{strings} str, {fstrings} f-str"
    elif class_name == 'keyword':
        if isinstance(node.value, ast.Str):
            val = f"arg={node.arg}..Str.value.s={node.value.s}"
        elif isinstance(node.value, ast.Name):
            val = f"arg={node.arg}..Name.value.id={node.value.id}"
        else:
            val = f"arg={node.arg}..value={node.value.__class__.__name__}"
    elif class_name == 'Name':
        val = f"id={node.id!r}"
    elif class_name == 'NameConstant':
        val = f"value={node.value!r}"
    elif class_name == 'Num':
        val = f"n={node.n}"
    elif class_name == 'Starred':
        if isinstance(node.value, ast.Str):
            val = f"s={node.value.s}"
        elif isinstance(node.value, ast.Name):
            val = f"id={node.value.id}"
        else:
            val = f"s={node.value.__class__.__name__}"
    elif class_name == 'Str':
        val = f"s={node.s!r}"
    elif class_name in ('AugAssign', 'BinOp', 'BoolOp', 'UnaryOp'):  # IfExp
        name = node.op.__class__.__name__
        val = f"op={_op_names.get(name, name)}"
    elif class_name == 'Compare':
        ops = ','.join([op_name(z) for z in node.ops])
        val = f"ops='{ops}'"
    else:
        val = ''
    return g.truncate(val, truncate_n)
</t>
<t tx="ekr.20250426045002.54">def show_line_range(self, node: Node) -&gt; str:

    token_list = get_node_token_list(node, self.tokens)
    if not token_list:
        return ''
    min_ = min([z.line_number for z in token_list])
    max_ = max([z.line_number for z in token_list])
    return f"{min_}" if min_ == max_ else f"{min_}..{max_}"
</t>
<t tx="ekr.20250426045002.55">def show_tokens(self, node: Node, n: int, m: int) -&gt; str:
    """
    Return a string showing node.token_list.

    Split the result if n + len(result) &gt; m
    """
    token_list = get_node_token_list(node, self.tokens)
    result = []
    for z in token_list:
        val = None
        if z.kind == 'comment':
            val = g.truncate(z.value, 10)  # Short is good.
            result.append(f"{z.kind}.{z.index}({val})")
        elif z.kind == 'name':
            val = g.truncate(z.value, 20)
            result.append(f"{z.kind}.{z.index}({val})")
        elif z.kind == 'newline':
            result.append(f"{z.kind}.{z.index}")
        elif z.kind == 'number':
            result.append(f"{z.kind}.{z.index}({z.value})")
        elif z.kind == 'op':
            result.append(f"{z.kind}.{z.index}({z.value})")
        elif z.kind == 'string':
            val = g.truncate(z.value, 30)
            result.append(f"{z.kind}.{z.index}({val})")
        elif z.kind == 'ws':
            result.append(f"{z.kind}.{z.index}({len(z.value)})")
        else:
            # Indent, dedent, encoding, etc.
            # Don't put a blank.
            continue
        if result and result[-1] != ' ':
            result.append(' ')
    # split the line if it is too long.
    line, lines = [], []
    for r in result:
        line.append(r)
        if n + len(''.join(line)) &gt;= m:
            lines.append(''.join(line))
            line = []
    lines.append(''.join(line))
    pad = '\n' + ' ' * n
    return pad.join(lines)
</t>
<t tx="ekr.20250426045002.56">def show_header(self) -&gt; str:
    """Return a header string, but only the fist time."""
    return (
        f"{'parent':&lt;16} {'lines':&lt;10} {'node':&lt;34} {'tokens'}\n"
        f"{'======':&lt;16} {'=====':&lt;10} {'====':&lt;34} {'======'}\n")
</t>
<t tx="ekr.20250426045002.57">annotate_fields = False
include_attributes = False
indent_ws = ' '

def dump_ast(self, node: Node, level: int = 0) -&gt; str:
    """
    Dump an ast tree. Adapted from ast.dump.
    """
    sep1 = '\n%s' % (self.indent_ws * (level + 1))
    if isinstance(node, ast.AST):
        fields = [(a, self.dump_ast(b, level + 1)) for a, b in self.get_fields(node)]
        if self.include_attributes and node._attributes:
            fields.extend([(a, self.dump_ast(getattr(node, a), level + 1))
                for a in node._attributes])
        if self.annotate_fields:
            aList = ['%s=%s' % (a, b) for a, b in fields]
        else:
            aList = [b for a, b in fields]
        name = node.__class__.__name__
        sep = '' if len(aList) &lt;= 1 else sep1
        return '%s(%s%s)' % (name, sep, sep1.join(aList))
    if isinstance(node, list):
        sep = sep1
        return 'LIST[%s]' % ''.join(
            ['%s%s' % (sep, self.dump_ast(z, level + 1)) for z in node])
    return repr(node)
</t>
<t tx="ekr.20250426045002.58">def get_fields(self, node: Node) -&gt; Generator:

    return (
        (a, b) for a, b in ast.iter_fields(node)
            if a not in ['ctx',] and b not in (None, [])
    )
</t>
<t tx="ekr.20250426045002.59">class Fstringify:
    """A class to fstringify files."""

    silent = True  # for pytest. Defined in all entries.
    line_number = 0
    line = ''

    @others
</t>
<t tx="ekr.20250426045002.6">def fstringify_diff_command(files: list[str]) -&gt; None:
    """
    Entry point for --fstringify-diff.

    Print the diff that would be produced by fstringify.
    """
    if not check_g():
        return
    for filename in files:
        if os.path.exists(filename):
            print(f"fstringify-diff {filename}")
            Fstringify().fstringify_file_diff(filename)
        else:
            print(f"file not found: {filename}")
</t>
<t tx="ekr.20250426045002.60">def fstringify(self, contents: str, filename: str, tokens: list[Token], tree: Node) -&gt; str:
    """
    Fstringify.fstringify:

    f-stringify the sources given by (tokens, tree).

    Return the resulting string.
    """
    self.filename = filename
    self.tokens = tokens
    self.tree = tree
    # Prepass: reassign tokens.
    ReassignTokens().reassign(filename, tokens, tree)

    # Main pass.
    string_node = ast.Str if g.python_version_tuple &lt; (3, 12, 0) else ast.Constant
    for node in ast.walk(tree):
        if (
            isinstance(node, ast.BinOp)
            and op_name(node.op) == '%'
            and isinstance(node.left, string_node)
        ):
            self.make_fstring(node)
    results = tokens_to_string(self.tokens)
    return results
</t>
<t tx="ekr.20250426045002.61">def fstringify_file(self, filename: str) -&gt; bool:  # pragma: no cover
    """
    Fstringify.fstringify_file.

    The entry point for the fstringify-file command.

    f-stringify the given external file with the Fstrinfify class.

    Return True if the file was changed.
    """
    tag = 'fstringify-file'
    self.filename = filename
    self.silent = False
    tog = TokenOrderGenerator()
    try:
        contents, encoding, tokens, tree = tog.init_from_file(filename)
        if not contents or not tokens or not tree:
            print(f"{tag}: Can not fstringify: {filename}")
            return False
        results = self.fstringify(contents, filename, tokens, tree)
    except Exception as e:
        print(e)
        return False
    # Something besides newlines must change.
    changed = regularize_nls(contents) != regularize_nls(results)
    status = 'Wrote' if changed else 'Unchanged'
    print(f"{tag}: {status:&gt;9}: {filename}")
    if changed:
        write_file(filename, results, encoding=encoding)
    return changed
</t>
<t tx="ekr.20250426045002.62">def fstringify_file_diff(self, filename: str) -&gt; bool:  # pragma: no cover
    """
    Fstringify.fstringify_file_diff.

    The entry point for the diff-fstringify-file command.

    Print the diffs that would result from the fstringify-file command.

    Return True if the file would be changed.
    """
    tag = 'diff-fstringify-file'
    self.filename = filename
    self.silent = False
    tog = TokenOrderGenerator()
    try:
        contents, encoding, tokens, tree = tog.init_from_file(filename)
        if not contents or not tokens or not tree:
            return False
        results = self.fstringify(contents, filename, tokens, tree)
    except Exception as e:
        print(e)
        return False
    # Something besides newlines must change.
    changed = regularize_nls(contents) != regularize_nls(results)
    if changed:
        show_diffs(contents, results, filename=filename)
    else:
        print(f"{tag}: Unchanged: {filename}")
    return changed
</t>
<t tx="ekr.20250426045002.63">def fstringify_file_silent(self, filename: str) -&gt; bool:  # pragma: no cover
    """
    Fstringify.fstringify_file_silent.

    The entry point for the silent-fstringify-file command.

    fstringify the given file, suppressing all but serious error messages.

    Return True if the file would be changed.
    """
    self.filename = filename
    self.silent = True
    tog = TokenOrderGenerator()
    try:
        contents, encoding, tokens, tree = tog.init_from_file(filename)
        if not contents or not tokens or not tree:
            return False
        results = self.fstringify(contents, filename, tokens, tree)
    except Exception as e:
        print(e)
        return False
    # Something besides newlines must change.
    changed = regularize_nls(contents) != regularize_nls(results)
    status = 'Wrote' if changed else 'Unchanged'
    # Write the results.
    print(f"{status:&gt;9}: {filename}")
    if changed:
        write_file(filename, results, encoding=encoding)
    return changed
</t>
<t tx="ekr.20250426045002.64">def make_fstring(self, node: Node) -&gt; None:
    """
    node is BinOp node representing an '%' operator.
    node.left is an ast.Str or ast.Constant node.
    node.right represents the RHS of the '%' operator.

    Convert this tree to an f-string, if possible.
    Replace the node's entire tree with a new ast.Str node.
    Replace all the relevant tokens with a single new 'string' token.
    """
    trace = False
    string_node = ast.Str if g.python_version_tuple &lt; (3, 12, 0) else ast.Constant
    assert isinstance(node.left, string_node), (repr(node.left), g.callers())

    # Careful: use the tokens, not Str.s or Constant.value. This preserves spelling.
    lt_token_list = get_node_token_list(node.left, self.tokens)
    if not lt_token_list:  # pragma: no cover
        print('')
        g.trace('Error: no token list in Str')
        dump_tree(self.tokens, node)
        print('')
        return
    lt_s = tokens_to_string(lt_token_list)
    if trace:
        g.trace('lt_s:', lt_s)  # pragma: no cover
    # Get the RHS values, a list of token lists.
    values = self.scan_rhs(node.right)
    if trace:  # pragma: no cover
        for i, z in enumerate(values):
            dump_tokens(z, tag=f"RHS value {i}")
    # Compute rt_s, self.line and self.line_number for later messages.
    token0 = lt_token_list[0]
    self.line_number = token0.line_number
    self.line = token0.line.strip()
    rt_s = ''.join(tokens_to_string(z) for z in values)
    # Get the % specs in the LHS string.
    specs = self.scan_format_string(lt_s)
    if len(values) != len(specs):  # pragma: no cover
        self.message(
            f"can't create f-fstring: {lt_s!r}\n"
            f":f-string mismatch: "
            f"{len(values)} value{g.plural(len(values))}, "
            f"{len(specs)} spec{g.plural(len(specs))}")
        return
    # Replace specs with values.
    results = self.substitute_values(lt_s, specs, values)
    result = self.compute_result(lt_s, results)
    if not result:
        return
    # Remove whitespace before ! and :.
    result = self.clean_ws(result)
    # Show the results
    if trace:  # pragma: no cover
        before = (lt_s + ' % ' + rt_s).replace('\n', '&lt;NL&gt;')
        after = result.replace('\n', '&lt;NL&gt;')
        self.message(
            f"trace:\n"
            f":from: {before!s}\n"
            f":  to: {after!s}")
    # Adjust the tree and the token list.
    self.replace(node, result, values)
</t>
<t tx="ekr.20250426045002.65">ws_pat = re.compile(r'(\s+)([:!][0-9]\})')

def clean_ws(self, s: str) -&gt; str:
    """Carefully remove whitespace before ! and : specifiers."""
    s = re.sub(self.ws_pat, r'\2', s)
    return s
</t>
<t tx="ekr.20250426045002.66">def compute_result(self, lt_s: str, tokens: list[Token]) -&gt; str:
    """
    Create the final result, with various kinds of munges.

    Return the result string, or None if there are errors.
    """
    # Fail if there is a backslash within { and }.
    if not self.check_back_slashes(lt_s, tokens):
        return None  # pragma: no cover
    # Ensure consistent quotes.
    if not self.change_quotes(lt_s, tokens):
        return None  # pragma: no cover
    return tokens_to_string(tokens)
</t>
<t tx="ekr.20250426045002.67">def check_back_slashes(self, lt_s: str, tokens: list[Token]) -&gt; bool:
    """
    Return False if any backslash appears with an {} expression.

    Tokens is a list of tokens on the RHS.
    """
    count = 0
    for z in tokens:
        if z.kind == 'op':
            if z.value == '{':
                count += 1
            elif z.value == '}':
                count -= 1
        if (count % 2) == 1 and '\\' in z.value:
            if not self.silent:
                self.message(  # pragma: no cover (silent during unit tests)
                    f"can't create f-fstring: {lt_s!r}\n"
                    f":backslash in {{expr}}:")
            return False
    return True
</t>
<t tx="ekr.20250426045002.68">def change_quotes(self, lt_s: str, aList: list[Token]) -&gt; bool:
    """
    Carefully check quotes in all "inner" tokens as necessary.

    Return False if the f-string would contain backslashes.

    We expect the following "outer" tokens.

    aList[0]:  ('string', 'f')
    aList[1]:  ('string',  a single or double quote.
    aList[-1]: ('string', a single or double quote matching aList[1])
    """
    # Sanity checks.
    if len(aList) &lt; 4:
        return True  # pragma: no cover (defensive)
    if not lt_s:  # pragma: no cover (defensive)
        self.message("can't create f-fstring: no lt_s!")
        return False
    delim = lt_s[0]
    # Check tokens 0, 1 and -1.
    token0 = aList[0]
    token1 = aList[1]
    token_last = aList[-1]
    for token in token0, token1, token_last:
        # These are the only kinds of tokens we expect to generate.
        ok = (
            token.kind == 'string' or
            token.kind == 'op' and token.value in '{}')
        if not ok:  # pragma: no cover (defensive)
            self.message(
                f"unexpected token: {token.kind} {token.value}\n"
                f":           lt_s: {lt_s!r}")
            return False
    # These checks are important...
    if token0.value != 'f':
        return False  # pragma: no cover (defensive)
    val1 = token1.value
    if delim != val1:
        return False  # pragma: no cover (defensive)
    val_last = token_last.value
    if delim != val_last:
        return False  # pragma: no cover (defensive)
    #
    # Check for conflicting delims, preferring f"..." to f'...'.
    for delim in ('"', "'"):
        aList[1] = aList[-1] = Token('string', delim)
        for z in aList[2:-1]:
            if delim in z.value:
                break
        else:
            return True
    if not self.silent:  # pragma: no cover (silent unit test)
        self.message(
            f"can't create f-fstring: {lt_s!r}\n"
            f":   conflicting delims:")
    return False
</t>
<t tx="ekr.20250426045002.69">def munge_spec(self, spec: str) -&gt; tuple[str, str]:
    """
    Return (head, tail).

    The format is spec !head:tail or :tail

    Example specs: s2, r3
    """
    # To do: handle more specs.
    head, tail = [], []
    if spec.startswith('+'):
        pass  # Leave it alone!
    elif spec.startswith('-'):
        tail.append('&gt;')
        spec = spec[1:]
    if spec.endswith('s'):
        spec = spec[:-1]
    if spec.endswith('r'):
        head.append('r')
        spec = spec[:-1]
    tail_s = ''.join(tail) + spec
    head_s = ''.join(head)
    return head_s, tail_s
</t>
<t tx="ekr.20250426045002.7">def orange_command(
    arg_files: list[str], files: list[str], settings: Settings = None,
) -&gt; None:

    if not check_g():
        return
    t1 = time.process_time()
    any_changed = 0
    for filename in files:
        if os.path.exists(filename):
            # print(f"orange {filename}")
            changed = Orange(settings).beautify_file(filename)
            if changed:
                any_changed += 1
        else:
            print(f"file not found: {filename}")
    t2 = time.process_time()
    if any_changed or Orange(settings).verbose:
        n, files_s = any_changed, ','.join(arg_files)
        print(f"orange: {t2-t1:3.1f} sec. changed {n} file{g.plural(n)} in {files_s}")
</t>
<t tx="ekr.20250426045002.70"># format_spec ::=  [[fill]align][sign][#][0][width][,][.precision][type]
# fill        ::=  &lt;any character&gt;
# align       ::=  "&lt;" | "&gt;" | "=" | "^"
# sign        ::=  "+" | "-" | " "
# width       ::=  integer
# precision   ::=  integer
# type        ::=  "b" | "c" | "d" | "e" | "E" | "f" | "F" | "g" | "G" | "n" | "o" | "s" | "x" | "X" | "%"

format_pat = re.compile(r'%(([+-]?[0-9]*(\.)?[0.9]*)*[bcdeEfFgGnoxrsX]?)')

def scan_format_string(self, s: str) -&gt; list[re.Match]:
    """Scan the format string s, returning a list match objects."""
    result = list(re.finditer(self.format_pat, s))
    return result
</t>
<t tx="ekr.20250426045002.71">def scan_rhs(self, node: Node) -&gt; list[list[Token]]:
    """
    Scan the right-hand side of a potential f-string.

    Return a list of the token lists for each element.
    """
    trace = False
    # First, Try the most common cases.
    string_node = ast.Str if g.python_version_tuple &lt; (3, 12, 0) else ast.Constant
    if isinstance(node, string_node):
        token_list = get_node_token_list(node, self.tokens)
        return [token_list]
    if isinstance(node, (list, tuple, ast.Tuple)):
        result = []
        elts = node.elts if isinstance(node, ast.Tuple) else node
        for i, elt in enumerate(elts):
            tokens = tokens_for_node(self.filename, elt, self.tokens)
            result.append(tokens)
            if trace:  # pragma: no cover
                g.trace(f"item: {i}: {elt.__class__.__name__}")
                g.printObj(tokens, tag=f"Tokens for item {i}")
        return result
    # Now we expect only one result.
    tokens = tokens_for_node(self.filename, node, self.tokens)
    return [tokens]
</t>
<t tx="ekr.20250426045002.72">def substitute_values(self, lt_s: str, specs: list[re.Match], values: list[list[Token]]) -&gt; list[Token]:
    """
    Replace specifiers with values in lt_s string.

    Double { and } as needed.
    """
    i, results = 0, [Token('string', 'f')]
    for spec_i, m in enumerate(specs):
        value = tokens_to_string(values[spec_i])
        start, end, spec = m.start(0), m.end(0), m.group(1)
        if start &gt; i:
            val = lt_s[i:start].replace('{', '{{').replace('}', '}}')
            results.append(Token('string', val[0]))
            results.append(Token('string', val[1:]))
        head, tail = self.munge_spec(spec)
        results.append(Token('op', '{'))
        results.append(Token('string', value))
        if head:
            results.append(Token('string', '!'))
            results.append(Token('string', head))
        if tail:
            results.append(Token('string', ':'))
            results.append(Token('string', tail))
        results.append(Token('op', '}'))
        i = end
    # Add the tail.
    tail = lt_s[i:]
    if tail:
        tail = tail.replace('{', '{{').replace('}', '}}')
        results.append(Token('string', tail[:-1]))
        results.append(Token('string', tail[-1]))
    return results
</t>
<t tx="ekr.20250426045002.73">def message(self, message: str) -&gt; None:  # pragma: no cover.
    """
    Print one or more message lines aligned on the first colon of the message.
    """
    # Print a leading blank line.
    print('')
    # Calculate the padding.
    lines = g.splitLines(message)
    pad = max(lines[0].find(':'), 30)
    # Print the first line.
    z = lines[0]
    i = z.find(':')
    if i == -1:
        print(z.rstrip())
    else:
        print(f"{z[:i+2].strip():&gt;{pad+1}} {z[i+2:].strip()}")
    # Print the remaining message lines.
    for z in lines[1:]:
        if z.startswith('&lt;'):
            # Print left aligned.
            print(z[1:].strip())
        elif z.startswith(':') and -1 &lt; z[1:].find(':') &lt;= pad:
            # Align with the first line.
            i = z[1:].find(':')
            print(f"{z[1:i+2].strip():&gt;{pad+1}} {z[i+2:].strip()}")
        elif z.startswith('&gt;'):
            # Align after the aligning colon.
            print(f"{' ':&gt;{pad+2}}{z[1:].strip()}")
        else:
            # Default: Put the entire line after the aligning colon.
            print(f"{' ':&gt;{pad+2}}{z.strip()}")
    # Print the standard message lines.
    file_s = f"{'file':&gt;{pad}}"
    ln_n_s = f"{'line number':&gt;{pad}}"
    line_s = f"{'line':&gt;{pad}}"
    print(
        f"{file_s}: {self.filename}\n"
        f"{ln_n_s}: {self.line_number}\n"
        f"{line_s}: {self.line!r}")
</t>
<t tx="ekr.20250426045002.74">def replace(self, node: Node, s: str, values: list[list[Token]]) -&gt; None:
    """
    Replace node with an ast.Str or ast.Constant node for s.
    Replace all tokens in the range of values with a single 'string' node.
    """
    # Replace the tokens...
    tokens = tokens_for_node(self.filename, node, self.tokens)
    i1 = i = tokens[0].index
    replace_token(self.tokens[i], 'string', s)
    j = 1
    while j &lt; len(tokens):
        replace_token(self.tokens[i1 + j], 'killed', '')
        j += 1
    # Replace the node.
    new_node: ast.AST
    if g.python_version_tuple &lt; (3, 12, 0):
        new_node = ast.Str(value=s)  # pylint: disable=deprecated-class
    else:
        new_node = ast.Constant(value=s)
    replace_node(new_node, node)
    # Update the token.
    token = self.tokens[i1]
    token.node = new_node
    # Update the token list.
    add_token_to_token_list(token, new_node)
</t>
<t tx="ekr.20250426045002.75">class InputToken:
    """
    A class representing an Orange input token.
    """

    def __init__(self, kind: str, value: str):

        self.kind = kind
        self.value = value
        self.five_tuple: tuple = None
        self.index = 0
        self.line = ''  # The entire line containing the token.
        self.line_number = 0  # The line number, for errors and dumps.
        self.level = 0
        self.node: Optional[Node] = None

    def __repr__(self) -&gt; str:  # pragma: no cover
        s = f"{self.index:&lt;3} {self.kind}"
        return f"Token {s}: {self.show_val(20)}"

    __str__ = __repr__


    def to_string(self) -&gt; str:
        """Return the contribution of the token to the source file."""
        return self.value if isinstance(self.value, str) else ''
    @others
</t>
<t tx="ekr.20250426045002.76">def brief_dump(self) -&gt; str:  # pragma: no cover
    """Dump a token."""
    return (
        f"{self.index:&gt;3} line: {self.line_number:&lt;2} "
        f"{self.kind:&gt;15} {self.show_val(100)}")
</t>
<t tx="ekr.20250426045002.77">def dump(self) -&gt; str:  # pragma: no cover
    """Dump a token and related links."""
    # Let block.
    node_id = self.node.node_index if self.node else ''
    node_cn = self.node.__class__.__name__ if self.node else ''
    return (
        f"{self.line_number:4} "
        f"{node_id:5} {node_cn:16} "
        f"{self.index:&gt;5} {self.kind:&gt;15} "
        f"{self.show_val(100)}")
</t>
<t tx="ekr.20250426045002.78">def dump_header(self) -&gt; None:  # pragma: no cover
    """Print the header for token.dump"""
    print(
        f"\n"
        f"         node    {'':10} token {'':10}   token\n"
        f"line index class {'':10} index {'':10} kind value\n"
        f"==== ===== ===== {'':10} ===== {'':10} ==== =====\n")
</t>
<t tx="ekr.20250426045002.79">def error_dump(self) -&gt; str:  # pragma: no cover
    """Dump a token or result node for error message."""
    if self.node:
        node_id = obj_id(self.node)
        node_s = f"{node_id} {self.node.__class__.__name__}"
    else:
        node_s = "None"
    return (
        f"index: {self.index:&lt;3} {self.kind:&gt;12} {self.show_val(20):&lt;20} "
        f"{node_s}")
</t>
<t tx="ekr.20250426045002.8">def orange_diff_command(files: list[str], settings: Settings = None) -&gt; None:

    if not check_g():
        return
    for filename in files:
        if os.path.exists(filename):
            print(f"orange-diff {filename}")
            Orange(settings).beautify_file_diff(filename)
        else:
            print(f"file not found: {filename}")
</t>
<t tx="ekr.20250426045002.80">def show_val(self, truncate_n: int) -&gt; str:  # pragma: no cover
    """Return the token.value field."""
    if self.kind in ('ws', 'indent'):
        val = str(len(self.value))
    elif self.kind == 'string' or self.kind.startswith('fstring'):
        # repr would be confusing.
        val = g.truncate(self.value, truncate_n)
    else:
        val = g.truncate(repr(self.value), truncate_n)
    return val
</t>
<t tx="ekr.20250426045002.81">class Orange:  # Orange is the new Black.
    """
    This class is deprecated. Use the TokenBasedOrange class in leoTokens.py

    This class is a demo of the TokenOrderGenerator class.

    This is a predominantly a *token-based* beautifier. However,
    orange.do_op, orange.colon, and orange.possible_unary_op use the parse
    tree to provide context that would otherwise be difficult to deduce.
    """
    # This switch is really a comment. It will always be false.
    # It marks the code that simulates the operation of the black tool.
    black_mode = False

    # Patterns...
    nobeautify_pat = re.compile(r'\s*#\s*pragma:\s*no\s*beautify\b|#\s*@@nobeautify')

    # Patterns from FastAtRead class, specialized for python delims.
    node_pat = re.compile(r'^(\s*)#@\+node:([^:]+): \*(\d+)?(\*?) (.*)$')  # @node
    start_doc_pat = re.compile(r'^\s*#@\+(at|doc)?(\s.*?)?$')  # @doc or @
    at_others_pat = re.compile(r'^(\s*)#@(\+|-)others\b(.*)$')  # @others

    # Doc parts end with @c or a node sentinel. Specialized for python.
    end_doc_pat = re.compile(r"^\s*#@(@(c(ode)?)|([+]node\b.*))$")
    @others
</t>
<t tx="ekr.20250426045002.82">def __init__(self, settings: Settings = None):
    """Ctor for Orange class."""
    if settings is None:
        settings = {}
    valid_keys = (
        'allow_joined_strings',
        'force',
        'max_join_line_length',
        'max_split_line_length',
        'orange',
        'tab_width',
        'verbose',
    )
    # For mypy...
    self.kind: str = ''
    # Default settings...
    self.allow_joined_strings = False  # EKR's preference.
    self.force = False
    self.max_join_line_length = 88
    self.max_split_line_length = 88
    self.tab_width = 4
    self.verbose = False
    # Override from settings dict...
    for key in settings:  # pragma: no cover
        value = settings.get(key)
        if key in valid_keys and value is not None:
            setattr(self, key, value)
        else:
            g.trace(f"Unexpected setting: {key} = {value!r}")
</t>
<t tx="ekr.20250426045002.83">def push_state(self, kind: str, value: Union[int, str] = None) -&gt; None:
    """Append a state to the state stack."""
    state = ParseState(kind, value)
    self.state_stack.append(state)
</t>
<t tx="ekr.20250426045002.84"></t>
<t tx="ekr.20250426045002.85">def oops(self) -&gt; None:  # pragma: no cover
    g.trace(f"Unknown kind: {self.kind}")

def beautify(self,
    contents: str,
    filename: str,
    tokens: list[Token],
    tree: Optional[Node] = None,
    max_join_line_length: Optional[int] = None,
    max_split_line_length: Optional[int] = None,
) -&gt; str:
    """
    The main line. Create output tokens and return the result as a string.

    beautify_file and beautify_file_def call this method.
    """
    # Config overrides
    if max_join_line_length is not None:
        self.max_join_line_length = max_join_line_length
    if max_split_line_length is not None:
        self.max_split_line_length = max_split_line_length
    # State vars...
    self.curly_brackets_level = 0  # Number of unmatched '{' tokens.
    self.decorator_seen = False  # Set by do_name for do_op.
    self.in_arg_list = 0  # &gt; 0 if in an arg list of a def.
    self.in_fstring = False  # True: scanning an f-string.
    self.level = 0  # Set only by do_indent and do_dedent.
    self.lws = ''  # Leading whitespace.
    self.paren_level = 0  # Number of unmatched '(' tokens.
    self.square_brackets_stack: list[bool] = []  # A stack of bools, for self.word().
    self.state_stack: list["ParseState"] = []  # Stack of ParseState objects.
    self.val = None  # The input token's value (a string).
    self.verbatim = False  # True: don't beautify.
    #
    # Init output list and state...
    self.code_list: list[OutputToken] = []  # The list of output tokens.
    self.tokens = tokens  # The list of input tokens.
    self.tree = tree
    self.add_token('file-start', '')
    self.push_state('file-start')
    for token in tokens:
        self.token = token
        self.kind, self.val, self.line = token.kind, token.value, token.line
        if self.verbatim:
            self.do_verbatim()
        elif self.in_fstring:
            self.continue_fstring()
        else:
            func = getattr(self, f"do_{token.kind}", self.oops)
            func()
    # Any post pass would go here.
    return output_tokens_to_string(self.code_list)
</t>
<t tx="ekr.20250426045002.86">def beautify_file(self, filename: str) -&gt; bool:  # pragma: no cover
    """
    Orange: Beautify the the given external file.

    Return True if the file was changed.
    """
    self.filename = filename
    # Annotate the tokens.
    tog = TokenOrderGenerator()
    contents, encoding, tokens, tree = tog.init_from_file(filename)
    if not contents or not tokens or not tree:
        return False  # Not an error.
    # Beautify.
    try:
        results = self.beautify(contents, filename, tokens, tree)  # type:ignore
    except BeautifyError:
        return False  # #2578.
    # Something besides newlines must change.
    if regularize_nls(contents) == regularize_nls(results):
        return False
    if 0:  # This obscures more import error messages.
        show_diffs(contents, results, filename=filename)
    # Write the results
    print(f"Beautified: {g.shortFileName(filename)}")
    write_file(filename, results, encoding=encoding)
    return True
</t>
<t tx="ekr.20250426045002.87">def beautify_file_diff(self, filename: str) -&gt; bool:  # pragma: no cover
    """
    Orange: Print the diffs that would result from the orange-file command.

    Return True if the file would be changed.
    """
    tag = 'diff-beautify-file'
    self.filename = filename

    if 1:  ### Legacy: use parse trees.
        tog = TokenOrderGenerator()
        contents, encoding, tokens, tree = tog.init_from_file(filename)
        if not contents or not tokens or not tree:
            print(f"{tag}: Can not beautify: {filename}")
            return False

    # Beautify
    results = self.beautify(contents, filename, tokens, tree)
    # Something besides newlines must change.
    if regularize_nls(contents) == regularize_nls(results):
        print(f"{tag}: Unchanged: {filename}")
        return False
    # Show the diffs.
    show_diffs(contents, results, filename=filename)
    return True
</t>
<t tx="ekr.20250426045002.88">def init_tokens_from_file(self, filename: str) -&gt; tuple[
    str, str, list[Token]
]:  # pragma: no cover
    """
    Create the list of tokens for the given file.
    Return (contents, encoding, tokens).
    """
    self.level = 0
    self.filename = filename
    contents, encoding = g.readFileIntoString(filename)
    if not contents:
        return None, None, None
    self.tokens = tokens = self.make_tokens(contents)
    return contents, encoding, tokens
</t>
<t tx="ekr.20250426045002.89">def make_tokens(self, contents: str) -&gt; list[Token]:
    """
    Return a list (not a generator) of Token objects corresponding to the
    list of 5-tuples generated by tokenize.tokenize.

    Perform consistency checks and handle all exceptions.
    """

    def check(contents: str, tokens: list[Token]) -&gt; bool:
        result = tokens_to_string(tokens)
        ok = result == contents
        if not ok:
            print('\nRound-trip check FAILS')
            print('Contents...\n')
            g.printObj(contents)
            print('\nResult...\n')
            g.printObj(result)
        return ok

    try:
        five_tuples = tokenize.tokenize(
            io.BytesIO(contents.encode('utf-8')).readline)
    except Exception:
        print('make_tokens: exception in tokenize.tokenize')
        g.es_exception()
        return None
    tokens = Tokenizer().create_tokens(contents, five_tuples)
    assert check(contents, tokens)
    return tokens
</t>
<t tx="ekr.20250426045002.9">if 1:  # pragma: no cover
    @others
</t>
<t tx="ekr.20250426045002.90"></t>
<t tx="ekr.20250426045002.91">in_doc_part = False

comment_pat = re.compile(r'^(\s*)#[^@!# \n]')

def do_comment(self) -&gt; None:
    """Handle a comment token."""
    val = self.val
    #
    # Leo-specific code...
    if self.node_pat.match(val):
        # Clear per-node state.
        self.in_doc_part = False
        self.verbatim = False
        self.decorator_seen = False
        # Do *not clear other state, which may persist across @others.
            # self.curly_brackets_level = 0
            # self.in_arg_list = 0
            # self.level = 0
            # self.lws = ''
            # self.paren_level = 0
            # self.square_brackets_stack = []
            # self.state_stack = []
    else:
        # Keep track of verbatim mode.
        if self.beautify_pat.match(val):
            self.verbatim = False
        elif self.nobeautify_pat.match(val):
            self.verbatim = True
        # Keep trace of @doc parts, to honor the convention for splitting lines.
        if self.start_doc_pat.match(val):
            self.in_doc_part = True
        if self.end_doc_pat.match(val):
            self.in_doc_part = False
    #
    # General code: Generate the comment.
    self.clean('blank')
    entire_line = self.line.lstrip().startswith('#')
    if entire_line:
        self.clean('hard-blank')
        self.clean('line-indent')
        # #1496: No further munging needed.
        val = self.line.rstrip()
        # #3056: Insure one space after '#' in non-sentinel comments.
        #        Do not change bang lines or '##' comments.
        if m := self.comment_pat.match(val):
            i = len(m.group(1))
            val = val[:i] + '# ' + val[i + 1 :]
    else:
        # Exactly two spaces before trailing comments.
        val = '  ' + self.val.rstrip()
    self.add_token('comment', val)
</t>
<t tx="ekr.20250426045002.92">def do_encoding(self) -&gt; None:
    """
    Handle the encoding token.
    """
    pass
</t>
<t tx="ekr.20250426045002.93">def do_endmarker(self) -&gt; None:
    """Handle an endmarker token."""
    # Ensure exactly one blank at the end of the file.
    self.clean_blank_lines()
    self.add_token('line-end', '\n')
</t>
<t tx="ekr.20250426045002.94">def do_fstring_start(self) -&gt; None:
    """Handle the 'fstring_start' token. Enter f-string mode."""
    self.in_fstring = True
    self.add_token('verbatim', self.val)

def continue_fstring(self) -&gt; None:
    """
    Put the next token in f-fstring mode.
    Exit f-string mode if the token is 'fstring_end'.
    """
    self.add_token('verbatim', self.val)
    if self.kind == 'fstring_end':
        self.in_fstring = False
</t>
<t tx="ekr.20250426045002.95"># Note: other methods use self.level.

def do_dedent(self) -&gt; None:
    """Handle dedent token."""
    self.level -= 1
    self.lws = self.level * self.tab_width * ' '
    self.line_indent()
    if self.black_mode:  # pragma: no cover (black)
        state = self.state_stack[-1]
        if state.kind == 'indent' and state.value == self.level:
            self.state_stack.pop()
            state = self.state_stack[-1]
            if state.kind in ('class', 'def'):
                self.state_stack.pop()
                self.handle_dedent_after_class_or_def(state.kind)

def do_indent(self) -&gt; None:
    """Handle indent token."""
    # #2578: Refuse to beautify files containing leading tabs or unusual indentation.
    consider_message = 'consider using python/Tools/scripts/reindent.py'
    if '\t' in self.val:  # pragma: no cover
        message = f"Leading tabs found: {self.filename}"
        print(message)
        print(consider_message)
        raise BeautifyError(message)
    if (len(self.val) % self.tab_width) != 0:  # pragma: no cover
        message = f" Indentation error: {self.filename}"
        print(message)
        print(consider_message)
        raise BeautifyError(message)
    new_indent = self.val
    old_indent = self.level * self.tab_width * ' '
    if new_indent &gt; old_indent:
        self.level += 1
    elif new_indent &lt; old_indent:  # pragma: no cover (defensive)
        g.trace('\n===== can not happen', repr(new_indent), repr(old_indent))
    self.lws = new_indent
    self.line_indent()
</t>
<t tx="ekr.20250426045002.96">def handle_dedent_after_class_or_def(self, kind: str) -&gt; None:  # pragma: no cover (black)
    """
    Insert blank lines after a class or def as the result of a 'dedent' token.

    Normal comment lines may precede the 'dedent'.
    Insert the blank lines *before* such comment lines.
    """
    #
    # Compute the tail.
    i = len(self.code_list) - 1
    tail: list[OutputToken] = []
    while i &gt; 0:
        t = self.code_list.pop()
        i -= 1
        if t.kind == 'line-indent':
            pass
        elif t.kind == 'line-end':
            tail.insert(0, t)
        elif t.kind == 'comment':
            # Only underindented single-line comments belong in the tail.
            # @+node comments must never be in the tail.
            single_line = self.code_list[i].kind in ('line-end', 'line-indent')
            lws = len(t.value) - len(t.value.lstrip())
            underindent = lws &lt;= len(self.lws)
            if underindent and single_line and not self.node_pat.match(t.value):
                # A single-line comment.
                tail.insert(0, t)
            else:
                self.code_list.append(t)
                break
        else:
            self.code_list.append(t)
            break
    #
    # Remove leading 'line-end' tokens from the tail.
    while tail and tail[0].kind == 'line-end':
        tail = tail[1:]
    #
    # Put the newlines *before* the tail.
    # For Leo, always use 1 blank lines.
    n = 1  # n = 2 if kind == 'class' else 1
    # Retain the token (intention) for debugging.
    self.add_token('blank-lines', n)
    for _i in range(0, n + 1):
        self.add_token('line-end', '\n')
    if tail:
        self.code_list.extend(tail)
    self.line_indent()
</t>
<t tx="ekr.20250426045002.97">def do_name(self) -&gt; None:
    """Handle a name token."""
    name = self.val
    if self.black_mode and name in ('class', 'def'):  # pragma: no cover (black)
        # Handle newlines before and after 'class' or 'def'
        self.decorator_seen = False
        state = self.state_stack[-1]
        if state.kind == 'decorator':
            # Always do this, regardless of @bool clean-blank-lines.
            self.clean_blank_lines()
            # Suppress split/join.
            self.add_token('hard-newline', '\n')
            self.add_token('line-indent', self.lws)
            self.state_stack.pop()
        else:
            # Always do this, regardless of @bool clean-blank-lines.
            self.blank_lines(2 if name == 'class' else 1)
        self.push_state(name)
        # For trailing lines after inner classes/defs.
        self.push_state('indent', self.level)
        self.word(name)
        return
    #
    # Leo mode...
    if name in ('class', 'def'):
        self.word(name)
    elif name in (
        'and', 'elif', 'else', 'for', 'if', 'in', 'not', 'not in', 'or', 'while'
    ):
        self.word_op(name)
    else:
        self.word(name)
</t>
<t tx="ekr.20250426045002.98">def do_newline(self) -&gt; None:
    """Handle a regular newline."""
    self.line_end()

def do_nl(self) -&gt; None:
    """Handle a continuation line."""
    self.line_end()
</t>
<t tx="ekr.20250426045002.99">def do_number(self) -&gt; None:
    """Handle a number token."""
    self.blank()
    self.add_token('number', self.val)
</t>
<t tx="ekr.20250426045416.1"># This file is part of Leo: https://leo-editor.github.io/leo-editor
# Leo's copyright notice is based on the MIT license:
# https://leo-editor.github.io/leo-editor/license.html

# This file may be compiled with mypyc as follows:
# python -m mypyc leo\core\leoTokens.py --strict-optional

&lt;&lt; leoTokens.py: docstring &gt;&gt;
&lt;&lt; leoTokens.py: imports &amp; annotations &gt;&gt;

@others

if __name__ == '__main__' or 'leoTokens' in __name__:
    main()  # pragma: no cover

@language python
@tabwidth -4
@pagewidth 70
</t>
<t tx="ekr.20250426045416.10">def beautify_file(filename: str) -&gt; bool:
    """
    Beautify the given file, writing it if has changed.
    """
    settings: SettingsDict = {
        'all': False,  # Don't beautify all files.
        'beautified': True,  # Report changed files.
        'diff': False,  # Don't show diffs.
        'report': True,  # Report changed files.
        'write': True,  # Write changed files.
    }
    tbo = TokenBasedOrange(settings)
    return tbo.beautify_file(filename)
</t>
<t tx="ekr.20250426045416.11">def main() -&gt; None:  # pragma: no cover
    """Run commands specified by sys.argv."""
    args, settings_dict, arg_files = scan_args()
    cwd = os.getcwd()

    # Calculate requested files.
    requested_files: list[str] = []
    for path in arg_files:
        if path.endswith('.py'):
            requested_files.append(os.path.join(cwd, path))
        else:
            root_dir = os.path.join(cwd, path)
            requested_files.extend(
                glob.glob(f'{root_dir}**{os.sep}*.py', recursive=True)
            )
    if not requested_files:
        # print(f"No files in {arg_files!r}")
        return

    # Calculate the actual list of files.
    modified_files = g.getModifiedFiles(cwd)

    def is_dirty(path: str) -&gt; bool:
        return os.path.abspath(path) in modified_files

    # Compute the files to be checked.
    if args.all:
        # Handle all requested files.
        to_be_checked_files = requested_files
    else:
        # Handle only modified files.
        to_be_checked_files = [z for z in requested_files if is_dirty(z)]

    # Compute the dirty files among the to-be-checked files.
    dirty_files = [z for z in to_be_checked_files if is_dirty(z)]

    # Do the command.
    if to_be_checked_files:
        orange_command(arg_files, requested_files, dirty_files, to_be_checked_files, settings_dict)
</t>
<t tx="ekr.20250426045416.12">def orange_command(
    arg_files: list[str],
    requested_files: list[str],
    dirty_files: list[str],
    to_be_checked_files: list[str],
    settings: Optional[SettingsDict] = None,
) -&gt; None:  # pragma: no cover
    """The outer level of the 'tbo/orange' command."""
    t1 = time.process_time()
    # n_tokens = 0
    n_beautified = 0
    if settings is None:
        settings = {}
    for filename in to_be_checked_files:
        if os.path.exists(filename):
            tbo = TokenBasedOrange(settings)
            beautified = tbo.beautify_file(filename)
            if beautified:
                n_beautified += 1
            # n_tokens += len(tbo.input_tokens)
        else:
            print(f"file not found: {filename}")
    # Report the results.
    t2 = time.process_time()
    if n_beautified or settings.get('report'):
        print(
            f"tbo: {t2-t1:4.2f} sec. "
            f"dirty: {len(dirty_files):&lt;3} "
            f"checked: {len(to_be_checked_files):&lt;3} "
            f"beautified: {n_beautified:&lt;3} in {','.join(arg_files)}"
        )
</t>
<t tx="ekr.20250426045416.13">def scan_args() -&gt; tuple[argparse.Namespace, SettingsDict, list[str]]:  # pragma: no cover
    description = textwrap.dedent(
    """Beautify or diff files""")
    parser = argparse.ArgumentParser(
        description=description,
        formatter_class=argparse.RawTextHelpFormatter,
    )
    parser.add_argument('PATHS', nargs='*', help='directory or list of files')
    add2 = parser.add_argument

    # Arguments.
    add2('-a', '--all', dest='all', action='store_true',
        help='Beautify all files, even unchanged files')
    add2('-b', '--beautified', dest='beautified', action='store_true',
        help='Report beautified files individually, even if not written')
    add2('-d', '--diff', dest='diff', action='store_true',
        help='show diffs instead of changing files')
    add2('-r', '--report', dest='report', action='store_true',
        help='show summary report')
    add2('-w', '--write', dest='write', action='store_true',
        help='write beautifed files (dry-run mode otherwise)')

    # Create the return values, using EKR's prefs as the defaults.
    parser.set_defaults(
        all=False, beautified=False, diff=False, report=False, write=False,
        tab_width=4,
    )
    args: argparse.Namespace = parser.parse_args()
    files = args.PATHS

    # Create the settings dict, ensuring proper values.
    settings_dict: SettingsDict = {
        'all': bool(args.all),
        'beautified': bool(args.beautified),
        'diff': bool(args.diff),
        'report': bool(args.report),
        'write': bool(args.write)
    }
    return args, settings_dict, files
</t>
<t tx="ekr.20250426045416.14"></t>
<t tx="ekr.20250426045416.15">class InternalBeautifierError(Exception):
    """
    An internal error in the beautifier.

    Errors in the user's source code may raise standard Python errors
    such as IndentationError or SyntaxError.
    """
</t>
<t tx="ekr.20250426045416.16">class InputToken:  # leoTokens.py.
    """A class representing a TBO input token."""

    __slots__ = (
        'context',
        'index',
        'kind',
        'line',
        'line_number',
        'value',
    )

    def __init__(
        self, kind: str, value: str, index: int, line: str, line_number: int,
    ) -&gt; None:
        self.context: Optional[str] = None
        self.index = index
        self.kind = kind
        self.line = line  # The entire line containing the token.
        self.line_number = line_number
        self.value = value

    def __repr__(self) -&gt; str:  # pragma: no cover
        s = f"{self.index:&lt;5} {self.kind:&gt;8}"
        return f"Token {s}: {self.show_val(20):22}"

    def __str__(self) -&gt; str:  # pragma: no cover
        s = f"{self.index:&lt;5} {self.kind:&gt;8}"
        return f"Token {s}: {self.show_val(20):22}"

    def to_string(self) -&gt; str:
        """Return the contribution of the token to the source file."""
        return self.value if isinstance(self.value, str) else ''

    @others
</t>
<t tx="ekr.20250426045416.17">def brief_dump(self) -&gt; str:  # pragma: no cover
    """Dump a token."""
    token_s = f"{self.kind:&gt;10} : {self.show_val(10):12}"
    return f"&lt;line: {self.line_number} index: {self.index:3} {token_s}&gt;"

</t>
<t tx="ekr.20250426045416.18">def dump(self) -&gt; str:  # pragma: no cover
    """Dump a token and related links."""
    return (
        f"{self.line_number:4} "
        f"{self.index:&gt;5} {self.kind:&gt;15} "
        f"{self.show_val(100)}"
    )
</t>
<t tx="ekr.20250426045416.19">def dump_header(self) -&gt; None:  # pragma: no cover
    """Print the header for token.dump"""
    print(
        f"\n"
        f"         node    {'':10} token {'':10}   token\n"
        f"line index class {'':10} index {'':10} kind value\n"
        f"==== ===== ===== {'':10} ===== {'':10} ==== =====\n")
</t>
<t tx="ekr.20250426045416.2">"""
leoTokens.py: A beautifier for Python that uses *only* tokens.

For help: `python -m leo.core.leoTokens --help`

Use Leo https://leo-editor.github.io/leo-editor/ to study this code!

Without Leo, you will see special **sentinel comments** that create
Leo's outline structure. These comments have the form::

    `#@&lt;comment-kind&gt;:&lt;user-id&gt;.&lt;timestamp&gt;.&lt;number&gt;: &lt;outline-level&gt; &lt;headline&gt;`
"""
</t>
<t tx="ekr.20250426045416.20">def error_dump(self) -&gt; str:  # pragma: no cover
    """Dump a token for error message."""
    return f"index: {self.index:&lt;3} {self.kind:&gt;12} {self.show_val(20):&lt;20}"
</t>
<t tx="ekr.20250426045416.21">def show_val(self, truncate_n: int = 8) -&gt; str:  # pragma: no cover
    """Return the token.value field."""
    if self.kind in ('dedent', 'indent', 'newline', 'ws'):
        # val = str(len(self.value))
        val = repr(self.value)
    elif self.kind == 'string' or self.kind.startswith('fstring'):
        # repr would be confusing.
        val = g.truncate(self.value, truncate_n)
    else:
        val = g.truncate(repr(self.value), truncate_n)
    return val
</t>
<t tx="ekr.20250426045416.22">class Tokenizer:
    """
    Use Python's tokenizer module to create InputTokens
    See: https://docs.python.org/3/library/tokenize.html
    """

    __slots__ = (
        'contents',
        'fstring_level',
        'fstring_line',
        'fstring_line_number',
        'fstring_values',
        'lines',
        'offsets',
        'prev_offset',
        'token_index',
        'token_list',
    )

    def __init__(self) -&gt; None:
        self.contents: str = ''
        self.offsets: list[int] = [0]  # Index of start of each line.
        self.prev_offset = -1
        self.token_index = 0
        self.token_list: list[InputToken] = []
        # Describing the scanned f-string...
        self.fstring_level: int = 0
        self.fstring_line: Optional[str] = None
        self.fstring_line_number: Optional[int] = None
        self.fstring_values: Optional[list[str]] = None

    @others
</t>
<t tx="ekr.20250426045416.23">def add_token(self, kind: str, line: str, line_number: int, value: str,) -&gt; None:
    """
    Add an InputToken to the token list.

    Convert (potentially nested!) fstrings to simple strings.
    """
    if kind == 'fstring_start':
        self.fstring_level += 1
        if self.fstring_level == 1:
            self.fstring_values = []
            self.fstring_line = line
            self.fstring_line_number = line_number
        self.fstring_values.append(value)
        return
    if self.fstring_level &gt; 0:
        # Accumulating an f-string.
        self.fstring_values.append(value)
        if kind == 'fstring_end':
            self.fstring_level -= 1
        if self.fstring_level &gt; 0:
            return
        # End of the outer f-string.
        # Create a single 'string' token from the saved values.
        kind = 'string'
        value = ''.join(self.fstring_values)
        # Use the line and line number of the 'string-start' token.
        line = self.fstring_line or ''
        line_number = self.fstring_line_number or 0
        # Clear the saved values.
        assert self.fstring_level == 0
        self.fstring_line = None
        self.fstring_line_number = None
        self.fstring_values = None

    tok = InputToken(kind, value, self.token_index, line, line_number)
    self.token_index += 1
    self.token_list.append(tok)


</t>
<t tx="ekr.20250426045416.24">def check_results(self, contents: str) -&gt; None:

    # Split the results into lines.
    result = ''.join([z.to_string() for z in self.token_list])
    result_lines = g.splitLines(result)
    # Check.
    ok = result == contents and result_lines == self.lines
    assert ok, (
        f"\n"
        f"      result: {result!r}\n"
        f"    contents: {contents!r}\n"
        f"result_lines: {result_lines}\n"
        f"       lines: {self.lines}"
    )
</t>
<t tx="ekr.20250426045416.25">def check_round_trip(self, contents: str, tokens: list[InputToken]) -&gt; bool:
    result = self.tokens_to_string(tokens)
    ok = result == contents
    if not ok:  # pragma: no cover
        print('\nRound-trip check FAILS')
        print('Contents...\n')
        print(contents)
        print('\nResult...\n')
        print(result)
    return ok
</t>
<t tx="ekr.20250426045416.26">def create_input_tokens(
    self,
    contents: str,
    five_tuples: Generator,
) -&gt; list[InputToken]:
    """
    InputTokenizer.create_input_tokens.

    Return list of InputToken's from tokens, a list of 5-tuples.
    """
    # Remember the contents for debugging.
    self.contents = contents

    # Create the physical lines.
    self.lines = contents.splitlines(True)

    # Create the list of character offsets of the start of each physical line.
    last_offset = 0
    for line in self.lines:
        last_offset += len(line)
        self.offsets.append(last_offset)

    # Create self.token_list.
    for five_tuple in five_tuples:
        self.do_token(contents, five_tuple)

    # Print the token list when tracing.
    self.check_results(contents)
    return self.token_list
</t>
<t tx="ekr.20250426045416.27">def do_token(self, contents: str, five_tuple: tuple) -&gt; None:
    """
    Handle the given token, optionally including between-token whitespace.

    https://docs.python.org/3/library/tokenize.html
    https://docs.python.org/3/library/token.html

    five_tuple is a named tuple with these fields:
    - type:     The token type;
    - string:   The token string.
    - start:    (srow: int, scol: int) The row (line_number!) and column
                where the token begins in the source.
    - end:      (erow: int, ecol: int)) The row (line_number!) and column
                where the token ends in the source;
    - line:     The *physical line on which the token was found.
    """
    import token as token_module

    # Unpack..
    tok_type, val, start, end, line = five_tuple
    s_row, s_col = start  # row/col offsets of start of token.
    e_row, e_col = end  # row/col offsets of end of token.
    line_number = s_row
    kind = token_module.tok_name[tok_type].lower()
    # Calculate the token's start/end offsets: character offsets into contents.
    s_offset = self.offsets[max(0, s_row - 1)] + s_col
    e_offset = self.offsets[max(0, e_row - 1)] + e_col
    # tok_s is corresponding string in the line.
    tok_s = contents[s_offset:e_offset]
    # Add any preceding between-token whitespace.
    ws = contents[self.prev_offset:s_offset]
    if ws:
        # Create the 'ws' pseudo-token.
        self.add_token('ws', line, line_number, ws)
    # Always add token, even if it contributes no text!
    self.add_token(kind, line, line_number, tok_s)
    # Update the ending offset.
    self.prev_offset = e_offset
</t>
<t tx="ekr.20250426045416.28">def make_input_tokens(self, contents: str) -&gt; list[InputToken]:
    """
    Return a list  of InputToken objects using tokenize.tokenize.

    Perform consistency checks and handle all exceptions.
    """
    try:
        five_tuples = tokenize.tokenize(
            io.BytesIO(contents.encode('utf-8')).readline)
    except Exception as e:  # pragma: no cover
        print(f"make_input_tokens: exception {e!r}")
        return []
    tokens = self.create_input_tokens(contents, five_tuples)
    if 1:
        # True: 2.9 sec. False: 2.8 sec.
        assert self.check_round_trip(contents, tokens)
    return tokens
</t>
<t tx="ekr.20250426045416.29">def tokens_to_string(self, tokens: list[InputToken]) -&gt; str:
    """Return the string represented by the list of tokens."""
    if tokens is None:  # pragma: no cover
        # This indicates an internal error.
        print('')
        print('===== No tokens ===== ')
        print('')
        return ''
    return ''.join([z.to_string() for z in tokens])
</t>
<t tx="ekr.20250426045416.3">from __future__ import annotations
import argparse
import difflib
import glob
import keyword
import io
import os
import re
import textwrap
import time
import tokenize
from typing import Generator, Optional, Union

# Leo Imports.
from leo.core import leoGlobals as g
assert g

SettingsDict = dict[str, Union[int, bool]]
</t>
<t tx="ekr.20250426045416.30">class ParseState:
    """
    A class representing items in the parse state stack.

    The present states:

    'file-start': Ensures the stack stack is never empty.

    'decorator': The last '@' was a decorator.

        do_op():    push_state('decorator')
        do_name():  pops the stack if state.kind == 'decorator'.

    'indent': The indentation level for 'class' and 'def' names.

        do_name():      push_state('indent', self.level)
        do_dendent():   pops the stack once or
                        twice if state.value == self.level.
    """

    __slots__ = ('kind', 'value')

    def __init__(self, kind: str, value: Union[int, str, None]) -&gt; None:
        self.kind = kind
        self.value = value

    def __repr__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover

    def __str__(self) -&gt; str:
        return f"State: {self.kind} {self.value!r}"  # pragma: no cover
</t>
<t tx="ekr.20250426045416.31">class ScanState:  # leoTokens.py.
    """
    A class representing tbo.pre_scan's scanning state.

    Valid (kind, value) pairs:

       kind  Value
       ====  =====
      'args' None
      'from' None
    'import' None
     'slice' list of colon indices
      'dict' list of colon indices

    """

    __slots__ = ('kind', 'token', 'value')

    def __init__(self, kind: str, token: InputToken) -&gt; None:
        self.kind = kind
        self.token = token
        self.value: list[int] = []  # Not always used.

    def __repr__(self) -&gt; str:  # pragma: no cover
        return f"ScanState: i: {self.token.index:&lt;4} kind: {self.kind} value: {self.value}"

    def __str__(self) -&gt; str:  # pragma: no cover
        return f"ScanState: i: {self.token.index:&lt;4} kind: {self.kind} value: {self.value}"
</t>
<t tx="ekr.20250426045416.32">class TokenBasedOrange:  # Orange is the new Black.

    &lt;&lt; TokenBasedOrange: docstring &gt;&gt;
    &lt;&lt; TokenBasedOrange: __slots__ &gt;&gt;
    &lt;&lt; TokenBasedOrange: python-related constants &gt;&gt;

    @others
</t>
<t tx="ekr.20250426045416.33">@language rest
@wrap

"""
Leo's token-based beautifier, three times faster than the beautifier in leoAst.py.

**Design**

The *pre_scan* method is the heart of the algorithm. It sets context
for the `:`, `=`, `**` and `.` tokens *without* using the parse tree.
*pre_scan* calls three *finishers*.

Each finisher uses a list of *relevant earlier tokens* to set the
context for one kind of (input) token. Finishers look behind (in the
stream of input tokens) with essentially no cost.

After the pre-scan, *tbo.beautify* (the main loop) calls *visitors*
for each separate type of *input* token.

Visitors call *code generators* to generate strings in the output
list, using *lazy evaluation* to generate whitespace.
"""
</t>
<t tx="ekr.20250426045416.34">__slots__ = [
    # Command-line arguments.
    'all', 'beautified', 'diff', 'report', 'write',

    # Global data.
    'contents', 'filename', 'input_tokens', 'output_list', 'tab_width',
    'insignificant_tokens',  # New.

    # Token-related data for visitors.
    'index', 'input_token', 'line_number',
    'pending_lws', 'pending_ws', 'prev_output_kind', 'prev_output_value',  # New.

    # Parsing state for visitors.
    'decorator_seen', 'in_arg_list', 'in_doc_part', 'state_stack', 'verbatim',

    # Whitespace state. Don't even *think* about changing these!
    'curly_brackets_level', 'indent_level', 'lws', 'paren_level', 'square_brackets_stack',

    # Regular expressions.
    'at_others_pat', 'beautify_pat', 'comment_pat', 'end_doc_pat',
    'nobeautify_pat', 'node_pat', 'start_doc_pat',
]
</t>
<t tx="ekr.20250426045416.35">insignificant_kinds = (
    'comment', 'dedent', 'encoding', 'endmarker', 'indent', 'newline', 'nl', 'ws',
)

# 'name' tokens that may appear in expressions.
operator_keywords = (
    'await',  # Debatable.
    'and', 'in', 'not', 'not in', 'or',  # Operators.
    'True', 'False', 'None',  # Values.
)
</t>
<t tx="ekr.20250426045416.36">def __init__(self, settings: Optional[SettingsDict] = None):
    """Ctor for Orange class."""

    # Set default settings.
    if settings is None:
        settings = {}

    # Hard-code 4-space tabs.
    self.tab_width = 4

    # Define tokens even for empty files.
    self.input_token: InputToken = None
    self.input_tokens: list[InputToken] = []
    self.lws: str = ""  # Set only by Indent/Dedent tokens.
    self.pending_lws: str = ""
    self.pending_ws: str = ""

    # Set by gen_token and all do_* methods that bypass gen_token.
    self.prev_output_kind: str = None
    self.prev_output_value: str = None

    # Set ivars from the settings dict *without* using setattr.
    self.all = settings.get('all', False)
    self.beautified = settings.get('beautified', False)
    self.diff = settings.get('diff', False)
    self.report = settings.get('report', False)
    self.write = settings.get('write', False)

    # The list of tokens that tbo._next/_prev skip.
    self.insignificant_tokens = (
        'comment', 'dedent', 'indent', 'newline', 'nl', 'ws',
    )

    # General patterns.
    self.beautify_pat = re.compile(
        r'#\s*pragma:\s*beautify\b|#\s*@@beautify|#\s*@\+node|#\s*@[+-]others|#\s*@[+-]&lt;&lt;')
    self.comment_pat = re.compile(r'^(\s*)#[^@!# \n]')
    self.nobeautify_pat = re.compile(r'\s*#\s*pragma:\s*no\s*beautify\b|#\s*@@nobeautify')

    # Patterns from FastAtRead class, specialized for python delims.
    self.node_pat = re.compile(r'^(\s*)#@\+node:([^:]+): \*(\d+)?(\*?) (.*)$')  # @node
    self.start_doc_pat = re.compile(r'^\s*#@\+(at|doc)?(\s.*?)?$')  # @doc or @
    self.at_others_pat = re.compile(r'^(\s*)#@(\+|-)others\b(.*)$')  # @others

    # Doc parts end with @c or a node sentinel. Specialized for python.
    self.end_doc_pat = re.compile(r"^\s*#@(@(c(ode)?)|([+]node\b.*))$")
</t>
<t tx="ekr.20250426045416.37"></t>
<t tx="ekr.20250426045416.38">def dump_token_range(self, i1: int, i2: int, tag: Optional[str] = None) -&gt; None:  # pragma: no cover
    """Dump the given range of input tokens."""
    if tag:
        print(tag)
    for token in self.input_tokens[i1 : i2 + 1]:
        print(token.dump())
</t>
<t tx="ekr.20250426045416.39">def internal_error_message(self, message: str) -&gt; str:  # pragma: no cover
    """Print a message about an error in the beautifier itself."""
    # Compute lines_s.
    line_number = self.input_token.line_number
    lines = g.splitLines(self.contents)
    n1 = max(0, line_number - 5)
    n2 = min(line_number + 5, len(lines))
    prev_lines = ['\n']
    for i in range(n1, n2):
        marker_s = '***' if i + 1 == line_number else '   '
        prev_lines.append(f"Line {i+1:5}:{marker_s}{lines[i]!r}\n")
    context_s = ''.join(prev_lines) + '\n'

    # Return the full error message.
    return (
        # '\n\n'
        'Error in token-based beautifier!\n'
        f"{message.strip()}\n"
        '\n'
        f"At token {self.index}, line: {line_number} file: {self.filename}\n"
        f"{context_s}"
        "Please report this message to Leo's developers"
    )
</t>
<t tx="ekr.20250426045416.4"></t>
<t tx="ekr.20250426045416.40">def user_error_message(self, message: str) -&gt; str:  # pragma: no cover
    """Print a message about a user error."""
    # Compute lines_s.
    line_number = self.input_token.line_number
    lines = g.splitLines(self.contents)
    n1 = max(0, line_number - 5)
    n2 = min(line_number + 5, len(lines))
    prev_lines = ['\n']
    for i in range(n1, n2):
        marker_s = '***' if i + 1 == line_number else '   '
        prev_lines.append(f"Line {i+1:5}:{marker_s}{lines[i]!r}\n")
    context_s = ''.join(prev_lines) + '\n'

    # Return the full error message.
    return (
        f"{message.strip()}\n"
        '\n'
        f"At token {self.index}, line: {line_number} file: {self.filename}\n"
        f"{context_s}"
    )
</t>
<t tx="ekr.20250426045416.41">def oops(self, message: str) -&gt; None:  # pragma: no cover
    """Raise InternalBeautifierError."""
    raise InternalBeautifierError(self.internal_error_message(message))
</t>
<t tx="ekr.20250426045416.42"></t>
<t tx="ekr.20250426045416.43">def no_visitor(self) -&gt; None:  # pragma: no cover
    self.oops(f"Unknown kind: {self.input_token.kind!r}")

def beautify(self,
    contents: str, filename: str, input_tokens: list[InputToken],
) -&gt; str:
    """
    The main line. Create output tokens and return the result as a string.

    beautify_file and beautify_file_def call this method.
    """
    &lt;&lt; tbo.beautify: init ivars &gt;&gt;

    try:
        # Pre-scan the token list, setting context.s
        self.pre_scan()

        # Init ivars first.
        self.input_token = None
        self.pending_lws = ''
        self.pending_ws = ''
        self.prev_output_kind = None
        self.prev_output_value = None

        # Init state.
        self.gen_token('file-start', '')
        self.push_state('file-start')

        # The main loop:
        prev_line_number: int = 0
        for self.index, self.input_token in enumerate(input_tokens):
            # Set global for visitors.
            if prev_line_number != self.input_token.line_number:
                prev_line_number = self.input_token.line_number
            # Call the proper visitor.
            if self.verbatim:
                self.do_verbatim()
            else:
                func = getattr(self, f"do_{self.input_token.kind}", self.no_visitor)
                func()

        # Return the result.
        result = ''.join(self.output_list)
        return result

    # Make no change if there is any error.
    except InternalBeautifierError as e:  # pragma: no cover
        # oops calls self.internal_error_message to creates e.
        print(repr(e))
    except AssertionError as e:  # pragma: no cover
        print(self.internal_error_message(repr(e)))
    return contents
</t>
<t tx="ekr.20250426045416.44"># Debugging vars...
self.contents = contents
self.filename = filename
self.line_number: Optional[int] = None

# The input and output lists...
self.output_list: list[str] = []
self.input_tokens = input_tokens  # The list of input tokens.

# State vars for whitespace.
self.curly_brackets_level = 0  # Number of unmatched '{' tokens.
self.paren_level = 0  # Number of unmatched '(' tokens.
self.square_brackets_stack: list[bool] = []  # A stack of bools, for self.gen_word().
self.indent_level = 0  # Set only by do_indent and do_dedent.

# Parse state.
self.decorator_seen = False  # Set by do_name for do_op.
self.in_arg_list = 0  # &gt; 0 if in an arg list of a def.
self.in_doc_part = False
self.state_stack: list[ParseState] = []  # Stack of ParseState objects.

# Leo-related state.
self.verbatim = False  # True: don't beautify.

# Ivars describing the present input token...
self.index = 0  # The index within the tokens array of the token being scanned.
self.lws = ''  # Leading whitespace. Required!
</t>
<t tx="ekr.20250426045416.45">def beautify_file(self, filename: str) -&gt; bool:  # pragma: no cover
    """
    TokenBasedOrange: Beautify the the given external file.

    Return True if the file was beautified.
    """
    if 0:
        print(
            f"all: {int(self.all)} "
            f"beautified: {int(self.beautified)} "
            f"diff: {int(self.diff)} "
            f"report: {int(self.report)} "
            f"write: {int(self.write)} "
            f"{g.shortFileName(filename)}"
        )
    self.filename = filename
    contents, tokens = self.init_tokens_from_file(filename)
    if not (contents and tokens):
        return False  # Not an error.
    if not isinstance(tokens[0], InputToken):
        self.oops(f"Not an InputToken: {tokens[0]!r}")

    # Beautify the contents, returning the original contents on any error.
    results = self.beautify(contents, filename, tokens)

    # Ignore changes only to newlines.
    if self.regularize_newlines(contents) == self.regularize_newlines(results):
        return False

    # Print reports.
    if self.beautified:  # --beautified.
        print(f"tbo: beautified: {g.shortFileName(filename)}")
    if self.diff:  # --diff.
        print(f"Diffs: {filename}")
        self.show_diffs(contents, results)

    # Write the (changed) file .
    if self.write:  # --write.
        self.write_file(filename, results)
    return True
</t>
<t tx="ekr.20250426045416.46">def init_tokens_from_file(self, filename: str) -&gt; tuple[
    str, list[InputToken]
]:  # pragma: no cover
    """
    Create the list of tokens for the given file.
    Return (contents, encoding, tokens).
    """
    self.indent_level = 0
    self.filename = filename
    t1 = time.perf_counter_ns()
    contents = g.readFile(filename)
    t2 = time.perf_counter_ns()
    if not contents:
        self.input_tokens = []
        return '', []
    t3 = time.perf_counter_ns()
    self.input_tokens = input_tokens = Tokenizer().make_input_tokens(contents)
    t4 = time.perf_counter_ns()
    if 0:
        print(f"       read: {(t2-t1)/1000000:6.2f} ms")
        print(f"make_tokens: {(t4-t3)/1000000:6.2f} ms")
        print(f"      total: {(t4-t1)/1000000:6.2f} ms")
    return contents, input_tokens
</t>
<t tx="ekr.20250426045416.47">def regularize_newlines(self, s: str) -&gt; str:
    """Regularize newlines within s."""
    return s.replace('\r\n', '\n').replace('\r', '\n')
</t>
<t tx="ekr.20250426045416.48">def write_file(self, filename: str, s: str) -&gt; None:  # pragma: no cover
    """
    Write the string s to the file whose name is given.

    Handle all exceptions.

    Before calling this function, the caller should ensure
    that the file actually has been changed.
    """
    try:
        s2 = g.toEncodedString(s)  # May raise exception.
        with open(filename, 'wb') as f:
            f.write(s2)
    except Exception as e:  # pragma: no cover
        print(f"Error {e!r}: {filename!r}")
</t>
<t tx="ekr.20250426045416.49">def show_diffs(self, s1: str, s2: str) -&gt; None:  # pragma: no cover
    """Print diffs between strings s1 and s2."""
    filename = self.filename
    lines = list(difflib.unified_diff(
        g.splitLines(s1),
        g.splitLines(s2),
        fromfile=f"Old {filename}",
        tofile=f"New {filename}",
    ))
    print('')
    print(f"Diffs for {filename}")
    for line in lines:
        print(line)
</t>
<t tx="ekr.20250426045416.5">def dump_contents(contents: str, tag: str = 'Contents') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    for i, z in enumerate(g.splitLines(contents)):
        print(f"{i+1:&lt;3} ", z.rstrip())
    print('')
</t>
<t tx="ekr.20250426045416.50"># Visitors (tbo.do_* methods) handle input tokens.
# Generators (tbo.gen_* methods) create zero or more output tokens.
</t>
<t tx="ekr.20250426045416.51">def do_comment(self) -&gt; None:
    """Handle a comment token."""
    val = self.input_token.value
    &lt;&lt; do_comment: update comment-related state &gt;&gt;

    # Generate the comment.
    self.pending_lws = ''
    self.pending_ws = ''
    entire_line = self.input_token.line.lstrip().startswith('#')

    if entire_line:
        # The comment includes all ws.
        # #1496: No further munging needed.
        val = self.input_token.line.rstrip()
        # #3056: Insure one space after '#' in non-sentinel comments.
        #        Do not change bang lines or '##' comments.
        if m := self.comment_pat.match(val):
            i = len(m.group(1))
            val = val[:i] + '# ' + val[i + 1 :]
    else:
        # Exactly two spaces before trailing comments.
        val = '  ' + val.rstrip()
    self.gen_token('comment', val)
</t>
<t tx="ekr.20250426045416.52"># Leo-specific code...
if self.node_pat.match(val):
    # Clear per-node state.
    self.in_doc_part = False
    self.verbatim = False
    self.decorator_seen = False
    # Do *not* clear other state, which may persist across @others.
        # self.curly_brackets_level = 0
        # self.in_arg_list = 0
        # self.indent_level = 0
        # self.lws = ''
        # self.paren_level = 0
        # self.square_brackets_stack = []
        # self.state_stack = []
else:
    # Keep track of verbatim mode.
    if self.beautify_pat.match(val):
        self.verbatim = False
    elif self.nobeautify_pat.match(val):
        self.verbatim = True
    # Keep trace of @doc parts, to honor the convention for splitting lines.
    if self.start_doc_pat.match(val):
        self.in_doc_part = True
    if self.end_doc_pat.match(val):
        self.in_doc_part = False
</t>
<t tx="ekr.20250426045416.53">def do_dedent(self) -&gt; None:
    """Handle dedent token."""
    # Note: other methods use self.indent_level.
    self.indent_level -= 1
    self.lws = self.indent_level * self.tab_width * ' '
    self.pending_lws = self.lws
    self.pending_ws = ''
    self.prev_output_kind = 'dedent'
</t>
<t tx="ekr.20250426045416.54">def do_encoding(self) -&gt; None:
    """Handle the encoding token."""
</t>
<t tx="ekr.20250426045416.55">def do_endmarker(self) -&gt; None:
    """Handle an endmarker token."""

    # Ensure exactly one newline at the end of file.
    if self.prev_output_kind not in (
        'indent', 'dedent', 'line-indent', 'newline',
    ):
        self.output_list.append('\n')
    self.pending_lws = ''  # Defensive.
    self.pending_ws = ''  # Defensive.
</t>
<t tx="ekr.20250426045416.56">consider_message = 'consider using python/Tools/scripts/reindent.py'

def do_indent(self) -&gt; None:
    """Handle indent token."""

    # Only warn about indentation errors.
    if '\t' in self.input_token.value:  # pragma: no cover
        print(f"Found tab character in {self.filename}")
        print(self.consider_message)
    elif (len(self.input_token.value) % self.tab_width) != 0:  # pragma: no cover
        print(f"Indentation error in {self.filename}")
        print(self.consider_message)

    # Handle the token!
    new_indent = self.input_token.value
    old_indent = self.indent_level * self.tab_width * ' '
    if new_indent &gt; old_indent:
        self.indent_level += 1
    elif new_indent &lt; old_indent:  # pragma: no cover (defensive)
        print(f"\n===== do_indent: can not happen {new_indent!r}, {old_indent!r}")

    self.lws = new_indent
    self.pending_lws = self.lws
    self.pending_ws = ''
    self.prev_output_kind = 'indent'
</t>
<t tx="ekr.20250426045416.57"></t>
<t tx="ekr.20250426045416.58">def do_name(self) -&gt; None:
    """Handle a name token."""
    name = self.input_token.value
    if name in self.operator_keywords:
        self.gen_word_op(name)
    else:
        self.gen_word(name)
</t>
<t tx="ekr.20250426045416.59">def gen_word(self, s: str) -&gt; None:
    """Add a word request to the code list."""
    assert s == self.input_token.value
    assert s and isinstance(s, str), repr(s)
    self.gen_blank()
    self.gen_token('word', s)
    self.gen_blank()
</t>
<t tx="ekr.20250426045416.6">def dump_lines(tokens: list[InputToken], tag: str = 'lines') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    for z in tokens:
        if z.line.strip():
            print(z.line.rstrip())
        else:
            print(repr(z.line))
    print('')
</t>
<t tx="ekr.20250426045416.60">def gen_word_op(self, s: str) -&gt; None:
    """Add a word-op request to the code list."""
    assert s == self.input_token.value
    assert s and isinstance(s, str), repr(s)
    self.gen_blank()
    self.gen_token('word-op', s)
    self.gen_blank()
</t>
<t tx="ekr.20250426045416.61"></t>
<t tx="ekr.20250426045416.62">def do_newline(self) -&gt; None:
    """
    do_newline: Handle a regular newline.

    From https://docs.python.org/3/library/token.html

    NEWLINE tokens end *logical* lines of Python code.
    """

    self.output_list.append('\n')
    self.pending_lws = ''  # Set only by 'dedent', 'indent' or 'ws' tokens.
    self.pending_ws = ''
    self.prev_output_kind = 'newline'
    self.prev_output_value = '\n'
</t>
<t tx="ekr.20250426045416.63">def do_nl(self) -&gt; None:
    """
    do_nl: Handle a continuation line.

    From https://docs.python.org/3/library/token.html

    NL tokens end *physical* lines. They appear when when a logical line of
    code spans multiple physical lines.
    """
    return self.do_newline()
</t>
<t tx="ekr.20250426045416.64">def do_number(self) -&gt; None:
    """Handle a number token."""
    self.gen_blank()
    self.gen_token('number', self.input_token.value)
</t>
<t tx="ekr.20250426045416.65"></t>
<t tx="ekr.20250426045416.66">def do_op(self) -&gt; None:
    """Handle an op token."""
    val = self.input_token.value

    if val == '.':
        self.gen_dot_op()
    elif val == '@':
        self.gen_token('op-no-blanks', val)
        self.push_state('decorator')
    elif val == ':':
        # Treat slices differently.
        self.gen_colon()
    elif val in ',;':
        # Pep 8: Avoid extraneous whitespace immediately before
        # comma, semicolon, or colon.
        self.pending_ws = ''
        self.gen_token('op', val)
        self.gen_blank()
    elif val in '([{':
        # Pep 8: Avoid extraneous whitespace immediately inside
        # parentheses, brackets or braces.
        self.gen_lt()
    elif val in ')]}':
        # Ditto.
        self.gen_rt()
    elif val == '=':
        self.gen_equal_op()
    elif val in '~+-':
        self.gen_possible_unary_op()
    elif val == '*':
        self.gen_star_op()
    elif val == '**':
        self.gen_star_star_op()
    else:
        # Pep 8: always surround binary operators with a single space.
        # '==','+=','-=','*=','**=','/=','//=','%=','!=','&lt;=','&gt;=','&lt;','&gt;',
        # '^','~','*','**','&amp;','|','/','//',
        # Pep 8: If operators with different priorities are used, consider
        # adding whitespace around the operators with the lowest priorities.
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20250426045416.67">def gen_colon(self) -&gt; None:
    """Handle a colon."""
    val = self.input_token.value
    context = self.input_token.context

    self.pending_ws = ''
    if context == 'complex-slice':
        if self.prev_output_value not in '[:':
            self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
    elif context == 'simple-slice':
        self.gen_token('op-no-blanks', val)
    elif context == 'dict':
        self.gen_token('op', val)
        self.gen_blank()
    else:
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20250426045416.68">def gen_dot_op(self) -&gt; None:
    """Handle the '.' input token."""
    context = self.input_token.context

    # Get the previous significant **input** token.
    # This is the only call to next(i) anywhere!
    next_i = self._next(self.index)
    next = 'None' if next_i is None else self.input_tokens[next_i]
    import_is_next = next and next.kind == 'name' and next.value == 'import'  # type:ignore

    if context == 'import':
        if (
            self.prev_output_kind == 'word'
            and self.prev_output_value in ('from', 'import')
        ):
            self.gen_blank()
            op = 'op' if import_is_next else 'op-no-blanks'
            self.gen_token(op, '.')
        elif import_is_next:
            self.gen_token('op', '.')
            self.gen_blank()
        else:
            self.pending_ws = ''
            self.gen_token('op-no-blanks', '.')
    else:
        self.pending_ws = ''
        self.gen_token('op-no-blanks', '.')
</t>
<t tx="ekr.20250426045416.69">def _next(self, i: int) -&gt; Optional[int]:
    """
    Return the next *significant* input token.

    Ignore insignificant tokens: whitespace, indentation, comments, etc.

    The **Global Token Ratio** is tbo.n_scanned_tokens / len(tbo.tokens),
    where tbo.n_scanned_tokens is the total number of calls calls to
    tbo.next or tbo.prev.

    For Leo's sources, this ratio ranges between 0.48 and 1.51!

    The orange_command function warns if this ratio is greater than 2.5.
    Previous versions of this code suffered much higher ratios.
    """
    i += 1
    while i &lt; len(self.input_tokens):
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_tokens:
            # g.trace(f"token: {token!r}")
            return i
        i += 1
    return None  # pragma: no cover
</t>
<t tx="ekr.20250426045416.7">def dump_results(results: list[str], tag: str = 'Results') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    print(''.join(results))
    print('')
</t>
<t tx="ekr.20250426045416.70">def gen_equal_op(self) -&gt; None:

    val = self.input_token.value
    context = self.input_token.context

    if context == 'initializer':
        # Pep 8: Don't use spaces around the = sign when used to indicate
        #        a keyword argument or a default parameter value.
        #        However, when combining an argument annotation with a default value,
        #        *do* use spaces around the = sign.
        self.pending_ws = ''
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20250426045416.71">def gen_lt(self) -&gt; None:
    """Generate code for a left paren or curly/square bracket."""
    val = self.input_token.value
    assert val in '([{', repr(val)

    # Update state vars.
    if val == '(':
        self.paren_level += 1
    elif val == '[':
        self.square_brackets_stack.append(False)
    else:
        self.curly_brackets_level += 1

    # Generate or suppress the leading blank.
    # Update self.in_arg_list if necessary.
    if self.input_token.context == 'import':
        self.gen_blank()
    elif self.prev_output_kind in ('op', 'word-op'):
        self.gen_blank()
    elif self.prev_output_kind == 'word':
        # Only suppress blanks before '(' or '[' for non-keywords.
        if val == '{' or self.prev_output_value in (
            'if', 'else', 'elif', 'return', 'for', 'while',
        ):
            self.gen_blank()
        elif val == '(':
            self.in_arg_list += 1
            self.pending_ws = ''
        else:
            self.pending_ws = ''
    elif self.prev_output_kind != 'line-indent':
        self.pending_ws = ''

    # Output the token!
    self.gen_token('op-no-blanks', val)
</t>
<t tx="ekr.20250426045416.72">def gen_possible_unary_op(self) -&gt; None:
    """Add a unary or binary op to the token list."""
    val = self.input_token.value
    if self.is_unary_op(self.index, val):
        prev = self.input_token
        if prev.kind == 'lt':
            self.gen_token('op-no-blanks', val)
        else:
            self.gen_blank()
            self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()

</t>
<t tx="ekr.20250426045416.73">def is_unary_op(self, i: int, val: str) -&gt; bool:

    if val == '~':
        return True
    if val not in '+-':  # pragma: no cover
        return False

    # Get the previous significant **input** token.
    # This is the only call to _prev(i) anywhere!
    prev_i = self._prev(i)
    prev_token = None if prev_i is None else self.input_tokens[prev_i]
    kind = prev_token.kind if prev_token else ''
    value = prev_token.value if prev_token else ''

    if kind in ('number', 'string'):
        return_val = False
    elif kind == 'op' and value in ')]':
        return_val = False
    elif kind == 'op' and value in '{([:':
        return_val = True
    elif kind != 'name':
        return_val = True
    else:
        # The hard case: prev_token is a 'name' token.
        # Any Python keyword indicates a unary operator.
        return_val = keyword.iskeyword(value) or keyword.issoftkeyword(value)
    return return_val
</t>
<t tx="ekr.20250426045416.74">def _prev(self, i: int) -&gt; Optional[int]:
    """
    Return the previous *significant* input token.

    Ignore insignificant tokens: whitespace, indentation, comments, etc.
    """
    i -= 1
    while i &gt;= 0:
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_tokens:
            return i
        i -= 1
    return None  # pragma: no cover
</t>
<t tx="ekr.20250426045416.75">def gen_rt(self) -&gt; None:
    """Generate code for a right paren or curly/square bracket."""
    val = self.input_token.value
    assert val in ')]}', repr(val)

    # Update state vars.
    if val == ')':
        self.paren_level -= 1
        self.in_arg_list = max(0, self.in_arg_list - 1)
    elif val == ']':
        self.square_brackets_stack.pop()
    else:
        self.curly_brackets_level -= 1

    if self.prev_output_kind != 'line-indent':
        self.pending_ws = ''
    self.gen_token('rt', val)
</t>
<t tx="ekr.20250426045416.76">def gen_star_op(self) -&gt; None:
    """Put a '*' op, with special cases for *args."""
    val = self.input_token.value
    context = self.input_token.context

    if context == 'arg':
        self.gen_blank()
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20250426045416.77">def gen_star_star_op(self) -&gt; None:
    """Put a ** operator, with a special case for **kwargs."""
    val = self.input_token.value
    context = self.input_token.context

    if context == 'arg':
        self.gen_blank()
        self.gen_token('op-no-blanks', val)
    else:
        self.gen_blank()
        self.gen_token('op', val)
        self.gen_blank()
</t>
<t tx="ekr.20250426045416.78">def push_state(self, kind: str, value: Union[int, str, None] = None) -&gt; None:
    """Append a state to the state stack."""
    state = ParseState(kind, value)
    self.state_stack.append(state)
</t>
<t tx="ekr.20250426045416.79">def do_string(self) -&gt; None:
    """
    Handle a 'string' token.

    The Tokenizer converts all f-string tokens to a single 'string' token.
    """
    # Careful: continued strings may contain '\r'
    val = self.regularize_newlines(self.input_token.value)
    self.gen_token('string', val)
    self.gen_blank()
</t>
<t tx="ekr.20250426045416.8">def dump_tokens(tokens: list[InputToken], tag: str = 'Tokens') -&gt; None:  # pragma: no cover
    print('')
    print(f"{tag}...\n")
    if not tokens:
        return
    print(
        "Note: values shown are repr(value) "
        "*except* for 'string' and 'fstring*' tokens."
    )
    tokens[0].dump_header()
    for z in tokens:
        print(z.dump())
    print('')
</t>
<t tx="ekr.20250426045416.80">def do_verbatim(self) -&gt; None:
    """
    Handle one token in verbatim mode.
    End verbatim mode when the appropriate comment is seen.
    """
    kind = self.input_token.kind
    #
    # Careful: tokens may contain '\r'
    val = self.regularize_newlines(self.input_token.value)
    if kind == 'comment':
        if self.beautify_pat.match(val):
            self.verbatim = False
        val = val.rstrip()
        self.gen_token('comment', val)
        return
    if kind == 'indent':
        self.indent_level += 1
        self.lws = self.indent_level * self.tab_width * ' '
    if kind == 'dedent':
        self.indent_level -= 1
        self.lws = self.indent_level * self.tab_width * ' '
    self.gen_token('verbatim', val)
</t>
<t tx="ekr.20250426045416.81">def do_ws(self) -&gt; None:
    """
    Handle the "ws" pseudo-token.  See Tokenizer.itok.do_token (the gem).

    Put the whitespace only if if ends with backslash-newline.
    """
    val = self.input_token.value
    last_token = self.input_tokens[self.index - 1]

    if last_token.kind in ('nl', 'newline'):
        self.pending_lws = val
        self.pending_ws = ''
    elif '\\\n' in val:
        self.pending_lws = ''
        self.pending_ws = val
    else:
        self.pending_ws = val
</t>
<t tx="ekr.20250426045416.82">def gen_blank(self) -&gt; None:
    """
    Queue a *request* a blank.
    Change *neither* prev_output_kind *nor* pending_lws.
    """

    prev_kind = self.prev_output_kind
    if prev_kind == 'op-no-blanks':
        # A demand that no blank follows this op.
        self.pending_ws = ''
    elif prev_kind == 'hard-blank':
        # Eat any further blanks.
        self.pending_ws = ''
    elif prev_kind in (
        'dedent',
        'file-start',
        'indent',
        'line-indent',
        'newline',
    ):
        # Suppress the blank, but do *not* change the pending ws.
        pass
    elif self.pending_ws:
        # Use the existing pending ws.
        pass
    else:
        self.pending_ws = ' '
</t>
<t tx="ekr.20250426045416.83">def gen_token(self, kind: str, value: str) -&gt; None:
    """Add an output token to the code list."""

    if self.pending_lws:
        self.output_list.append(self.pending_lws)
    elif self.pending_ws:
        self.output_list.append(self.pending_ws)

    self.output_list.append(value)
    self.pending_lws = ''
    self.pending_ws = ''
    self.prev_output_value = value
    self.prev_output_kind = kind
</t>
<t tx="ekr.20250426045416.84"># The parser calls scanner methods to move through the list of input tokens.
</t>
<t tx="ekr.20250426045416.85">def pre_scan(self) -&gt; None:
    """
    Scan the entire file in one iterative pass, adding context to a few
    kinds of tokens as follows:

    Token   Possible Contexts (or None)
    =====   ===========================
    ':'     'annotation', 'dict', 'complex-slice', 'simple-slice'
    '='     'annotation', 'initializer'
    '*'     'arg'
    '**'    'arg'
    '.'     'import'
    """

    # The main loop.
    in_import = False
    scan_stack: list[ScanState] = []
    prev_token: Optional[InputToken] = None
    for i, token in enumerate(self.input_tokens):
        kind, value = token.kind, token.value
        if kind in 'newline':
            &lt;&lt; pre-scan 'newline' tokens &gt;&gt;
        elif kind == 'op':
            &lt;&lt; pre-scan 'op' tokens &gt;&gt;
        elif kind == 'name':
            &lt;&lt; pre-scan 'name' tokens &gt;&gt;
        # Remember the previous significant token.
        if kind not in self.insignificant_kinds:
            prev_token = token
    # Sanity check.
    if scan_stack:  # pragma: no cover
        print('pre_scan: non-empty scan_stack')
        print(scan_stack)
</t>
<t tx="ekr.20250426045416.86"># 'import' and 'from x import' statements may span lines.
# 'ws' tokens represent continued lines like this:   ws: ' \\\n    '
if in_import and not scan_stack:
    in_import = False
</t>
<t tx="ekr.20250426045416.87">top_state: Optional[ScanState] = scan_stack[-1] if scan_stack else None

# Handle '[' and ']'.
if value == '[':
    scan_stack.append(ScanState('slice', token))
elif value == ']':
    assert top_state and top_state.kind == 'slice'
    self.finish_slice(i, top_state)
    scan_stack.pop()

# Handle '{' and '}'.
if value == '{':
    scan_stack.append(ScanState('dict', token))
elif value == '}':
    assert top_state and top_state.kind == 'dict'
    self.finish_dict(i, top_state)
    scan_stack.pop()

# Handle '(' and ')'
elif value == '(':
    if self.is_python_keyword(prev_token) or prev_token and prev_token.kind != 'name':
        state_kind = '('
    else:
        state_kind = 'arg'
    scan_stack.append(ScanState(state_kind, token))
elif value == ')':
    assert top_state and top_state.kind in ('(', 'arg'), repr(top_state)
    if top_state.kind == 'arg':
        self.finish_arg(i, top_state)
    scan_stack.pop()

# Handle interior tokens in 'arg' and 'slice' states.
if top_state:
    if top_state.kind in ('dict', 'slice') and value == ':':
        top_state.value.append(i)
    if top_state.kind == 'arg' and value in '**=:,':
        top_state.value.append(i)

# Handle '.' and '(' tokens inside 'import' and 'from' statements.
if in_import and value in '(.':
    self.set_context(i, 'import')
</t>
<t tx="ekr.20250426045416.88">prev_is_yield = prev_token and prev_token.kind == 'name' and prev_token.value == 'yield'
if value in ('from', 'import') and not prev_is_yield:
    # 'import' and 'from x import' statements should be at the outer level.
    assert not scan_stack, scan_stack
    in_import = True
</t>
<t tx="ekr.20250426045416.89">def finish_arg(self, end: int, state: Optional[ScanState]) -&gt; None:
    """Set context for all ':' when scanning from '(' to ')'."""

    # Sanity checks.
    if not state:
        return
    assert state.kind == 'arg', repr(state)
    token = state.token
    assert token.value == '(', repr(token)
    values = state.value
    assert isinstance(values, list), repr(values)
    i1 = token.index
    assert i1 &lt; end, (i1, end)
    if not values:
        return

    # Compute the context for each *separate* '=' token.
    equal_context = 'initializer'
    for i in values:
        token = self.input_tokens[i]
        assert token.kind == 'op', repr(token)
        if token.value == ',':
            equal_context = 'initializer'
        elif token.value == ':':
            equal_context = 'annotation'
        elif token.value == '=':
            self.set_context(i, equal_context)
            equal_context = 'initializer'

    # Set the context of all outer-level ':', '*', and '**' tokens.
    prev: Optional[InputToken] = None
    for i in range(i1, end):
        token = self.input_tokens[i]
        if token.kind not in self.insignificant_kinds:
            if token.kind == 'op':
                if token.value in ('*', '**'):
                    if self.is_unary_op_with_prev(prev, token):
                        self.set_context(i, 'arg')
                elif token.value == '=':
                    # The code above has set the context.
                    assert token.context in ('initializer', 'annotation'), (i, repr(token.context))
                elif token.value == ':':
                    self.set_context(i, 'annotation')
            prev = token
</t>
<t tx="ekr.20250426045416.9">def input_tokens_to_string(tokens: list[InputToken]) -&gt; str:  # pragma: no cover
    """Return the string represented by the list of tokens."""
    if tokens is None:
        # This indicates an internal error.
        print('')
        print('===== input token list is None ===== ')
        print('')
        return ''
    return ''.join([z.to_string() for z in tokens])
</t>
<t tx="ekr.20250426045416.90">def finish_slice(self, end: int, state: ScanState) -&gt; None:
    """Set context for all ':' when scanning from '[' to ']'."""

    # Sanity checks.
    assert state.kind == 'slice', repr(state)
    token = state.token
    assert token.value == '[', repr(token)
    colons = state.value
    assert isinstance(colons, list), repr(colons)
    i1 = token.index
    assert i1 &lt; end, (i1, end)

    # Do nothing if there are no ':' tokens in the slice.
    if not colons:
        return

    # Compute final context by scanning the tokens.
    final_context = 'simple-slice'
    inter_colon_tokens = 0
    prev = token
    for i in range(i1 + 1, end - 1):
        token = self.input_tokens[i]
        kind, value = token.kind, token.value
        if kind not in self.insignificant_kinds:
            if kind == 'op':
                if value == '.':
                    # Ignore '.' tokens and any preceding 'name' token.
                    if prev and prev.kind == 'name':  # pragma: no cover
                        inter_colon_tokens -= 1
                elif value == ':':
                    inter_colon_tokens = 0
                elif value in '-+':
                    # Ignore unary '-' or '+' tokens.
                    if not self.is_unary_op_with_prev(prev, token):
                        inter_colon_tokens += 1
                        if inter_colon_tokens &gt; 1:
                            final_context = 'complex-slice'
                            break
                elif value == '~':
                    # '~' is always a unary op.
                    pass
                else:
                    # All other ops contribute.
                    inter_colon_tokens += 1
                    if inter_colon_tokens &gt; 1:
                        final_context = 'complex-slice'
                        break
            else:
                inter_colon_tokens += 1
                if inter_colon_tokens &gt; 1:
                    final_context = 'complex-slice'
                    break
            prev = token

    # Set the context of all outer-level ':' tokens.
    for i in colons:
        self.set_context(i, final_context)
</t>
<t tx="ekr.20250426045416.91">def finish_dict(self, end: int, state: Optional[ScanState]) -&gt; None:
    """
    Set context for all ':' when scanning from '{' to '}'

    Strictly speaking, setting this context is unnecessary because
    tbo.gen_colon generates the same code regardless of this context.

    In other words, this method can be a do-nothing!
    """

    # Sanity checks.
    if not state:
        return
    assert state.kind == 'dict', repr(state)
    token = state.token
    assert token.value == '{', repr(token)
    colons = state.value
    assert isinstance(colons, list), repr(colons)
    i1 = token.index
    assert i1 &lt; end, (i1, end)

    # Set the context for all ':' tokens.
    for i in colons:
        self.set_context(i, 'dict')
</t>
<t tx="ekr.20250426045416.92">def is_unary_op_with_prev(self, prev: Optional[InputToken], token: InputToken) -&gt; bool:
    """
    Return True if token is a unary op in the context of prev, the previous
    significant token.
    """
    if token.value == '~':  # pragma: no cover
        return True
    if prev is None:
        return True  # pragma: no cover
    assert token.value in '**-+', repr(token.value)
    if prev.kind in ('number', 'string'):
        return_val = False
    elif prev.kind == 'op' and prev.value in ')]':
         # An unnecessary test?
        return_val = False  # pragma: no cover
    elif prev.kind == 'op' and prev.value in '{([:,':
        return_val = True
    elif prev.kind != 'name':
        # An unnecessary test?
        return_val = True  # pragma: no cover
    else:
        # prev is a'name' token.
        return self.is_python_keyword(token)
    return return_val
</t>
<t tx="ekr.20250426045416.93">def is_python_keyword(self, token: Optional[InputToken]) -&gt; bool:
    """Return True if token is a 'name' token referring to a Python keyword."""
    if not token or token.kind != 'name':
        return False
    return keyword.iskeyword(token.value) or keyword.issoftkeyword(token.value)
</t>
<t tx="ekr.20250426045416.94">def set_context(self, i: int, context: str) -&gt; None:
    """
    Set self.input_tokens[i].context, but only if it does not already exist!

    See the docstring for pre_scan for details.
    """

    trace = False  # Do not delete the trace below.

    valid_contexts = (
        'annotation', 'arg', 'complex-slice', 'simple-slice',
        'dict', 'import', 'initializer',
    )
    if context not in valid_contexts:
        self.oops(f"Unexpected context! {context!r}")  # pragma: no cover

    token = self.input_tokens[i]

    if trace:  # pragma: no cover
        token_s = f"&lt;{token.kind}: {token.show_val(12)}&gt;"
        ignore_s = 'Ignore' if token.context else ' ' * 6
        print(f"{i:3} {ignore_s} token: {token_s} context: {context}")

    if not token.context:
        token.context = context
</t>
<t tx="ekr.20250426050131.1"></t>
<t tx="ekr.20250426052508.1">def main():
    assert g.app is None, repr(g.app)
    assert g.unitTesting is False
    x = CacheController()
    updated_files = x.get_changed_files()
    if updated_files:
        x.compute_diffs(updated_files)
        x.write_cache()
        x.commit()
    x.close()
    x.print_stats(updated_files)
</t>
<t tx="ekr.20250426191746.1">@language rest

Stats: How long does it take:
- check the modification dates of all Leo files?
- to update the cache for each file?

Investigate diffing Ast's.

Can Ast's be cached? Can they be pickled?

- Create an artificial diff.
- Give/Take arrays for every namespace.

Later:
- Function: module_path: converts import to path.

@language python
</t>
<t tx="ekr.20250426202645.1"></t>
<t tx="ekr.20250427052613.1"># from leo.core import (
    # leoAPI, leoApp, leoAtFile,  # leoAst,
    # leoBackground,  # leoBridge, # leoBeautify,
    # leoCache, leoChapters, leoColor, leoColorizer,
    # leoCommands, leoConfig,  # leoCompare, leoDebugger,
    # leoExternalFiles,
    # leoFileCommands, leoFind, leoFrame,
    # leoGlobals, leoGui,
    # leoHistory, leoImport,  # leoJupytext,
    # leoKeys, leoMarkup, leoMenu,
    # leoNodes,
    # leoPlugins,  # leoPersistence, leoPrinting, leoPymacs,
    # leoQt,
    # leoRst,  # leoRope,
    # leoSessions, leoShadow,
    # # leoTest2, leoTips, leoTokens,
    # leoUndo, leoVersion,  # leoVim
# )
</t>
<t tx="ekr.20250427052747.1">if 0:  # Too slow.
    global g
    import leo.core.leoBridge as leoBridge

    controller = leoBridge.controller(gui='nullGui',
        loadPlugins=False,  # True: attempt to load plugins.
        readSettings=False,  # True: read standard settings files.
        silent=True,  # True: don't print signon messages.
        verbose=True)  # True: print informational messages.
    g = controller.globals()
    assert g.app</t>
<t tx="ekr.20250427052951.1">class SemanticCache(SqlitePickleShare):
    """The persistent cache object"""
    @others

    def _makedirs(self, fn: str, mode: int = 0o777) -&gt; None:
        raise NotImplementedError

    def _walkfiles(self, s: str, pattern: Any = None) -&gt; None:
        raise NotImplementedError
</t>
<t tx="ekr.20250427053445.1">def __init__(self, root: str) -&gt; None:  # pylint: disable=super-init-not-called
    """ctor for the SemanticCache object."""
    self.root = root  # For traces.
    dbfile = ':memory:' if g.unitTesting else root
    self.conn = sqlite3.connect(dbfile)
    self.init_dbtables(self.conn)

    def loadz(data: Value) -&gt; Optional[Value]:
        if data:
            # Retain this code for maximum compatibility.
            try:
                val = pickle.loads(zlib.decompress(data))
            except(ValueError, TypeError):
                g.es("Unpickling error - Python 3 data accessed from Python 2?")
                return None
            return val
        return None

    def dumpz(val: Value) -&gt; Value:
        try:
            # Use Python 2's highest protocol, 2, if possible
            data = pickle.dumps(val, protocol=2)
        except Exception:
            # Use best available if that doesn't work (unlikely)
            data = pickle.dumps(val, pickle.HIGHEST_PROTOCOL)
        return sqlite3.Binary(zlib.compress(data))

    self.loader = loadz
    self.dumper = dumpz
    self.reset_protocol_in_values()
</t>
<t tx="ekr.20250427065029.1">class CacheData:

    def __init__(self, mod_time: float, path: str) -&gt; None:
        self.mod_time = mod_time
        self.path = path
</t>
<t tx="ekr.20250427065404.1">"""Check source files with mypy and pylint."""
import os
import sys

g.cls()
os.chdir(r'C:\Repos\ekr-semantic-cache')
path = 'semantic_cache.py'
rc_file = r'C:\Users\Dev\.leo\.pylintrc'
g.execute_shell_commands([
    f"python -m mypy {path}",
    f"python -m pylint --rcfile {rc_file} {path}",
])
print('Done')</t>
<t tx="ekr.20250427190248.1">class CacheController:
    """The driver class for the semantic caching project."""

    @others
</t>
<t tx="ekr.20250427190307.1">def get_changed_files(self) -&gt; list[str]:
    """
    Update the tree and modification file for all new and changed files.
    """
    t1 = time.process_time()
    updated_paths: list[str] = []
    n_files = 0
    for z in core_names:
        n_files += 1
        path = f"{core_path}{os.sep}{z}.py"
        assert os.path.exists(path), repr(path)
        mod_time = os.path.getmtime(path)
        old_mod_time = self.mod_time_dict.get(path, None)
        if old_mod_time is None or mod_time &gt; old_mod_time or 'leoCache.py' in path:
            kind = 'Create' if old_mod_time is None else 'Update'
            updated_paths.append(path)
            path_s = f"{z}.py"
            print(f"{kind} {path_s} {time.ctime(mod_time)}")
            contents = g.readFile(path)
            tree = parse_ast(contents)
            self.module_dict[path] = tree
            self.mod_time_dict[path] = mod_time
        if 0:
            lines = g.splitlines(dump_ast(tree))
            for i, line in enumerate(lines[:30]):
                print(f"{i:2} {line.rstrip()}")
    t2 = time.process_time()
    self.stats.append(('Find changed', t2 - t1))
    return updated_paths
</t>
<t tx="ekr.20250427194628.1">def write_cache(self):
    """Update the persistent cache with all data."""
    t1 = time.process_time()
    self.cache['module_dict'] = self.module_dict
    self.cache['mod_time_dict'] = self.mod_time_dict
    t2 = time.process_time()
    self.stats.append(('Write cache', (t2 - t1)))
</t>
<t tx="ekr.20250427200712.1">def commit(self) -&gt; None:
    """Commit the cache."""
    self.cache.conn.commit()

def close(self) -&gt; None:
    """Close the cache."""
    self.cache.conn.close()
</t>
<t tx="ekr.20250428033750.1">def __init__(self) -&gt; None:

    # Load the persistent cache.
    t1 = time.process_time()
    self.cache = SemanticCache('semantic_cache.db')

    # Dictionaries. Keys are full path names.
    self.module_dict: dict[str, Optional[Node]] = self.cache.get('module_dict') or {}
    self.mod_time_dict: dict[str, float] = self.cache.get('mod_time_dict') or {}
    t2 = time.process_time()

    # Stats:
    self.stats: list[tuple[str, float]] = [
        ('Load', (t2 - t1)),
    ]

</t>
<t tx="ekr.20250428034510.1">def dump(self):
    """Dump the modification times of all paths in the cache."""
    g.trace(g.caller())
    # g.printObj(list(self.module_dict.keys()), tag='module_dict')
    # g.printObj(list(self.mod_time_dict.keys()), tag='mod_time_dict')
    for key, val in self.mod_time_dict.items():
        print(f"{val:&lt;18} {key}")
</t>
<t tx="ekr.20250428071526.1">def print_stats(self, updated_files: list[str]) -&gt; None:
    n = len(updated_files)
    total = 0.0
    print(f"{n} updated file{g.plural(n)}")
    for key, value in self.stats:
        total += value
        print(f"{key:&gt;12} {value:.2f} sec.")
    print(f"{'Total':&gt;12} {total:.2f} sec.")
</t>
<t tx="ekr.20250428100117.1">def compute_diffs(self, updated_files: list[str]) -&gt; None:
    pass
</t>
<t tx="ekr.20250430053636.10">def checkRecursive(paths, reporter):
    """
    Recursively check all source files in C{paths}.

    @param paths: A list of paths to Python source files and directories
        containing Python source files.
    @param reporter: A L{Reporter} where all of the warnings and errors
        will be reported to.
    @return: The number of warnings found.
    """
    warnings = 0
    for sourcePath in iterSourceCode(paths):
        warnings += checkPath(sourcePath, reporter)
    return warnings
</t>
<t tx="ekr.20250430053636.100">@property
def futuresAllowed(self):
    if not all(isinstance(scope, ModuleScope)
               for scope in self.scopeStack):
        return False

    return self.scope._futures_allowed
</t>
<t tx="ekr.20250430053636.101">@futuresAllowed.setter
def futuresAllowed(self, value):
    assert value is False
    if isinstance(self.scope, ModuleScope):
        self.scope._futures_allowed = False
</t>
<t tx="ekr.20250430053636.102">@property
def annotationsFutureEnabled(self):
    scope = self.scopeStack[0]
    if not isinstance(scope, ModuleScope):
        return False
    return scope._annotations_future_enabled
</t>
<t tx="ekr.20250430053636.103">@annotationsFutureEnabled.setter
def annotationsFutureEnabled(self, value):
    assert value is True
    assert isinstance(self.scope, ModuleScope)
    self.scope._annotations_future_enabled = True
</t>
<t tx="ekr.20250430053636.104">@property
def scope(self):
    return self.scopeStack[-1]
</t>
<t tx="ekr.20250430053636.105">@contextlib.contextmanager
def in_scope(self, cls):
    self.scopeStack.append(cls())
    try:
        yield
    finally:
        self.deadScopes.append(self.scopeStack.pop())
</t>
<t tx="ekr.20250430053636.106">def checkDeadScopes(self):
    """
    Look at scopes which have been fully examined and report names in them
    which were imported but unused.
    """
    for scope in self.deadScopes:
        # imports in classes are public members
        if isinstance(scope, ClassScope):
            continue

        if isinstance(scope, FunctionScope):
            for name, binding in scope.unused_assignments():
                self.report(messages.UnusedVariable, binding.source, name)
            for name, binding in scope.unused_annotations():
                self.report(messages.UnusedAnnotation, binding.source, name)

        all_binding = scope.get('__all__')
        if all_binding and not isinstance(all_binding, ExportBinding):
            all_binding = None

        if all_binding:
            all_names = set(all_binding.names)
            undefined = [
                name for name in all_binding.names
                if name not in scope
            ]
        else:
            all_names = undefined = []

        if undefined:
            if not scope.importStarred and \
               os.path.basename(self.filename) != '__init__.py':
                # Look for possible mistakes in the export list
                for name in undefined:
                    self.report(messages.UndefinedExport,
                                scope['__all__'].source, name)

            # mark all import '*' as used by the undefined in __all__
            if scope.importStarred:
                from_list = []
                for binding in scope.values():
                    if isinstance(binding, StarImportation):
                        binding.used = all_binding
                        from_list.append(binding.fullName)
                # report * usage, with a list of possible sources
                from_list = ', '.join(sorted(from_list))
                for name in undefined:
                    self.report(messages.ImportStarUsage,
                                scope['__all__'].source, name, from_list)

        # Look for imported names that aren't used.
        for value in scope.values():
            if isinstance(value, Importation):
                used = value.used or value.name in all_names
                if not used:
                    messg = messages.UnusedImport
                    self.report(messg, value.source, str(value))
                for node in value.redefined:
                    if isinstance(self.getParent(node), FOR_TYPES):
                        messg = messages.ImportShadowedByLoopVar
                    elif used:
                        continue
                    else:
                        messg = messages.RedefinedWhileUnused
                    self.report(messg, node, value.name, value.source)
</t>
<t tx="ekr.20250430053636.107">def report(self, messageClass, *args, **kwargs):
    self.messages.append(messageClass(self.filename, *args, **kwargs))
</t>
<t tx="ekr.20250430053636.108">def getParent(self, node):
    # Lookup the first parent which is not Tuple, List or Starred
    while True:
        node = node._pyflakes_parent
        if not hasattr(node, 'elts') and not hasattr(node, 'ctx'):
            return node
</t>
<t tx="ekr.20250430053636.109">def getCommonAncestor(self, lnode, rnode, stop):
    if (
            stop in (lnode, rnode) or
            not (
                hasattr(lnode, '_pyflakes_parent') and
                hasattr(rnode, '_pyflakes_parent')
            )
    ):
        return None
    if lnode is rnode:
        return lnode

    if (lnode._pyflakes_depth &gt; rnode._pyflakes_depth):
        return self.getCommonAncestor(lnode._pyflakes_parent, rnode, stop)
    if (lnode._pyflakes_depth &lt; rnode._pyflakes_depth):
        return self.getCommonAncestor(lnode, rnode._pyflakes_parent, stop)
    return self.getCommonAncestor(
        lnode._pyflakes_parent,
        rnode._pyflakes_parent,
        stop,
    )
</t>
<t tx="ekr.20250430053636.11">def _exitOnSignal(sigName, message):
    """Handles a signal with sys.exit.

    Some of these signals (SIGPIPE, for example) don't exist or are invalid on
    Windows. So, ignore errors that might arise.
    """
    import signal

    try:
        sigNumber = getattr(signal, sigName)
    except AttributeError:
        # the signal constants defined in the signal module are defined by
        # whether the C library supports them or not. So, SIGPIPE might not
        # even be defined.
        return

    def handler(sig, f):
        sys.exit(message)

    try:
        signal.signal(sigNumber, handler)
    except ValueError:
        # It's also possible the signal is defined, but then it's invalid. In
        # this case, signal.signal raises ValueError.
        pass
</t>
<t tx="ekr.20250430053636.110">def descendantOf(self, node, ancestors, stop):
    for a in ancestors:
        if self.getCommonAncestor(node, a, stop):
            return True
    return False
</t>
<t tx="ekr.20250430053636.111">def _getAncestor(self, node, ancestor_type):
    parent = node
    while True:
        if parent is self.root:
            return None
        parent = self.getParent(parent)
        if isinstance(parent, ancestor_type):
            return parent
</t>
<t tx="ekr.20250430053636.112">def getScopeNode(self, node):
    return self._getAncestor(node, tuple(Checker._ast_node_scope.keys()))
</t>
<t tx="ekr.20250430053636.113">def differentForks(self, lnode, rnode):
    """True, if lnode and rnode are located on different forks of IF/TRY"""
    ancestor = self.getCommonAncestor(lnode, rnode, self.root)
    parts = getAlternatives(ancestor)
    if parts:
        for items in parts:
            if self.descendantOf(lnode, items, ancestor) ^ \
               self.descendantOf(rnode, items, ancestor):
                return True
    return False
</t>
<t tx="ekr.20250430053636.114">def addBinding(self, node, value):
    """
    Called when a binding is altered.

    - `node` is the statement responsible for the change
    - `value` is the new value, a Binding instance
    """
    # assert value.source in (node, node._pyflakes_parent):
    for scope in self.scopeStack[::-1]:
        if value.name in scope:
            break
    existing = scope.get(value.name)

    if (existing and not isinstance(existing, Builtin) and
            not self.differentForks(node, existing.source)):

        parent_stmt = self.getParent(value.source)
        if isinstance(existing, Importation) and isinstance(parent_stmt, FOR_TYPES):
            self.report(messages.ImportShadowedByLoopVar,
                        node, value.name, existing.source)

        elif scope is self.scope:
            if (
                    (not existing.used and value.redefines(existing)) and
                    (value.name != '_' or isinstance(existing, Importation)) and
                    not is_typing_overload(existing, self.scopeStack)
            ):
                self.report(messages.RedefinedWhileUnused,
                            node, value.name, existing.source)

        elif isinstance(existing, Importation) and value.redefines(existing):
            existing.redefined.append(node)

    if value.name in self.scope:
        # then assume the rebound name is used as a global or within a loop
        value.used = self.scope[value.name].used

    # don't treat annotations as assignments if there is an existing value
    # in scope
    if value.name not in self.scope or not isinstance(value, Annotation):
        cur_scope_pos = -1
        # As per PEP 572, use scope in which outermost generator is defined
        while (
            isinstance(value, NamedExprAssignment) and
            isinstance(self.scopeStack[cur_scope_pos], GeneratorScope)
        ):
            cur_scope_pos -= 1
        self.scopeStack[cur_scope_pos][value.name] = value
</t>
<t tx="ekr.20250430053636.115">def _unknown_handler(self, node):
    # this environment variable configures whether to error on unknown
    # ast types.
    #
    # this is silent by default but the error is enabled for the pyflakes
    # testsuite.
    #
    # this allows new syntax to be added to python without *requiring*
    # changes from the pyflakes side.  but will still produce an error
    # in the pyflakes testsuite (so more specific handling can be added if
    # needed).
    if os.environ.get('PYFLAKES_ERROR_UNKNOWN'):
        raise NotImplementedError(f'Unexpected type: {type(node)}')
    else:
        self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.116">def getNodeHandler(self, node_class):
    try:
        return self._nodeHandlers[node_class]
    except KeyError:
        nodeType = node_class.__name__.upper()
    self._nodeHandlers[node_class] = handler = getattr(
        self, nodeType, self._unknown_handler,
    )
    return handler
</t>
<t tx="ekr.20250430053636.117">def handleNodeLoad(self, node, parent):
    name = getNodeName(node)
    if not name:
        return

    # only the following can access class scoped variables (since classes
    # aren't really a scope)
    # - direct accesses (not within a nested scope)
    # - generators
    # - type annotations (for generics, etc.)
    can_access_class_vars = None
    importStarred = None

    # try enclosing function scopes and global scope
    for scope in self.scopeStack[-1::-1]:
        if isinstance(scope, ClassScope):
            if name == '__class__':
                return
            elif can_access_class_vars is False:
                # only generators used in a class scope can access the
                # names of the class. this is skipped during the first
                # iteration
                continue

        binding = scope.get(name, None)
        if isinstance(binding, Annotation) and not self._in_postponed_annotation:
            scope[name].used = (self.scope, node)
            continue

        if name == 'print' and isinstance(binding, Builtin):
            if (isinstance(parent, ast.BinOp) and
                    isinstance(parent.op, ast.RShift)):
                self.report(messages.InvalidPrintSyntax, node)

        try:
            scope[name].used = (self.scope, node)

            # if the name of SubImportation is same as
            # alias of other Importation and the alias
            # is used, SubImportation also should be marked as used.
            n = scope[name]
            if isinstance(n, Importation) and n._has_alias():
                try:
                    scope[n.fullName].used = (self.scope, node)
                except KeyError:
                    pass
        except KeyError:
            pass
        else:
            return

        importStarred = importStarred or scope.importStarred

        if can_access_class_vars is not False:
            can_access_class_vars = isinstance(
                scope, (TypeScope, GeneratorScope),
            )

    if importStarred:
        from_list = []

        for scope in self.scopeStack[-1::-1]:
            for binding in scope.values():
                if isinstance(binding, StarImportation):
                    # mark '*' imports as used for each scope
                    binding.used = (self.scope, node)
                    from_list.append(binding.fullName)

        # report * usage, with a list of possible sources
        from_list = ', '.join(sorted(from_list))
        self.report(messages.ImportStarUsage, node, name, from_list)
        return

    if name == '__path__' and os.path.basename(self.filename) == '__init__.py':
        # the special name __path__ is valid only in packages
        return

    if name in DetectClassScopedMagic.names and isinstance(self.scope, ClassScope):
        return

    # protected with a NameError handler?
    if 'NameError' not in self.exceptHandlers[-1]:
        self.report(messages.UndefinedName, node, name)
</t>
<t tx="ekr.20250430053636.118">def handleNodeStore(self, node):
    name = getNodeName(node)
    if not name:
        return
    # if the name hasn't already been defined in the current scope
    if isinstance(self.scope, FunctionScope) and name not in self.scope:
        # for each function or module scope above us
        for scope in self.scopeStack[:-1]:
            if not isinstance(scope, (FunctionScope, ModuleScope)):
                continue
            # if the name was defined in that scope, and the name has
            # been accessed already in the current scope, and hasn't
            # been declared global
            used = name in scope and scope[name].used
            if used and used[0] is self.scope and name not in self.scope.globals:
                # then it's probably a mistake
                self.report(messages.UndefinedLocal,
                            scope[name].used[1], name, scope[name].source)
                break

    parent_stmt = self.getParent(node)
    if isinstance(parent_stmt, ast.AnnAssign) and parent_stmt.value is None:
        binding = Annotation(name, node)
    elif isinstance(parent_stmt, (FOR_TYPES, ast.comprehension)) or (
            parent_stmt != node._pyflakes_parent and
            not self.isLiteralTupleUnpacking(parent_stmt)):
        binding = Binding(name, node)
    elif (
            name == '__all__' and
            isinstance(self.scope, ModuleScope) and
            isinstance(
                node._pyflakes_parent,
                (ast.Assign, ast.AugAssign, ast.AnnAssign)
            )
    ):
        binding = ExportBinding(name, node._pyflakes_parent, self.scope)
    elif isinstance(parent_stmt, ast.NamedExpr):
        binding = NamedExprAssignment(name, node)
    else:
        binding = Assignment(name, node)
    self.addBinding(node, binding)
</t>
<t tx="ekr.20250430053636.119">def handleNodeDelete(self, node):

    def on_conditional_branch():
        """
        Return `True` if node is part of a conditional body.
        """
        current = getattr(node, '_pyflakes_parent', None)
        while current:
            if isinstance(current, (ast.If, ast.While, ast.IfExp)):
                return True
            current = getattr(current, '_pyflakes_parent', None)
        return False

    name = getNodeName(node)
    if not name:
        return

    if on_conditional_branch():
        # We cannot predict if this conditional branch is going to
        # be executed.
        return

    if isinstance(self.scope, FunctionScope) and name in self.scope.globals:
        self.scope.globals.remove(name)
    else:
        try:
            del self.scope[name]
        except KeyError:
            self.report(messages.UndefinedName, node, name)
</t>
<t tx="ekr.20250430053636.12">def _get_version():
    """
    Retrieve and format package version along with python version &amp; OS used
    """
    return ('%s Python %s on %s' %
            (__version__, platform.python_version(), platform.system()))
</t>
<t tx="ekr.20250430053636.120">@contextlib.contextmanager
def _enter_annotation(self, ann_type=AnnotationState.BARE):
    orig, self._in_annotation = self._in_annotation, ann_type
    try:
        yield
    finally:
        self._in_annotation = orig
</t>
<t tx="ekr.20250430053636.121">@property
def _in_postponed_annotation(self):
    return (
        self._in_annotation == AnnotationState.STRING or
        self.annotationsFutureEnabled
    )
</t>
<t tx="ekr.20250430053636.122">def handleChildren(self, tree, omit=None):
    for node in iter_child_nodes(tree, omit=omit):
        self.handleNode(node, tree)
</t>
<t tx="ekr.20250430053636.123">def isLiteralTupleUnpacking(self, node):
    if isinstance(node, ast.Assign):
        for child in node.targets + [node.value]:
            if not hasattr(child, 'elts'):
                return False
        return True
</t>
<t tx="ekr.20250430053636.124">def isDocstring(self, node):
    """
    Determine if the given node is a docstring, as long as it is at the
    correct place in the node tree.
    """
    return (
        isinstance(node, ast.Expr) and
        isinstance(node.value, ast.Constant) and
        isinstance(node.value.value, str)
    )
</t>
<t tx="ekr.20250430053636.125">def getDocstring(self, node):
    if (
            isinstance(node, ast.Expr) and
            isinstance(node.value, ast.Constant) and
            isinstance(node.value.value, str)
    ):
        return node.value.value, node.lineno - 1
    else:
        return None, None
</t>
<t tx="ekr.20250430053636.126">def handleNode(self, node, parent):
    if node is None:
        return
    if self.offset and getattr(node, 'lineno', None) is not None:
        node.lineno += self.offset[0]
        node.col_offset += self.offset[1]
    if (
            self.futuresAllowed and
            self.nodeDepth == 0 and
            not isinstance(node, ast.ImportFrom) and
            not self.isDocstring(node)
    ):
        self.futuresAllowed = False
    self.nodeDepth += 1
    node._pyflakes_depth = self.nodeDepth
    node._pyflakes_parent = parent
    try:
        handler = self.getNodeHandler(node.__class__)
        handler(node)
    finally:
        self.nodeDepth -= 1
</t>
<t tx="ekr.20250430053636.127">_getDoctestExamples = doctest.DocTestParser().get_examples

def handleDoctests(self, node):
    try:
        (docstring, node_lineno) = self.getDocstring(node.body[0])
        examples = docstring and self._getDoctestExamples(docstring)
    except (ValueError, IndexError):
        # e.g. line 6 of the docstring for &lt;string&gt; has inconsistent
        # leading whitespace: ...
        return
    if not examples:
        return

    # Place doctest in module scope
    saved_stack = self.scopeStack
    self.scopeStack = [self.scopeStack[0]]
    node_offset = self.offset or (0, 0)
    with self.in_scope(DoctestScope):
        if '_' not in self.scopeStack[0]:
            self.addBinding(None, Builtin('_'))
        for example in examples:
            try:
                tree = ast.parse(example.source, "&lt;doctest&gt;")
            except SyntaxError as e:
                position = (node_lineno + example.lineno + e.lineno,
                            example.indent + 4 + (e.offset or 0))
                self.report(messages.DoctestSyntaxError, node, position)
            else:
                self.offset = (node_offset[0] + node_lineno + example.lineno,
                               node_offset[1] + example.indent + 4)
                self.handleChildren(tree)
                self.offset = node_offset
    self.scopeStack = saved_stack
</t>
<t tx="ekr.20250430053636.128">@in_string_annotation
def handleStringAnnotation(self, s, node, ref_lineno, ref_col_offset, err):
    try:
        tree = ast.parse(s)
    except SyntaxError:
        self.report(err, node, s)
        return

    body = tree.body
    if len(body) != 1 or not isinstance(body[0], ast.Expr):
        self.report(err, node, s)
        return

    parsed_annotation = tree.body[0].value
    for descendant in ast.walk(parsed_annotation):
        if (
                'lineno' in descendant._attributes and
                'col_offset' in descendant._attributes
        ):
            descendant.lineno = ref_lineno
            descendant.col_offset = ref_col_offset

    self.handleNode(parsed_annotation, node)
</t>
<t tx="ekr.20250430053636.129">def handle_annotation_always_deferred(self, annotation, parent):
    fn = in_annotation(Checker.handleNode)
    self.deferFunction(lambda: fn(self, annotation, parent))
</t>
<t tx="ekr.20250430053636.13">def main(prog=None, args=None):
    """Entry point for the script "pyflakes"."""
    import argparse

    # Handle "Keyboard Interrupt" and "Broken pipe" gracefully
    _exitOnSignal('SIGINT', '... stopped')
    _exitOnSignal('SIGPIPE', 1)

    parser = argparse.ArgumentParser(prog=prog,
                                     description='Check Python source files for errors')
    parser.add_argument('-V', '--version', action='version', version=_get_version())
    parser.add_argument('path', nargs='*',
                        help='Path(s) of Python file(s) to check. STDIN if not given.')
    args = parser.parse_args(args=args).path
    reporter = modReporter._makeDefaultReporter()
    if args:
        warnings = checkRecursive(args, reporter)
    else:
        warnings = check(sys.stdin.read(), '&lt;stdin&gt;', reporter)
    raise SystemExit(warnings &gt; 0)
</t>
<t tx="ekr.20250430053636.130">@in_annotation
def handleAnnotation(self, annotation, node):
    if (
            isinstance(annotation, ast.Constant) and
            isinstance(annotation.value, str)
    ):
        # Defer handling forward annotation.
        self.deferFunction(functools.partial(
            self.handleStringAnnotation,
            annotation.value,
            node,
            annotation.lineno,
            annotation.col_offset,
            messages.ForwardAnnotationSyntaxError,
        ))
    elif self.annotationsFutureEnabled:
        self.handle_annotation_always_deferred(annotation, node)
    else:
        self.handleNode(annotation, node)
</t>
<t tx="ekr.20250430053636.131">def ignore(self, node):
    pass
</t>
<t tx="ekr.20250430053636.132"># "stmt" type nodes
DELETE = FOR = ASYNCFOR = WHILE = WITH = WITHITEM = ASYNCWITH = \
    EXPR = ASSIGN = handleChildren

PASS = ignore

# "expr" type nodes
BOOLOP = UNARYOP = SET = ATTRIBUTE = STARRED = NAMECONSTANT = \
    NAMEDEXPR = handleChildren

def SUBSCRIPT(self, node):
    if _is_name_or_attr(node.value, 'Literal'):
        with self._enter_annotation(AnnotationState.NONE):
            self.handleChildren(node)
    elif _is_name_or_attr(node.value, 'Annotated'):
        self.handleNode(node.value, node)

        # py39+
        if isinstance(node.slice, ast.Tuple):
            slice_tuple = node.slice
        # &lt;py39
        elif (
                isinstance(node.slice, ast.Index) and
                isinstance(node.slice.value, ast.Tuple)
        ):
            slice_tuple = node.slice.value
        else:
            slice_tuple = None

        # not a multi-arg `Annotated`
        if slice_tuple is None or len(slice_tuple.elts) &lt; 2:
            self.handleNode(node.slice, node)
        else:
            # the first argument is the type
            self.handleNode(slice_tuple.elts[0], node)
            # the rest of the arguments are not
            with self._enter_annotation(AnnotationState.NONE):
                for arg in slice_tuple.elts[1:]:
                    self.handleNode(arg, node)

        self.handleNode(node.ctx, node)
    else:
        if _is_any_typing_member(node.value, self.scopeStack):
            with self._enter_annotation():
                self.handleChildren(node)
        else:
            self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.133">def _handle_string_dot_format(self, node):
    try:
        placeholders = tuple(parse_format_string(node.func.value.value))
    except ValueError as e:
        self.report(messages.StringDotFormatInvalidFormat, node, e)
        return

    auto = None
    next_auto = 0

    placeholder_positional = set()
    placeholder_named = set()

    def _add_key(fmtkey):
        """Returns True if there is an error which should early-exit"""
        nonlocal auto, next_auto

        if fmtkey is None:  # end of string or `{` / `}` escapes
            return False

        # attributes / indices are allowed in `.format(...)`
        fmtkey, _, _ = fmtkey.partition('.')
        fmtkey, _, _ = fmtkey.partition('[')

        try:
            fmtkey = int(fmtkey)
        except ValueError:
            pass
        else:  # fmtkey was an integer
            if auto is True:
                self.report(messages.StringDotFormatMixingAutomatic, node)
                return True
            else:
                auto = False

        if fmtkey == '':
            if auto is False:
                self.report(messages.StringDotFormatMixingAutomatic, node)
                return True
            else:
                auto = True

            fmtkey = next_auto
            next_auto += 1

        if isinstance(fmtkey, int):
            placeholder_positional.add(fmtkey)
        else:
            placeholder_named.add(fmtkey)

        return False

    for _, fmtkey, spec, _ in placeholders:
        if _add_key(fmtkey):
            return

        # spec can also contain format specifiers
        if spec is not None:
            try:
                spec_placeholders = tuple(parse_format_string(spec))
            except ValueError as e:
                self.report(messages.StringDotFormatInvalidFormat, node, e)
                return

            for _, spec_fmtkey, spec_spec, _ in spec_placeholders:
                # can't recurse again
                if spec_spec is not None and '{' in spec_spec:
                    self.report(
                        messages.StringDotFormatInvalidFormat,
                        node,
                        'Max string recursion exceeded',
                    )
                    return
                if _add_key(spec_fmtkey):
                    return

    # bail early if there is *args or **kwargs
    if (
            # *args
            any(isinstance(arg, ast.Starred) for arg in node.args) or
            # **kwargs
            any(kwd.arg is None for kwd in node.keywords)
    ):
        return

    substitution_positional = set(range(len(node.args)))
    substitution_named = {kwd.arg for kwd in node.keywords}

    extra_positional = substitution_positional - placeholder_positional
    extra_named = substitution_named - placeholder_named

    missing_arguments = (
        (placeholder_positional | placeholder_named) -
        (substitution_positional | substitution_named)
    )

    if extra_positional:
        self.report(
            messages.StringDotFormatExtraPositionalArguments,
            node,
            ', '.join(sorted(str(x) for x in extra_positional)),
        )
    if extra_named:
        self.report(
            messages.StringDotFormatExtraNamedArguments,
            node,
            ', '.join(sorted(extra_named)),
        )
    if missing_arguments:
        self.report(
            messages.StringDotFormatMissingArgument,
            node,
            ', '.join(sorted(str(x) for x in missing_arguments)),
        )
</t>
<t tx="ekr.20250430053636.134">def CALL(self, node):
    if (
            isinstance(node.func, ast.Attribute) and
            isinstance(node.func.value, ast.Constant) and
            isinstance(node.func.value.value, str) and
            node.func.attr == 'format'
    ):
        self._handle_string_dot_format(node)

    omit = []
    annotated = []
    not_annotated = []

    if (
        _is_typing(node.func, 'cast', self.scopeStack) and
        len(node.args) &gt;= 1
    ):
        with self._enter_annotation():
            self.handleNode(node.args[0], node)

    elif _is_typing(node.func, 'TypeVar', self.scopeStack):

        # TypeVar("T", "int", "str")
        omit += ["args"]
        annotated += [arg for arg in node.args[1:]]

        # TypeVar("T", bound="str")
        omit += ["keywords"]
        annotated += [k.value for k in node.keywords if k.arg == "bound"]
        not_annotated += [
            (k, ["value"] if k.arg == "bound" else None)
            for k in node.keywords
        ]

    elif _is_typing(node.func, "TypedDict", self.scopeStack):
        # TypedDict("a", {"a": int})
        if len(node.args) &gt; 1 and isinstance(node.args[1], ast.Dict):
            omit += ["args"]
            annotated += node.args[1].values
            not_annotated += [
                (arg, ["values"] if i == 1 else None)
                for i, arg in enumerate(node.args)
            ]

        # TypedDict("a", a=int)
        omit += ["keywords"]
        annotated += [k.value for k in node.keywords]
        not_annotated += [(k, ["value"]) for k in node.keywords]

    elif _is_typing(node.func, "NamedTuple", self.scopeStack):
        # NamedTuple("a", [("a", int)])
        if (
            len(node.args) &gt; 1 and
            isinstance(node.args[1], (ast.Tuple, ast.List)) and
            all(isinstance(x, (ast.Tuple, ast.List)) and
                len(x.elts) == 2 for x in node.args[1].elts)
        ):
            omit += ["args"]
            annotated += [elt.elts[1] for elt in node.args[1].elts]
            not_annotated += [(elt.elts[0], None) for elt in node.args[1].elts]
            not_annotated += [
                (arg, ["elts"] if i == 1 else None)
                for i, arg in enumerate(node.args)
            ]
            not_annotated += [(elt, "elts") for elt in node.args[1].elts]

        # NamedTuple("a", a=int)
        omit += ["keywords"]
        annotated += [k.value for k in node.keywords]
        not_annotated += [(k, ["value"]) for k in node.keywords]

    if omit:
        with self._enter_annotation(AnnotationState.NONE):
            for na_node, na_omit in not_annotated:
                self.handleChildren(na_node, omit=na_omit)
            self.handleChildren(node, omit=omit)

        with self._enter_annotation():
            for annotated_node in annotated:
                self.handleNode(annotated_node, node)
    else:
        self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.135">def _handle_percent_format(self, node):
    try:
        placeholders = parse_percent_format(node.left.value)
    except ValueError:
        self.report(
            messages.PercentFormatInvalidFormat,
            node,
            'incomplete format',
        )
        return

    named = set()
    positional_count = 0
    positional = None
    for _, placeholder in placeholders:
        if placeholder is None:
            continue
        name, _, width, precision, conversion = placeholder

        if conversion == '%':
            continue

        if conversion not in VALID_CONVERSIONS:
            self.report(
                messages.PercentFormatUnsupportedFormatCharacter,
                node,
                conversion,
            )

        if positional is None and conversion:
            positional = name is None

        for part in (width, precision):
            if part is not None and '*' in part:
                if not positional:
                    self.report(
                        messages.PercentFormatStarRequiresSequence,
                        node,
                    )
                else:
                    positional_count += 1

        if positional and name is not None:
            self.report(
                messages.PercentFormatMixedPositionalAndNamed,
                node,
            )
            return
        elif not positional and name is None:
            self.report(
                messages.PercentFormatMixedPositionalAndNamed,
                node,
            )
            return

        if positional:
            positional_count += 1
        else:
            named.add(name)

    if (
            isinstance(node.right, (ast.List, ast.Tuple)) and
            # does not have any *splats (py35+ feature)
            not any(
                isinstance(elt, ast.Starred)
                for elt in node.right.elts
            )
    ):
        substitution_count = len(node.right.elts)
        if positional and positional_count != substitution_count:
            self.report(
                messages.PercentFormatPositionalCountMismatch,
                node,
                positional_count,
                substitution_count,
            )
        elif not positional:
            self.report(messages.PercentFormatExpectedMapping, node)

    if (
            isinstance(node.right, ast.Dict) and
            all(
                isinstance(k, ast.Constant) and isinstance(k.value, str)
                for k in node.right.keys
            )
    ):
        if positional and positional_count &gt; 1:
            self.report(messages.PercentFormatExpectedSequence, node)
            return

        substitution_keys = {k.value for k in node.right.keys}
        extra_keys = substitution_keys - named
        missing_keys = named - substitution_keys
        if not positional and extra_keys:
            self.report(
                messages.PercentFormatExtraNamedArguments,
                node,
                ', '.join(sorted(extra_keys)),
            )
        if not positional and missing_keys:
            self.report(
                messages.PercentFormatMissingArgument,
                node,
                ', '.join(sorted(missing_keys)),
            )
</t>
<t tx="ekr.20250430053636.136">def BINOP(self, node):
    if (
            isinstance(node.op, ast.Mod) and
            isinstance(node.left, ast.Constant) and
            isinstance(node.left.value, str)
    ):
        self._handle_percent_format(node)
    self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.137">def CONSTANT(self, node):
    if isinstance(node.value, str) and self._in_annotation:
        fn = functools.partial(
            self.handleStringAnnotation,
            node.value,
            node,
            node.lineno,
            node.col_offset,
            messages.ForwardAnnotationSyntaxError,
        )
        self.deferFunction(fn)
</t>
<t tx="ekr.20250430053636.138"># "slice" type nodes
SLICE = EXTSLICE = INDEX = handleChildren

# expression contexts are node instances too, though being constants
LOAD = STORE = DEL = AUGLOAD = AUGSTORE = PARAM = ignore

# same for operators
AND = OR = ADD = SUB = MULT = DIV = MOD = POW = LSHIFT = RSHIFT = \
    BITOR = BITXOR = BITAND = FLOORDIV = INVERT = NOT = UADD = USUB = \
    EQ = NOTEQ = LT = LTE = GT = GTE = IS = ISNOT = IN = NOTIN = \
    MATMULT = ignore

def RAISE(self, node):
    self.handleChildren(node)

    arg = node.exc

    if isinstance(arg, ast.Call):
        if is_notimplemented_name_node(arg.func):
            # Handle "raise NotImplemented(...)"
            self.report(messages.RaiseNotImplemented, node)
    elif is_notimplemented_name_node(arg):
        # Handle "raise NotImplemented"
        self.report(messages.RaiseNotImplemented, node)
</t>
<t tx="ekr.20250430053636.139"># additional node types
COMPREHENSION = KEYWORD = FORMATTEDVALUE = handleChildren

_in_fstring = False

def JOINEDSTR(self, node):
    if (
            # the conversion / etc. flags are parsed as f-strings without
            # placeholders
            not self._in_fstring and
            not any(isinstance(x, ast.FormattedValue) for x in node.values)
    ):
        self.report(messages.FStringMissingPlaceholders, node)

    self._in_fstring, orig = True, self._in_fstring
    try:
        self.handleChildren(node)
    finally:
        self._in_fstring = orig
</t>
<t tx="ekr.20250430053636.14">"""
Main module.

Implement the central Checker class.
Also, it models the Bindings and Scopes.
"""
import __future__
import builtins
import ast
import collections
import contextlib
import doctest
import functools
import os
import re
import string
import sys
import warnings

from pyflakes import messages

PYPY = hasattr(sys, 'pypy_version_info')

builtin_vars = dir(builtins)

parse_format_string = string.Formatter().parse


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053636.140">def DICT(self, node):
    # Complain if there are duplicate keys with different values
    # If they have the same value it's not going to cause potentially
    # unexpected behaviour so we'll not complain.
    keys = [
        convert_to_value(key) for key in node.keys
    ]

    key_counts = counter(keys)
    duplicate_keys = [
        key for key, count in key_counts.items()
        if count &gt; 1
    ]

    for key in duplicate_keys:
        key_indices = [i for i, i_key in enumerate(keys) if i_key == key]

        values = counter(
            convert_to_value(node.values[index])
            for index in key_indices
        )
        if any(count == 1 for value, count in values.items()):
            for key_index in key_indices:
                key_node = node.keys[key_index]
                if isinstance(key, VariableKey):
                    self.report(messages.MultiValueRepeatedKeyVariable,
                                key_node,
                                key.name)
                else:
                    self.report(
                        messages.MultiValueRepeatedKeyLiteral,
                        key_node,
                        key,
                    )
    self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.141">def IF(self, node):
    if isinstance(node.test, ast.Tuple) and node.test.elts != []:
        self.report(messages.IfTuple, node)
    self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.142">IFEXP = IF

def ASSERT(self, node):
    if isinstance(node.test, ast.Tuple) and node.test.elts != []:
        self.report(messages.AssertTuple, node)
    self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.143">def GLOBAL(self, node):
    """
    Keep track of globals declarations.
    """
    global_scope_index = 1 if self._in_doctest() else 0
    global_scope = self.scopeStack[global_scope_index]

    # Ignore 'global' statement in global scope.
    if self.scope is not global_scope:

        # One 'global' statement can bind multiple (comma-delimited) names.
        for node_name in node.names:
            node_value = Assignment(node_name, node)

            # Remove UndefinedName messages already reported for this name.
            # TODO: if the global is not used in this scope, it does not
            # become a globally defined name.  See test_unused_global.
            self.messages = [
                m for m in self.messages if not
                isinstance(m, messages.UndefinedName) or
                m.message_args[0] != node_name]

            # Bind name to global scope if it doesn't exist already.
            global_scope.setdefault(node_name, node_value)

            # Bind name to non-global scopes, but as already "used".
            node_value.used = (global_scope, node)
            for scope in self.scopeStack[global_scope_index + 1:]:
                scope[node_name] = node_value
</t>
<t tx="ekr.20250430053636.144">NONLOCAL = GLOBAL

def GENERATOREXP(self, node):
    with self.in_scope(GeneratorScope):
        self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.145">LISTCOMP = DICTCOMP = SETCOMP = GENERATOREXP

def NAME(self, node):
    """
    Handle occurrence of Name (which can be a load/store/delete access.)
    """
    # Locate the name in locals / function / globals scopes.
    if isinstance(node.ctx, ast.Load):
        self.handleNodeLoad(node, self.getParent(node))
        if (node.id == 'locals' and isinstance(self.scope, FunctionScope) and
                isinstance(node._pyflakes_parent, ast.Call)):
            # we are doing locals() call in current scope
            self.scope.usesLocals = True
    elif isinstance(node.ctx, ast.Store):
        self.handleNodeStore(node)
    elif isinstance(node.ctx, ast.Del):
        self.handleNodeDelete(node)
    else:
        # Unknown context
        raise RuntimeError(f"Got impossible expression context: {node.ctx!r}")
</t>
<t tx="ekr.20250430053636.146">def CONTINUE(self, node):
    # Walk the tree up until we see a loop (OK), a function or class
    # definition (not OK), for 'continue', a finally block (not OK), or
    # the top module scope (not OK)
    n = node
    while hasattr(n, '_pyflakes_parent'):
        n, n_child = n._pyflakes_parent, n
        if isinstance(n, (ast.While, ast.For, ast.AsyncFor)):
            # Doesn't apply unless it's in the loop itself
            if n_child not in n.orelse:
                return
        if isinstance(n, (ast.FunctionDef, ast.ClassDef)):
            break
    if isinstance(node, ast.Continue):
        self.report(messages.ContinueOutsideLoop, node)
    else:  # ast.Break
        self.report(messages.BreakOutsideLoop, node)
</t>
<t tx="ekr.20250430053636.147">BREAK = CONTINUE

def RETURN(self, node):
    if isinstance(self.scope, (ClassScope, ModuleScope)):
        self.report(messages.ReturnOutsideFunction, node)
        return

    if (
        node.value and
        hasattr(self.scope, 'returnValue') and
        not self.scope.returnValue
    ):
        self.scope.returnValue = node.value
    self.handleNode(node.value, node)
</t>
<t tx="ekr.20250430053636.148">def YIELD(self, node):
    if isinstance(self.scope, (ClassScope, ModuleScope)):
        self.report(messages.YieldOutsideFunction, node)
        return

    self.handleNode(node.value, node)
</t>
<t tx="ekr.20250430053636.149">AWAIT = YIELDFROM = YIELD

def FUNCTIONDEF(self, node):
    for deco in node.decorator_list:
        self.handleNode(deco, node)

    with self._type_param_scope(node):
        self.LAMBDA(node)

    self.addBinding(node, FunctionDefinition(node.name, node))
    # doctest does not process doctest within a doctest,
    # or in nested functions.
    if (self.withDoctest and
            not self._in_doctest() and
            not isinstance(self.scope, FunctionScope)):
        self.deferFunction(lambda: self.handleDoctests(node))
</t>
<t tx="ekr.20250430053636.15">def getAlternatives(n):
    if isinstance(n, ast.If):
        return [n.body]
    elif isinstance(n, ast.Try):
        return [n.body + n.orelse] + [[hdl] for hdl in n.handlers]
    elif sys.version_info &gt;= (3, 10) and isinstance(n, ast.Match):
        return [mc.body for mc in n.cases]
</t>
<t tx="ekr.20250430053636.150">ASYNCFUNCTIONDEF = FUNCTIONDEF

def LAMBDA(self, node):
    args = []
    annotations = []

    for arg in node.args.posonlyargs:
        args.append(arg.arg)
        annotations.append(arg.annotation)
    for arg in node.args.args + node.args.kwonlyargs:
        args.append(arg.arg)
        annotations.append(arg.annotation)
    defaults = node.args.defaults + node.args.kw_defaults

    has_annotations = not isinstance(node, ast.Lambda)

    for arg_name in ('vararg', 'kwarg'):
        wildcard = getattr(node.args, arg_name)
        if not wildcard:
            continue
        args.append(wildcard.arg)
        if has_annotations:
            annotations.append(wildcard.annotation)

    if has_annotations:
        annotations.append(node.returns)

    if len(set(args)) &lt; len(args):
        for (idx, arg) in enumerate(args):
            if arg in args[:idx]:
                self.report(messages.DuplicateArgument, node, arg)

    for annotation in annotations:
        self.handleAnnotation(annotation, node)

    for default in defaults:
        self.handleNode(default, node)

    def runFunction():
        with self.in_scope(FunctionScope):
            self.handleChildren(
                node,
                omit=('decorator_list', 'returns', 'type_params'),
            )

    self.deferFunction(runFunction)
</t>
<t tx="ekr.20250430053636.151">def ARGUMENTS(self, node):
    self.handleChildren(node, omit=('defaults', 'kw_defaults'))
</t>
<t tx="ekr.20250430053636.152">def ARG(self, node):
    self.addBinding(node, Argument(node.arg, self.getScopeNode(node)))
</t>
<t tx="ekr.20250430053636.153">def CLASSDEF(self, node):
    """
    Check names used in a class definition, including its decorators, base
    classes, and the body of its definition.  Additionally, add its name to
    the current scope.
    """
    for deco in node.decorator_list:
        self.handleNode(deco, node)

    with self._type_param_scope(node):
        for baseNode in node.bases:
            self.handleNode(baseNode, node)
        for keywordNode in node.keywords:
            self.handleNode(keywordNode, node)
        with self.in_scope(ClassScope):
            # doctest does not process doctest within a doctest
            # classes within classes are processed.
            if (self.withDoctest and
                    not self._in_doctest() and
                    not isinstance(self.scope, FunctionScope)):
                self.deferFunction(lambda: self.handleDoctests(node))
            for stmt in node.body:
                self.handleNode(stmt, node)

    self.addBinding(node, ClassDefinition(node.name, node))
</t>
<t tx="ekr.20250430053636.154">def AUGASSIGN(self, node):
    self.handleNodeLoad(node.target, node)
    self.handleNode(node.value, node)
    self.handleNode(node.target, node)
</t>
<t tx="ekr.20250430053636.155">def TUPLE(self, node):
    if isinstance(node.ctx, ast.Store):
        # Python 3 advanced tuple unpacking: a, *b, c = d.
        # Only one starred expression is allowed, and no more than 1&lt;&lt;8
        # assignments are allowed before a stared expression. There is
        # also a limit of 1&lt;&lt;24 expressions after the starred expression,
        # which is impossible to test due to memory restrictions, but we
        # add it here anyway
        has_starred = False
        star_loc = -1
        for i, n in enumerate(node.elts):
            if isinstance(n, ast.Starred):
                if has_starred:
                    self.report(messages.TwoStarredExpressions, node)
                    # The SyntaxError doesn't distinguish two from more
                    # than two.
                    break
                has_starred = True
                star_loc = i
        if star_loc &gt;= 1 &lt;&lt; 8 or len(node.elts) - star_loc - 1 &gt;= 1 &lt;&lt; 24:
            self.report(messages.TooManyExpressionsInStarredAssignment, node)
    self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.156">LIST = TUPLE

def IMPORT(self, node):
    for alias in node.names:
        if '.' in alias.name and not alias.asname:
            importation = SubmoduleImportation(alias.name, node)
        else:
            name = alias.asname or alias.name
            importation = Importation(name, node, alias.name)
        self.addBinding(node, importation)
</t>
<t tx="ekr.20250430053636.157">def IMPORTFROM(self, node):
    if node.module == '__future__':
        if not self.futuresAllowed:
            self.report(messages.LateFutureImport, node)
    else:
        self.futuresAllowed = False

    module = ('.' * node.level) + (node.module or '')

    for alias in node.names:
        name = alias.asname or alias.name
        if node.module == '__future__':
            importation = FutureImportation(name, node, self.scope)
            if alias.name not in __future__.all_feature_names:
                self.report(messages.FutureFeatureNotDefined,
                            node, alias.name)
            if alias.name == 'annotations':
                self.annotationsFutureEnabled = True
        elif alias.name == '*':
            if not isinstance(self.scope, ModuleScope):
                self.report(messages.ImportStarNotPermitted,
                            node, module)
                continue

            self.scope.importStarred = True
            self.report(messages.ImportStarUsed, node, module)
            importation = StarImportation(module, node)
        else:
            importation = ImportationFrom(name, node,
                                          module, alias.name)
        self.addBinding(node, importation)
</t>
<t tx="ekr.20250430053636.158">def TRY(self, node):
    handler_names = []
    # List the exception handlers
    for i, handler in enumerate(node.handlers):
        if isinstance(handler.type, ast.Tuple):
            for exc_type in handler.type.elts:
                handler_names.append(getNodeName(exc_type))
        elif handler.type:
            handler_names.append(getNodeName(handler.type))

        if handler.type is None and i &lt; len(node.handlers) - 1:
            self.report(messages.DefaultExceptNotLast, handler)
    # Memorize the except handlers and process the body
    self.exceptHandlers.append(handler_names)
    for child in node.body:
        self.handleNode(child, node)
    self.exceptHandlers.pop()
    # Process the other nodes: "except:", "else:", "finally:"
    self.handleChildren(node, omit='body')
</t>
<t tx="ekr.20250430053636.159">TRYSTAR = TRY

def EXCEPTHANDLER(self, node):
    if node.name is None:
        self.handleChildren(node)
        return

    # If the name already exists in the scope, modify state of existing
    # binding.
    if node.name in self.scope:
        self.handleNodeStore(node)

    # 3.x: the name of the exception, which is not a Name node, but a
    # simple string, creates a local that is only bound within the scope of
    # the except: block. As such, temporarily remove the existing binding
    # to more accurately determine if the name is used in the except:
    # block.

    try:
        prev_definition = self.scope.pop(node.name)
    except KeyError:
        prev_definition = None

    self.handleNodeStore(node)
    self.handleChildren(node)

    # See discussion on https://github.com/PyCQA/pyflakes/pull/59

    # We're removing the local name since it's being unbound after leaving
    # the except: block and it's always unbound if the except: block is
    # never entered. This will cause an "undefined name" error raised if
    # the checked code tries to use the name afterwards.
    #
    # Unless it's been removed already. Then do nothing.

    try:
        binding = self.scope.pop(node.name)
    except KeyError:
        pass
    else:
        if not binding.used:
            self.report(messages.UnusedVariable, node, node.name)

    # Restore.
    if prev_definition:
        self.scope[node.name] = prev_definition
</t>
<t tx="ekr.20250430053636.16">FOR_TYPES = (ast.For, ast.AsyncFor)


def _is_singleton(node):  # type: (ast.AST) -&gt; bool
    return (
        isinstance(node, ast.Constant) and
        isinstance(node.value, (bool, type(Ellipsis), type(None)))
    )
</t>
<t tx="ekr.20250430053636.160">def ANNASSIGN(self, node):
    self.handleAnnotation(node.annotation, node)
    # If the assignment has value, handle the *value* now.
    if node.value:
        # If the annotation is `TypeAlias`, handle the *value* as an annotation.
        if _is_typing(node.annotation, 'TypeAlias', self.scopeStack):
            self.handleAnnotation(node.value, node)
        else:
            self.handleNode(node.value, node)
    self.handleNode(node.target, node)
</t>
<t tx="ekr.20250430053636.161">def COMPARE(self, node):
    left = node.left
    for op, right in zip(node.ops, node.comparators):
        if (
                isinstance(op, (ast.Is, ast.IsNot)) and (
                    _is_const_non_singleton(left) or
                    _is_const_non_singleton(right)
                )
        ):
            self.report(messages.IsLiteral, node)
        left = right

    self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.162">MATCH = MATCH_CASE = MATCHCLASS = MATCHOR = MATCHSEQUENCE = handleChildren
MATCHSINGLETON = MATCHVALUE = handleChildren

def _match_target(self, node):
    self.handleNodeStore(node)
    self.handleChildren(node)
</t>
<t tx="ekr.20250430053636.163">MATCHAS = MATCHMAPPING = MATCHSTAR = _match_target

@contextlib.contextmanager
def _type_param_scope(self, node):
    with contextlib.ExitStack() as ctx:
        if sys.version_info &gt;= (3, 12):
            ctx.enter_context(self.in_scope(TypeScope))
            for param in node.type_params:
                self.handleNode(param, node)
        yield
</t>
<t tx="ekr.20250430053636.164">def TYPEVAR(self, node):
    self.handleNodeStore(node)
    self.handle_annotation_always_deferred(node.bound, node)
</t>
<t tx="ekr.20250430053636.165">PARAMSPEC = TYPEVARTUPLE = handleNodeStore

def TYPEALIAS(self, node):
    self.handleNode(node.name, node)
    with self._type_param_scope(node):
        self.handle_annotation_always_deferred(node.value, node)
</t>
<t tx="ekr.20250430053636.166">"""
Provide the class Message and its subclasses.
"""


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053636.167">class Message:
    message = ''
    message_args = ()

    ### def __init__(self, filename, loc):
    def __init__(self, filename, loc, message=None):
        
        self.filename = filename
        self.lineno = loc.lineno
        self.col = loc.col_offset
        self.message = message

    def __str__(self):
        # return '{}:{}:{}: {}'.format(
        #  self.filename, self.lineno, self.col+1, self.message % self.message_args)
        if message:  # EKR.
            return f"{self.filename}: {self.lineno}: {self.col+1}: {self.message}"
        else:  # Legacy.
            return f"{self.filename}: {self.lineno}: {self.col+1}: {self.message % message_args}"
    </t>
<t tx="ekr.20250430053636.168">class UnusedImport(Message):

    ### message = 'redefinition of unused %r from line %r'

    def __init__(self, filename, loc, name, orig_loc):
        message = f"redefinition of unused {name} from line {orig_loc.lineno}"
        Message.__init__(self, filename, loc, message)
        ### self.message_args = (name, orig_loc.lineno)
</t>
<t tx="ekr.20250430053636.169">class RedefinedWhileUnused(Message):

    def __init__(self, filename, loc, name, orig_loc):
        message = f"redefinition of unused {name} from line {orig_loc.lineno}"
        Message.__init__(self, filename, loc, message)</t>
<t tx="ekr.20250430053636.17">def _is_tuple_constant(node):  # type: (ast.AST) -&gt; bool
    return (
        isinstance(node, ast.Tuple) and
        all(_is_constant(elt) for elt in node.elts)
    )
</t>
<t tx="ekr.20250430053636.170">class ImportShadowedByLoopVar(Message):
    @others
</t>
<t tx="ekr.20250430053636.171">class ImportStarNotPermitted(Message):
    @others
</t>
<t tx="ekr.20250430053636.172">class ImportStarUsed(Message):
    @others
</t>
<t tx="ekr.20250430053636.173">class ImportStarUsage(Message):
    @others
</t>
<t tx="ekr.20250430053636.174">class UndefinedName(Message):
    @others
</t>
<t tx="ekr.20250430053636.175">class DoctestSyntaxError(Message):
    @others
</t>
<t tx="ekr.20250430053636.176">class UndefinedExport(Message):
    @others
</t>
<t tx="ekr.20250430053636.177">class UndefinedLocal(Message):
    @others
</t>
<t tx="ekr.20250430053636.178">class DuplicateArgument(Message):
    @others
</t>
<t tx="ekr.20250430053636.179">class MultiValueRepeatedKeyLiteral(Message):
    @others
</t>
<t tx="ekr.20250430053636.18">def _is_constant(node):
    return isinstance(node, ast.Constant) or _is_tuple_constant(node)
</t>
<t tx="ekr.20250430053636.180">class MultiValueRepeatedKeyVariable(Message):
    @others
</t>
<t tx="ekr.20250430053636.181">class LateFutureImport(Message):
    message = 'from __future__ imports must occur at the beginning of the file'
</t>
<t tx="ekr.20250430053636.182">class FutureFeatureNotDefined(Message):
    """An undefined __future__ feature name was imported."""
    @others
</t>
<t tx="ekr.20250430053636.183">class UnusedVariable(Message):
    """
    Indicates that a variable has been explicitly assigned to but not actually
    used.
    """
    @others
</t>
<t tx="ekr.20250430053636.184">class UnusedAnnotation(Message):
    """
    Indicates that a variable has been explicitly annotated to but not actually
    used.
    """
    @others
</t>
<t tx="ekr.20250430053636.185">class ReturnOutsideFunction(Message):
    """
    Indicates a return statement outside of a function/method.
    """
    message = '\'return\' outside function'
</t>
<t tx="ekr.20250430053636.186">class YieldOutsideFunction(Message):
    """
    Indicates a yield or yield from statement outside of a function/method.
    """
    message = '\'yield\' outside function'
</t>
<t tx="ekr.20250430053636.187"># For whatever reason, Python gives different error messages for these two. We
# match the Python error message exactly.
class ContinueOutsideLoop(Message):
    """
    Indicates a continue statement outside of a while or for loop.
    """
    message = '\'continue\' not properly in loop'
</t>
<t tx="ekr.20250430053636.188">class BreakOutsideLoop(Message):
    """
    Indicates a break statement outside of a while or for loop.
    """
    message = '\'break\' outside loop'
</t>
<t tx="ekr.20250430053636.189">class DefaultExceptNotLast(Message):
    """
    Indicates an except: block as not the last exception handler.
    """
    message = 'default \'except:\' must be last'
</t>
<t tx="ekr.20250430053636.19">def _is_const_non_singleton(node):  # type: (ast.AST) -&gt; bool
    return _is_constant(node) and not _is_singleton(node)
</t>
<t tx="ekr.20250430053636.190">class TwoStarredExpressions(Message):
    """
    Two or more starred expressions in an assignment (a, *b, *c = d).
    """
    message = 'two starred expressions in assignment'
</t>
<t tx="ekr.20250430053636.191">class TooManyExpressionsInStarredAssignment(Message):
    """
    Too many expressions in an assignment with star-unpacking
    """
    message = 'too many expressions in star-unpacking assignment'
</t>
<t tx="ekr.20250430053636.192">class IfTuple(Message):
    """
    Conditional test is a non-empty tuple literal, which are always True.
    """
    message = '\'if tuple literal\' is always true, perhaps remove accidental comma?'
</t>
<t tx="ekr.20250430053636.193">class AssertTuple(Message):
    """
    Assertion test is a non-empty tuple literal, which are always True.
    """
    message = 'assertion is always true, perhaps remove parentheses?'
</t>
<t tx="ekr.20250430053636.194">class ForwardAnnotationSyntaxError(Message):
    @others
</t>
<t tx="ekr.20250430053636.195">class RaiseNotImplemented(Message):
    message = "'raise NotImplemented' should be 'raise NotImplementedError'"
</t>
<t tx="ekr.20250430053636.196">class InvalidPrintSyntax(Message):
    message = 'use of &gt;&gt; is invalid with print function'
</t>
<t tx="ekr.20250430053636.197">class IsLiteral(Message):
    message = 'use ==/!= to compare constant literals (str, bytes, int, float, tuple)'
</t>
<t tx="ekr.20250430053636.198">class FStringMissingPlaceholders(Message):
    message = 'f-string is missing placeholders'
</t>
<t tx="ekr.20250430053636.199">class StringDotFormatExtraPositionalArguments(Message):
    @others
</t>
<t tx="ekr.20250430053636.2"># Python3.13/Lib/site-packages/pyflakes
</t>
<t tx="ekr.20250430053636.20">def _is_name_or_attr(node, name):  # type: (ast.AST, str) -&gt; bool
    return (
        (isinstance(node, ast.Name) and node.id == name) or
        (isinstance(node, ast.Attribute) and node.attr == name)
    )
</t>
<t tx="ekr.20250430053636.200">class StringDotFormatExtraNamedArguments(Message):
    @others
</t>
<t tx="ekr.20250430053636.201">class StringDotFormatMissingArgument(Message):
    @others
</t>
<t tx="ekr.20250430053636.202">class StringDotFormatMixingAutomatic(Message):
    message = "'...'.format(...) mixes automatic and manual numbering"
</t>
<t tx="ekr.20250430053636.203">class StringDotFormatInvalidFormat(Message):
    @others
</t>
<t tx="ekr.20250430053636.204">class PercentFormatInvalidFormat(Message):
    @others
</t>
<t tx="ekr.20250430053636.205">class PercentFormatMixedPositionalAndNamed(Message):
    message = "'...' %% ... has mixed positional and named placeholders"
</t>
<t tx="ekr.20250430053636.206">class PercentFormatUnsupportedFormatCharacter(Message):
    @others
</t>
<t tx="ekr.20250430053636.207">class PercentFormatPositionalCountMismatch(Message):
    @others
</t>
<t tx="ekr.20250430053636.208">class PercentFormatExtraNamedArguments(Message):
    @others
</t>
<t tx="ekr.20250430053636.209">class PercentFormatMissingArgument(Message):
    @others
</t>
<t tx="ekr.20250430053636.21">MAPPING_KEY_RE = re.compile(r'\(([^()]*)\)')
CONVERSION_FLAG_RE = re.compile('[#0+ -]*')
WIDTH_RE = re.compile(r'(?:\*|\d*)')
PRECISION_RE = re.compile(r'(?:\.(?:\*|\d*))?')
LENGTH_RE = re.compile('[hlL]?')
# https://docs.python.org/3/library/stdtypes.html#old-string-formatting
VALID_CONVERSIONS = frozenset('diouxXeEfFgGcrsa%')


def _must_match(regex, string, pos):
    match = regex.match(string, pos)
    assert match is not None
    return match
</t>
<t tx="ekr.20250430053636.210">class PercentFormatExpectedMapping(Message):
    message = "'...' %% ... expected mapping but got sequence"
</t>
<t tx="ekr.20250430053636.211">class PercentFormatExpectedSequence(Message):
    message = "'...' %% ... expected sequence but got mapping"
</t>
<t tx="ekr.20250430053636.212">class PercentFormatStarRequiresSequence(Message):
    message = "'...' %% ... `*` specifier requires sequence"
</t>
<t tx="ekr.20250430053636.217">message = 'import %r from line %r shadowed by loop variable'

def __init__(self, filename, loc, name, orig_loc):
    Message.__init__(self, filename, loc)
    self.message_args = (name, orig_loc.lineno)
</t>
<t tx="ekr.20250430053636.218">message = "'from %s import *' only allowed at module level"

def __init__(self, filename, loc, modname):
    Message.__init__(self, filename, loc)
    self.message_args = (modname,)
</t>
<t tx="ekr.20250430053636.219">message = "'from %s import *' used; unable to detect undefined names"

def __init__(self, filename, loc, modname):
    Message.__init__(self, filename, loc)
    self.message_args = (modname,)
</t>
<t tx="ekr.20250430053636.22">def parse_percent_format(s):
    """Parses the string component of a `'...' % ...` format call

    Copied from https://github.com/asottile/pyupgrade at v1.20.1
    """

    def _parse_inner():
        string_start = 0
        string_end = 0
        in_fmt = False

        i = 0
        while i &lt; len(s):
            if not in_fmt:
                try:
                    i = s.index('%', i)
                except ValueError:  # no more % fields!
                    yield s[string_start:], None
                    return
                else:
                    string_end = i
                    i += 1
                    in_fmt = True
            else:
                key_match = MAPPING_KEY_RE.match(s, i)
                if key_match:
                    key = key_match.group(1)
                    i = key_match.end()
                else:
                    key = None

                conversion_flag_match = _must_match(CONVERSION_FLAG_RE, s, i)
                conversion_flag = conversion_flag_match.group() or None
                i = conversion_flag_match.end()

                width_match = _must_match(WIDTH_RE, s, i)
                width = width_match.group() or None
                i = width_match.end()

                precision_match = _must_match(PRECISION_RE, s, i)
                precision = precision_match.group() or None
                i = precision_match.end()

                # length modifier is ignored
                i = _must_match(LENGTH_RE, s, i).end()

                try:
                    conversion = s[i]
                except IndexError:
                    raise ValueError('end-of-string while parsing format')
                i += 1

                fmt = (key, conversion_flag, width, precision, conversion)
                yield s[string_start:string_end], fmt

                in_fmt = False
                string_start = i

        if in_fmt:
            raise ValueError('end-of-string while parsing format')

    return tuple(_parse_inner())
</t>
<t tx="ekr.20250430053636.220">message = "%r may be undefined, or defined from star imports: %s"

def __init__(self, filename, loc, name, from_list):
    Message.__init__(self, filename, loc)
    self.message_args = (name, from_list)
</t>
<t tx="ekr.20250430053636.221">message = 'undefined name %r'

def __init__(self, filename, loc, name):
    Message.__init__(self, filename, loc)
    self.message_args = (name,)
</t>
<t tx="ekr.20250430053636.222">message = 'syntax error in doctest'

def __init__(self, filename, loc, position=None):
    Message.__init__(self, filename, loc)
    if position:
        (self.lineno, self.col) = position
    self.message_args = ()
</t>
<t tx="ekr.20250430053636.223">message = 'undefined name %r in __all__'

def __init__(self, filename, loc, name):
    Message.__init__(self, filename, loc)
    self.message_args = (name,)
</t>
<t tx="ekr.20250430053636.224">message = 'local variable %r {0} referenced before assignment'

default = 'defined in enclosing scope on line %r'
builtin = 'defined as a builtin'

def __init__(self, filename, loc, name, orig_loc):
    Message.__init__(self, filename, loc)
    if orig_loc is None:
        self.message = self.message.format(self.builtin)
        self.message_args = name
    else:
        self.message = self.message.format(self.default)
        self.message_args = (name, orig_loc.lineno)
</t>
<t tx="ekr.20250430053636.225">message = 'duplicate argument %r in function definition'

def __init__(self, filename, loc, name):
    Message.__init__(self, filename, loc)
    self.message_args = (name,)
</t>
<t tx="ekr.20250430053636.226">message = 'dictionary key %r repeated with different values'

def __init__(self, filename, loc, key):
    Message.__init__(self, filename, loc)
    self.message_args = (key,)
</t>
<t tx="ekr.20250430053636.227">message = 'dictionary key variable %s repeated with different values'

def __init__(self, filename, loc, key):
    Message.__init__(self, filename, loc)
    self.message_args = (key,)
</t>
<t tx="ekr.20250430053636.228">message = 'future feature %s is not defined'

def __init__(self, filename, loc, name):
    Message.__init__(self, filename, loc)
    self.message_args = (name,)
</t>
<t tx="ekr.20250430053636.229">message = 'local variable %r is assigned to but never used'

def __init__(self, filename, loc, names):
    Message.__init__(self, filename, loc)
    self.message_args = (names,)
</t>
<t tx="ekr.20250430053636.23">class _FieldsOrder(dict):
    """Fix order of AST node fields."""
    @others
</t>
<t tx="ekr.20250430053636.230">message = 'local variable %r is annotated but never used'

def __init__(self, filename, loc, names):
    Message.__init__(self, filename, loc)
    self.message_args = (names,)
</t>
<t tx="ekr.20250430053636.231">message = 'syntax error in forward annotation %r'

def __init__(self, filename, loc, annotation):
    Message.__init__(self, filename, loc)
    self.message_args = (annotation,)
</t>
<t tx="ekr.20250430053636.232">message = "'...'.format(...) has unused arguments at position(s): %s"

def __init__(self, filename, loc, extra_positions):
    Message.__init__(self, filename, loc)
    self.message_args = (extra_positions,)
</t>
<t tx="ekr.20250430053636.233">message = "'...'.format(...) has unused named argument(s): %s"

def __init__(self, filename, loc, extra_keywords):
    Message.__init__(self, filename, loc)
    self.message_args = (extra_keywords,)
</t>
<t tx="ekr.20250430053636.234">message = "'...'.format(...) is missing argument(s) for placeholder(s): %s"

def __init__(self, filename, loc, missing_arguments):
    Message.__init__(self, filename, loc)
    self.message_args = (missing_arguments,)
</t>
<t tx="ekr.20250430053636.235">message = "'...'.format(...) has invalid format string: %s"

def __init__(self, filename, loc, error):
    Message.__init__(self, filename, loc)
    self.message_args = (error,)
</t>
<t tx="ekr.20250430053636.236">message = "'...' %% ... has invalid format string: %s"

def __init__(self, filename, loc, error):
    Message.__init__(self, filename, loc)
    self.message_args = (error,)
</t>
<t tx="ekr.20250430053636.237">message = "'...' %% ... has unsupported format character %r"

def __init__(self, filename, loc, c):
    Message.__init__(self, filename, loc)
    self.message_args = (c,)
</t>
<t tx="ekr.20250430053636.238">message = "'...' %% ... has %d placeholder(s) but %d substitution(s)"

def __init__(self, filename, loc, n_placeholders, n_substitutions):
    Message.__init__(self, filename, loc)
    self.message_args = (n_placeholders, n_substitutions)
</t>
<t tx="ekr.20250430053636.239">message = "'...' %% ... has unused named argument(s): %s"

def __init__(self, filename, loc, extra_keywords):
    Message.__init__(self, filename, loc)
    self.message_args = (extra_keywords,)
</t>
<t tx="ekr.20250430053636.24">def counter(items):
    """
    Simplest required implementation of collections.Counter. Required as 2.6
    does not have Counter in collections.
    """
    results = {}
    for item in items:
        results[item] = results.get(item, 0) + 1
    return results
</t>
<t tx="ekr.20250430053636.240">message = "'...' %% ... is missing argument(s) for placeholder(s): %s"

def __init__(self, filename, loc, missing_arguments):
    Message.__init__(self, filename, loc)
    self.message_args = (missing_arguments,)
</t>
<t tx="ekr.20250430053636.241">"""
Provide the Reporter class.
"""

import re
import sys


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053636.242">class Reporter:
    """
    Formats the results of pyflakes checks to users.
    """
    @others
</t>
<t tx="ekr.20250430053636.243">def _makeDefaultReporter():
    """
    Make a reporter that can be used when no reporter is specified.
    """
    return Reporter(sys.stdout, sys.stderr)
</t>
<t tx="ekr.20250430053636.244">def __init__(self, warningStream, errorStream):
    """
    Construct a L{Reporter}.

    @param warningStream: A file-like object where warnings will be
        written to.  The stream's C{write} method must accept unicode.
        C{sys.stdout} is a good value.
    @param errorStream: A file-like object where error output will be
        written to.  The stream's C{write} method must accept unicode.
        C{sys.stderr} is a good value.
    """
    self._stdout = warningStream
    self._stderr = errorStream
</t>
<t tx="ekr.20250430053636.245">def unexpectedError(self, filename, msg):
    """
    An unexpected error occurred trying to process C{filename}.

    @param filename: The path to a file that we could not process.
    @ptype filename: C{unicode}
    @param msg: A message explaining the problem.
    @ptype msg: C{unicode}
    """
    self._stderr.write(f"{filename}: {msg}\n")
</t>
<t tx="ekr.20250430053636.246">def syntaxError(self, filename, msg, lineno, offset, text):
    """
    There was a syntax error in C{filename}.

    @param filename: The path to the file with the syntax error.
    @ptype filename: C{unicode}
    @param msg: An explanation of the syntax error.
    @ptype msg: C{unicode}
    @param lineno: The line number where the syntax error occurred.
    @ptype lineno: C{int}
    @param offset: The column on which the syntax error occurred, or None.
    @ptype offset: C{int}
    @param text: The source code containing the syntax error.
    @ptype text: C{unicode}
    """
    if text is None:
        line = None
    else:
        line = text.splitlines()[-1]

    # lineno might be None if the error was during tokenization
    # lineno might be 0 if the error came from stdin
    lineno = max(lineno or 0, 1)

    if offset is not None:
        # some versions of python emit an offset of -1 for certain encoding errors
        offset = max(offset, 1)
        self._stderr.write('%s:%d:%d: %s\n' %
                           (filename, lineno, offset, msg))
    else:
        self._stderr.write('%s:%d: %s\n' % (filename, lineno, msg))

    if line is not None:
        self._stderr.write(line)
        self._stderr.write('\n')
        if offset is not None:
            self._stderr.write(re.sub(r'\S', ' ', line[:offset - 1]) +
                               "^\n")
</t>
<t tx="ekr.20250430053636.247">def flake(self, message):
    """
    pyflakes found something wrong with the code.

    @param: A L{pyflakes.messages.Message}.
    """
    self._stdout.write(str(message))
    self._stdout.write('\n')
</t>
<t tx="ekr.20250430053636.25">def iter_child_nodes(node, omit=None, _fields_order=_FieldsOrder()):
    """
    Yield all direct child nodes of *node*, that is, all fields that
    are nodes and all items of fields that are lists of nodes.

    :param node:          AST node to be iterated upon
    :param omit:          String or tuple of strings denoting the
                          attributes of the node to be omitted from
                          further parsing
    :param _fields_order: Order of AST node fields
    """
    for name in _fields_order[node.__class__]:
        if omit and name in omit:
            continue
        field = getattr(node, name, None)
        if isinstance(field, ast.AST):
            yield field
        elif isinstance(field, list):
            for item in field:
                if isinstance(item, ast.AST):
                    yield item
</t>
<t tx="ekr.20250430053636.250">"""
Implementation of the command-line I{pyflakes} tool.
"""

# For backward compatibility
__all__ = ['check', 'checkPath', 'checkRecursive', 'iterSourceCode', 'main']
from pyflakes.api import check, checkPath, checkRecursive, iterSourceCode, main
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053636.251"></t>
<t tx="ekr.20250430053636.252">@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053636.253">import ast
import textwrap
import unittest

from pyflakes import checker

__all__ = ['TestCase', 'skip', 'skipIf']

skip = unittest.skip
skipIf = unittest.skipIf


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053636.254">class TestCase(unittest.TestCase):
    withDoctest = False

    def flakes(self, input, *expectedOutputs, **kw):
        tree = ast.parse(textwrap.dedent(input))
        if kw.get('is_segment'):
            tree = tree.body[0]
            kw.pop('is_segment')
        w = checker.Checker(tree, withDoctest=self.withDoctest, **kw)
        outputs = [type(o) for o in w.messages]
        expectedOutputs = list(expectedOutputs)
        outputs.sort(key=lambda t: t.__name__)
        expectedOutputs.sort(key=lambda t: t.__name__)
        self.assertEqual(outputs, expectedOutputs, '''\
for input:
{}
expected outputs:
{!r}
but got:
{}'''.format(input, expectedOutputs, '\n'.join([str(o) for o in w.messages])))
        return w

</t>
<t tx="ekr.20250430053636.26">def convert_to_value(item):
    if isinstance(item, ast.Constant):
        return item.value
    elif isinstance(item, ast.Tuple):
        return tuple(convert_to_value(i) for i in item.elts)
    elif isinstance(item, ast.Name):
        return VariableKey(item=item)
    else:
        return UnhandledKeyType()
</t>
<t tx="ekr.20250430053636.27">def is_notimplemented_name_node(node):
    return isinstance(node, ast.Name) and getNodeName(node) == 'NotImplemented'
</t>
<t tx="ekr.20250430053636.28">class Binding:
    """
    Represents the binding of a value to a name.
    
    The checker uses this to keep track of which names have been bound and
    which names have not. See L{Assignment} for a special type of binding that
    is checked with stricter rules.
    
    @ivar used: pair of (L{Scope}, node) indicating the scope and
                the node that this binding was last used.
    """
    @others
</t>
<t tx="ekr.20250430053636.29">class Definition(Binding):
    """
    A binding that defines a function or a class.
    """
    @others
</t>
<t tx="ekr.20250430053636.30">class Builtin(Definition):
    """A definition created for all Python builtins."""
    @others
</t>
<t tx="ekr.20250430053636.31">class UnhandledKeyType:
    """
    A dictionary key of a type that we cannot or do not check for duplicates.
    """
</t>
<t tx="ekr.20250430053636.32">class VariableKey:
    """
    A dictionary key which is a variable.
    
    @ivar item: The variable AST object.
    """
    @others
</t>
<t tx="ekr.20250430053636.33">class Importation(Definition):
    """
    A binding created by an import statement.
    
    @ivar fullName: The complete name given to the import statement,
        possibly including multiple dotted components.
    @type fullName: C{str}
    """
    @others
</t>
<t tx="ekr.20250430053636.34">class SubmoduleImportation(Importation):
    """
    A binding created by a submodule import statement.
    
    A submodule import is a special case where the root module is implicitly
    imported, without an 'as' clause, and the submodule is also imported.
    Python does not restrict which attributes of the root module may be used.
    
    This class is only used when the submodule import is without an 'as' clause.
    
    pyflakes handles this case by registering the root module name in the scope,
    allowing any attribute of the root module to be accessed.
    
    RedefinedWhileUnused is suppressed in `redefines` unless the submodule
    name is also the same, to avoid false positives.
    """
    @others
</t>
<t tx="ekr.20250430053636.35">class ImportationFrom(Importation):
    @others
</t>
<t tx="ekr.20250430053636.36">class StarImportation(Importation):
    """A binding created by a 'from x import *' statement."""
    @others
</t>
<t tx="ekr.20250430053636.37">class FutureImportation(ImportationFrom):
    """
    A binding created by a from `__future__` import statement.
    
    `__future__` imports are implicitly used.
    """
    @others
</t>
<t tx="ekr.20250430053636.38">class Argument(Binding):
    """
    Represents binding a name as an argument.
    """
</t>
<t tx="ekr.20250430053636.39">class Assignment(Binding):
    """
    Represents binding a name with an explicit assignment.

    The checker will raise warnings for any Assignment that isn't used. Also,
    the checker does not consider assignments in tuple/list unpacking to be
    Assignments, rather it treats them as simple Bindings.
    """
</t>
<t tx="ekr.20250430053636.40">class NamedExprAssignment(Assignment):
    """
    Represents binding a name with an assignment expression.
    """
</t>
<t tx="ekr.20250430053636.41">class Annotation(Binding):
    """
    Represents binding a name to a type without an associated value.
    
    As long as this name is not assigned a value in another binding, it is considered
    undefined for most purposes. One notable exception is using the name as a type
    annotation.
    """
    @others
</t>
<t tx="ekr.20250430053636.42">class FunctionDefinition(Definition):
    pass
</t>
<t tx="ekr.20250430053636.43">class ClassDefinition(Definition):
    pass
</t>
<t tx="ekr.20250430053636.44">class ExportBinding(Binding):
    """
    A binding created by an C{__all__} assignment.  If the names in the list
    can be determined statically, they will be treated as names for export and
    additional checking applied to them.
    
    The only recognized C{__all__} assignment via list/tuple concatenation is in the
    following format:
    
        __all__ = ['a'] + ['b'] + ['c']
    
    Names which are imported and not otherwise used but appear in the value of
    C{__all__} will not have an unused import warning reported for them.
    """
    @others
</t>
<t tx="ekr.20250430053636.45">class Scope(dict):
    @others
</t>
<t tx="ekr.20250430053636.46">class ClassScope(Scope):
    pass
</t>
<t tx="ekr.20250430053636.47">class FunctionScope(Scope):
    """
    I represent a name scope for a function.
    
    @ivar globals: Names declared 'global' in this function.
    """
    @others
</t>
<t tx="ekr.20250430053636.48">class TypeScope(Scope):
    pass
</t>
<t tx="ekr.20250430053636.49">class GeneratorScope(Scope):
    pass
</t>
<t tx="ekr.20250430053636.5">"""
API for the command-line I{pyflakes} tool.
"""
import ast
import os
import platform
import re
import sys

from pyflakes import checker, __version__
from pyflakes import reporter as modReporter

__all__ = ['check', 'checkPath', 'checkRecursive', 'iterSourceCode', 'main']

PYTHON_SHEBANG_REGEX = re.compile(br'^#!.*\bpython(3(\.\d+)?|w)?[dmu]?\s')


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053636.50">class ModuleScope(Scope):
    """Scope for a module."""
    _futures_allowed = True
    _annotations_future_enabled = False
</t>
<t tx="ekr.20250430053636.51">class DoctestScope(ModuleScope):
    """Scope for a doctest."""
</t>
<t tx="ekr.20250430053636.52">class DetectClassScopedMagic:
    names = dir()
</t>
<t tx="ekr.20250430053636.53"># Globally defined names which are not attributes of the builtins module, or
# are only present on some platforms.
_MAGIC_GLOBALS = ['__file__', '__builtins__', '__annotations__', 'WindowsError']


def getNodeName(node):
    # Returns node.id, or node.name, or None
    if hasattr(node, 'id'):     # One of the many nodes with an id
        return node.id
    if hasattr(node, 'name'):   # an ExceptHandler node
        return node.name
    if hasattr(node, 'rest'):   # a MatchMapping node
        return node.rest
</t>
<t tx="ekr.20250430053636.54">TYPING_MODULES = frozenset(('typing', 'typing_extensions'))


def _is_typing_helper(node, is_name_match_fn, scope_stack):
    """
    Internal helper to determine whether or not something is a member of a
    typing module. This is used as part of working out whether we are within a
    type annotation context.

    Note: you probably don't want to use this function directly. Instead see the
    utils below which wrap it (`_is_typing` and `_is_any_typing_member`).
    """

    def _bare_name_is_attr(name):
        for scope in reversed(scope_stack):
            if name in scope:
                return (
                    isinstance(scope[name], ImportationFrom) and
                    scope[name].module in TYPING_MODULES and
                    is_name_match_fn(scope[name].real_name)
                )

        return False

    def _module_scope_is_typing(name):
        for scope in reversed(scope_stack):
            if name in scope:
                return (
                    isinstance(scope[name], Importation) and
                    scope[name].fullName in TYPING_MODULES
                )

        return False

    return (
        (
            isinstance(node, ast.Name) and
            _bare_name_is_attr(node.id)
        ) or (
            isinstance(node, ast.Attribute) and
            isinstance(node.value, ast.Name) and
            _module_scope_is_typing(node.value.id) and
            is_name_match_fn(node.attr)
        )
    )
</t>
<t tx="ekr.20250430053636.55">def _is_typing(node, typing_attr, scope_stack):
    """
    Determine whether `node` represents the member of a typing module specified
    by `typing_attr`.

    This is used as part of working out whether we are within a type annotation
    context.
    """
    return _is_typing_helper(node, lambda x: x == typing_attr, scope_stack)
</t>
<t tx="ekr.20250430053636.56">def _is_any_typing_member(node, scope_stack):
    """
    Determine whether `node` represents any member of a typing module.

    This is used as part of working out whether we are within a type annotation
    context.
    """
    return _is_typing_helper(node, lambda x: True, scope_stack)
</t>
<t tx="ekr.20250430053636.57">def is_typing_overload(value, scope_stack):
    return (
        isinstance(value.source, (ast.FunctionDef, ast.AsyncFunctionDef)) and
        any(
            _is_typing(dec, 'overload', scope_stack)
            for dec in value.source.decorator_list
        )
    )
</t>
<t tx="ekr.20250430053636.58">class AnnotationState:
    NONE = 0
    STRING = 1
    BARE = 2
</t>
<t tx="ekr.20250430053636.59">def in_annotation(func):
    @functools.wraps(func)
    def in_annotation_func(self, *args, **kwargs):
        with self._enter_annotation():
            return func(self, *args, **kwargs)
    return in_annotation_func
</t>
<t tx="ekr.20250430053636.6">def check(codeString, filename, reporter=None):
    """
    Check the Python source given by C{codeString} for flakes.

    @param codeString: The Python source to check.
    @type codeString: C{str}

    @param filename: The name of the file the source came from, used to report
        errors.
    @type filename: C{str}

    @param reporter: A L{Reporter} instance, where errors and warnings will be
        reported.

    @return: The number of warnings emitted.
    @rtype: C{int}
    """
    if reporter is None:
        reporter = modReporter._makeDefaultReporter()
    # First, compile into an AST and handle syntax errors.
    try:
        tree = ast.parse(codeString, filename=filename)
    except SyntaxError as e:
        reporter.syntaxError(filename, e.args[0], e.lineno, e.offset, e.text)
        return 1
    except Exception:
        reporter.unexpectedError(filename, 'problem decoding source')
        return 1
    # Okay, it's syntactically valid.  Now check it.
    w = checker.Checker(tree, filename=filename)
    w.messages.sort(key=lambda m: m.lineno)
    for warning in w.messages:
        reporter.flake(warning)
    return len(w.messages)
</t>
<t tx="ekr.20250430053636.60">def in_string_annotation(func):
    @functools.wraps(func)
    def in_annotation_func(self, *args, **kwargs):
        with self._enter_annotation(AnnotationState.STRING):
            return func(self, *args, **kwargs)
    return in_annotation_func
</t>
<t tx="ekr.20250430053636.61">class Checker:
    """I check the cleanliness and sanity of Python code."""
    
    _ast_node_scope = {
        ast.Module: ModuleScope,
        ast.ClassDef: ClassScope,
        ast.FunctionDef: FunctionScope,
        ast.AsyncFunctionDef: FunctionScope,
        ast.Lambda: FunctionScope,
        ast.ListComp: GeneratorScope,
        ast.SetComp: GeneratorScope,
        ast.GeneratorExp: GeneratorScope,
        ast.DictComp: GeneratorScope,
    }

    nodeDepth = 0
    offset = None
    _in_annotation = AnnotationState.NONE

    builtIns = set(builtin_vars).union(_MAGIC_GLOBALS)
    _customBuiltIns = os.environ.get('PYFLAKES_BUILTINS')
    if _customBuiltIns:
        builtIns.update(_customBuiltIns.split(','))
    del _customBuiltIns

    @others
</t>
<t tx="ekr.20250430053636.62">def _get_fields(self, node_class):
    # handle iter before target, and generators before element
    fields = node_class._fields
    if 'iter' in fields:
        key_first = 'iter'.find
    elif 'generators' in fields:
        key_first = 'generators'.find
    else:
        key_first = 'value'.find
    return tuple(sorted(fields, key=key_first, reverse=True))
</t>
<t tx="ekr.20250430053636.63">def __missing__(self, node_class):
    self[node_class] = fields = self._get_fields(node_class)
    return fields
</t>
<t tx="ekr.20250430053636.64">def __init__(self, name, source):
    self.name = name
    self.source = source
    self.used = False
</t>
<t tx="ekr.20250430053636.65">def __str__(self):
    return self.name
</t>
<t tx="ekr.20250430053636.66">def __repr__(self):
    return '&lt;{} object {!r} from line {!r} at 0x{:x}&gt;'.format(
        self.__class__.__name__,
        self.name,
        self.source.lineno,
        id(self),
    )
</t>
<t tx="ekr.20250430053636.67">def redefines(self, other):
    return isinstance(other, Definition) and self.name == other.name
</t>
<t tx="ekr.20250430053636.68">def redefines(self, other):
    return (
        super().redefines(other) or
        (isinstance(other, Assignment) and self.name == other.name)
    )
</t>
<t tx="ekr.20250430053636.69">def __init__(self, name):
    super().__init__(name, None)
</t>
<t tx="ekr.20250430053636.7">def checkPath(filename, reporter=None):
    """
    Check the given path, printing out any warnings detected.

    @param reporter: A L{Reporter} instance, where errors and warnings will be
        reported.

    @return: the number of warnings printed
    """
    if reporter is None:
        reporter = modReporter._makeDefaultReporter()
    try:
        with open(filename, 'rb') as f:
            codestr = f.read()
    except OSError as e:
        reporter.unexpectedError(filename, e.args[1])
        return 1
    return check(codestr, filename, reporter)
</t>
<t tx="ekr.20250430053636.70">def __repr__(self):
    return '&lt;{} object {!r} at 0x{:x}&gt;'.format(
        self.__class__.__name__,
        self.name,
        id(self)
    )
</t>
<t tx="ekr.20250430053636.71">def __init__(self, item):
    self.name = item.id
</t>
<t tx="ekr.20250430053636.72">def __eq__(self, compare):
    return (
        compare.__class__ == self.__class__ and
        compare.name == self.name
    )
</t>
<t tx="ekr.20250430053636.73">def __hash__(self):
    return hash(self.name)
</t>
<t tx="ekr.20250430053636.74">def __init__(self, name, source, full_name=None):
    self.fullName = full_name or name
    self.redefined = []
    super().__init__(name, source)
</t>
<t tx="ekr.20250430053636.75">def redefines(self, other):
    if isinstance(other, SubmoduleImportation):
        # See note in SubmoduleImportation about RedefinedWhileUnused
        return self.fullName == other.fullName
    return isinstance(other, Definition) and self.name == other.name
</t>
<t tx="ekr.20250430053636.76">def _has_alias(self):
    """Return whether importation needs an as clause."""
    return not self.fullName.split('.')[-1] == self.name
</t>
<t tx="ekr.20250430053636.77">@property
def source_statement(self):
    """Generate a source statement equivalent to the import."""
    if self._has_alias():
        return f'import {self.fullName} as {self.name}'
    else:
        return 'import %s' % self.fullName
</t>
<t tx="ekr.20250430053636.78">def __str__(self):
    """Return import full name with alias."""
    if self._has_alias():
        return self.fullName + ' as ' + self.name
    else:
        return self.fullName
</t>
<t tx="ekr.20250430053636.79">def __init__(self, name, source):
    # A dot should only appear in the name when it is a submodule import
    assert '.' in name and (not source or isinstance(source, ast.Import))
    package_name = name.split('.')[0]
    super().__init__(package_name, source)
    self.fullName = name
</t>
<t tx="ekr.20250430053636.8">def isPythonFile(filename):
    """Return True if filename points to a Python file."""
    if filename.endswith('.py'):
        return True

    # Avoid obvious Emacs backup files
    if filename.endswith("~"):
        return False

    max_bytes = 128

    try:
        with open(filename, 'rb') as f:
            text = f.read(max_bytes)
            if not text:
                return False
    except OSError:
        return False

    return PYTHON_SHEBANG_REGEX.match(text)
</t>
<t tx="ekr.20250430053636.80">def redefines(self, other):
    if isinstance(other, Importation):
        return self.fullName == other.fullName
    return super().redefines(other)
</t>
<t tx="ekr.20250430053636.81">def __str__(self):
    return self.fullName
</t>
<t tx="ekr.20250430053636.82">@property
def source_statement(self):
    return 'import ' + self.fullName
</t>
<t tx="ekr.20250430053636.83">def __init__(self, name, source, module, real_name=None):
    self.module = module
    self.real_name = real_name or name

    if module.endswith('.'):
        full_name = module + self.real_name
    else:
        full_name = module + '.' + self.real_name

    super().__init__(name, source, full_name)
</t>
<t tx="ekr.20250430053636.84">def __str__(self):
    """Return import full name with alias."""
    if self.real_name != self.name:
        return self.fullName + ' as ' + self.name
    else:
        return self.fullName
</t>
<t tx="ekr.20250430053636.85">@property
def source_statement(self):
    if self.real_name != self.name:
        return f'from {self.module} import {self.real_name} as {self.name}'
    else:
        return f'from {self.module} import {self.name}'
</t>
<t tx="ekr.20250430053636.86">def __init__(self, name, source):
    super().__init__('*', source)
    # Each star importation needs a unique name, and
    # may not be the module name otherwise it will be deemed imported
    self.name = name + '.*'
    self.fullName = name
</t>
<t tx="ekr.20250430053636.87">@property
def source_statement(self):
    return 'from ' + self.fullName + ' import *'
</t>
<t tx="ekr.20250430053636.88">def __str__(self):
    # When the module ends with a ., avoid the ambiguous '..*'
    if self.fullName.endswith('.'):
        return self.source_statement
    else:
        return self.name
</t>
<t tx="ekr.20250430053636.89">def __init__(self, name, source, scope):
    super().__init__(name, source, '__future__')
    self.used = (scope, source)
</t>
<t tx="ekr.20250430053636.9">def iterSourceCode(paths):
    """
    Iterate over all Python source files in C{paths}.

    @param paths: A list of paths.  Directories will be recursed into and
        any .py files found will be yielded.  Any non-directories will be
        yielded as-is.
    """
    for path in paths:
        if os.path.isdir(path):
            for dirpath, dirnames, filenames in os.walk(path):
                for filename in filenames:
                    full_path = os.path.join(dirpath, filename)
                    if isPythonFile(full_path):
                        yield full_path
        else:
            yield path
</t>
<t tx="ekr.20250430053636.90">def redefines(self, other):
    """An Annotation doesn't define any name, so it cannot redefine one."""
    return False
</t>
<t tx="ekr.20250430053636.91">def __init__(self, name, source, scope):
    if '__all__' in scope and isinstance(source, ast.AugAssign):
        self.names = list(scope['__all__'].names)
    else:
        self.names = []

    def _add_to_names(container):
        for node in container.elts:
            if isinstance(node, ast.Constant) and isinstance(node.value, str):
                self.names.append(node.value)

    if isinstance(source.value, (ast.List, ast.Tuple)):
        _add_to_names(source.value)
    # If concatenating lists or tuples
    elif isinstance(source.value, ast.BinOp):
        currentValue = source.value
        while isinstance(currentValue.right, (ast.List, ast.Tuple)):
            left = currentValue.left
            right = currentValue.right
            _add_to_names(right)
            # If more lists are being added
            if isinstance(left, ast.BinOp):
                currentValue = left
            # If just two lists are being added
            elif isinstance(left, (ast.List, ast.Tuple)):
                _add_to_names(left)
                # All lists accounted for - done
                break
            # If not list concatenation
            else:
                break
    super().__init__(name, source)
</t>
<t tx="ekr.20250430053636.92">importStarred = False       # set to True when import * is found

def __repr__(self):
    scope_cls = self.__class__.__name__
    return f'&lt;{scope_cls} at 0x{id(self):x} {dict.__repr__(self)}&gt;'
</t>
<t tx="ekr.20250430053636.93">usesLocals = False
alwaysUsed = {'__tracebackhide__', '__traceback_info__',
              '__traceback_supplement__'}

def __init__(self):
    super().__init__()
    # Simplify: manage the special locals as globals
    self.globals = self.alwaysUsed.copy()
    self.returnValue = None     # First non-empty return
</t>
<t tx="ekr.20250430053636.94">def unused_assignments(self):
    """
    Return a generator for the assignments which have not been used.
    """
    for name, binding in self.items():
        if (not binding.used and
                name != '_' and  # see issue #202
                name not in self.globals and
                not self.usesLocals and
                isinstance(binding, Assignment)):
            yield name, binding
</t>
<t tx="ekr.20250430053636.95">def unused_annotations(self):
    """
    Return a generator for the annotations which have not been used.
    """
    for name, binding in self.items():
        if not binding.used and isinstance(binding, Annotation):
            yield name, binding
</t>
<t tx="ekr.20250430053636.96">def __init__(self, tree, filename='(none)', builtins=None,
             withDoctest='PYFLAKES_DOCTEST' in os.environ, file_tokens=()):
    self._nodeHandlers = {}
    self._deferred = collections.deque()
    self.deadScopes = []
    self.messages = []
    self.filename = filename
    if builtins:
        self.builtIns = self.builtIns.union(builtins)
    self.withDoctest = withDoctest
    self.exceptHandlers = [()]
    self.root = tree

    self.scopeStack = []
    try:
        scope_tp = Checker._ast_node_scope[type(tree)]
    except KeyError:
        raise RuntimeError('No scope implemented for the node %r' % tree)

    with self.in_scope(scope_tp):
        for builtin in self.builtIns:
            self.addBinding(None, Builtin(builtin))
        self.handleChildren(tree)
        self._run_deferred()

    self.checkDeadScopes()

    if file_tokens:
        warnings.warn(
            '`file_tokens` will be removed in a future version',
            stacklevel=2,
        )
</t>
<t tx="ekr.20250430053636.97">def deferFunction(self, callable):
    """
    Schedule a function handler to be called just before completion.

    This is used for handling function bodies, which must be deferred
    because code later in the file might modify the global scope. When
    `callable` is called, the scope at the time this is called will be
    restored, however it will contain any new bindings added to it.
    """
    self._deferred.append((callable, self.scopeStack[:], self.offset))
</t>
<t tx="ekr.20250430053636.98">def _run_deferred(self):
    orig = (self.scopeStack, self.offset)

    while self._deferred:
        handler, scope, offset = self._deferred.popleft()
        self.scopeStack, self.offset = scope, offset
        handler()

    self.scopeStack, self.offset = orig
</t>
<t tx="ekr.20250430053636.99">def _in_doctest(self):
    return (len(self.scopeStack) &gt;= 2 and
            isinstance(self.scopeStack[1], DoctestScope))
</t>
<t tx="ekr.20250430053637.1">"""
Tests for L{pyflakes.scripts.pyflakes}.
"""

import contextlib
import io
import os
import sys
import shutil
import subprocess
import tempfile

from pyflakes.checker import PYPY
from pyflakes.messages import UnusedImport
from pyflakes.reporter import Reporter
from pyflakes.api import (
    main,
    check,
    checkPath,
    checkRecursive,
    iterSourceCode,
)
from pyflakes.test.harness import TestCase, skipIf


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.10">class TestMain(IntegrationTests):
    """
    Tests of the pyflakes main function.
    """
    @others
</t>
<t tx="ekr.20250430053637.100">def test_no_duplicate_key_errors_vars(self):
    self.flakes('''
    test = 'yes'
    rest = 'yes'
    {test: 1, rest: 2}
    ''')
</t>
<t tx="ekr.20250430053637.101">def test_no_duplicate_key_errors_tuples(self):
    self.flakes('''
    {(0,1): 1, (0,2): 1}
    ''')
</t>
<t tx="ekr.20250430053637.102">def test_no_duplicate_key_errors_instance_attributes(self):
    self.flakes('''
    class Test():
        pass
    f = Test()
    f.a = 1
    {f.a: 1, f.a: 1}
    ''')
</t>
<t tx="ekr.20250430053637.103">import textwrap

from pyflakes import messages as m
from pyflakes.checker import (
    PYPY,
    DoctestScope,
    FunctionScope,
    ModuleScope,
)
from pyflakes.test.test_other import Test as TestOther
from pyflakes.test.test_imports import Test as TestImports
from pyflakes.test.test_undefined_names import Test as TestUndefinedNames
from pyflakes.test.harness import TestCase, skip


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.104">class _DoctestMixin:
    @others
</t>
<t tx="ekr.20250430053637.105">class Test(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.106">class TestOther(_DoctestMixin, TestOther):
    """Run TestOther with each test wrapped in a doctest."""
</t>
<t tx="ekr.20250430053637.107">class TestImports(_DoctestMixin, TestImports):
    """Run TestImports with each test wrapped in a doctest."""
</t>
<t tx="ekr.20250430053637.108">class TestUndefinedNames(_DoctestMixin, TestUndefinedNames):
    """Run TestUndefinedNames with each test wrapped in a doctest."""
</t>
<t tx="ekr.20250430053637.109">withDoctest = True

def doctestify(self, input):
    lines = []
    for line in textwrap.dedent(input).splitlines():
        if line.strip() == '':
            pass
        elif (line.startswith(' ') or
              line.startswith('except:') or
              line.startswith('except ') or
              line.startswith('finally:') or
              line.startswith('else:') or
              line.startswith('elif ') or
              (lines and lines[-1].startswith(('&gt;&gt;&gt; @', '... @')))):
            line = "... %s" % line
        else:
            line = "&gt;&gt;&gt; %s" % line
        lines.append(line)
    doctestificator = textwrap.dedent('''\
        def doctest_something():
            """
               %s
            """
        ''')
    return doctestificator % "\n       ".join(lines)
</t>
<t tx="ekr.20250430053637.11">def __init__(self, lineno, col_offset=0):
    self.lineno = lineno
    self.col_offset = col_offset
</t>
<t tx="ekr.20250430053637.110">def flakes(self, input, *args, **kw):
    return super().flakes(self.doctestify(input), *args, **kw)
</t>
<t tx="ekr.20250430053637.111">withDoctest = True

def test_scope_class(self):
    """Check that a doctest is given a DoctestScope."""
    checker = self.flakes("""
    m = None

    def doctest_stuff():
        '''
            &gt;&gt;&gt; d = doctest_stuff()
        '''
        f = m
        return f
    """)

    scopes = checker.deadScopes
    module_scopes = [
        scope for scope in scopes if scope.__class__ is ModuleScope]
    doctest_scopes = [
        scope for scope in scopes if scope.__class__ is DoctestScope]
    function_scopes = [
        scope for scope in scopes if scope.__class__ is FunctionScope]

    self.assertEqual(len(module_scopes), 1)
    self.assertEqual(len(doctest_scopes), 1)

    module_scope = module_scopes[0]
    doctest_scope = doctest_scopes[0]

    self.assertIsInstance(doctest_scope, DoctestScope)
    self.assertIsInstance(doctest_scope, ModuleScope)
    self.assertNotIsInstance(doctest_scope, FunctionScope)
    self.assertNotIsInstance(module_scope, DoctestScope)

    self.assertIn('m', module_scope)
    self.assertIn('doctest_stuff', module_scope)

    self.assertIn('d', doctest_scope)

    self.assertEqual(len(function_scopes), 1)
    self.assertIn('f', function_scopes[0])
</t>
<t tx="ekr.20250430053637.112">def test_nested_doctest_ignored(self):
    """Check that nested doctests are ignored."""
    checker = self.flakes("""
    m = None

    def doctest_stuff():
        '''
            &gt;&gt;&gt; def function_in_doctest():
            ...     \"\"\"
            ...     &gt;&gt;&gt; ignored_undefined_name
            ...     \"\"\"
            ...     df = m
            ...     return df
            ...
            &gt;&gt;&gt; function_in_doctest()
        '''
        f = m
        return f
    """)

    scopes = checker.deadScopes
    module_scopes = [
        scope for scope in scopes if scope.__class__ is ModuleScope]
    doctest_scopes = [
        scope for scope in scopes if scope.__class__ is DoctestScope]
    function_scopes = [
        scope for scope in scopes if scope.__class__ is FunctionScope]

    self.assertEqual(len(module_scopes), 1)
    self.assertEqual(len(doctest_scopes), 1)

    module_scope = module_scopes[0]
    doctest_scope = doctest_scopes[0]

    self.assertIn('m', module_scope)
    self.assertIn('doctest_stuff', module_scope)
    self.assertIn('function_in_doctest', doctest_scope)

    self.assertEqual(len(function_scopes), 2)

    self.assertIn('f', function_scopes[0])
    self.assertIn('df', function_scopes[1])
</t>
<t tx="ekr.20250430053637.113">def test_global_module_scope_pollution(self):
    """Check that global in doctest does not pollute module scope."""
    checker = self.flakes("""
    def doctest_stuff():
        '''
            &gt;&gt;&gt; def function_in_doctest():
            ...     global m
            ...     m = 50
            ...     df = 10
            ...     m = df
            ...
            &gt;&gt;&gt; function_in_doctest()
        '''
        f = 10
        return f

    """)

    scopes = checker.deadScopes
    module_scopes = [
        scope for scope in scopes if scope.__class__ is ModuleScope]
    doctest_scopes = [
        scope for scope in scopes if scope.__class__ is DoctestScope]
    function_scopes = [
        scope for scope in scopes if scope.__class__ is FunctionScope]

    self.assertEqual(len(module_scopes), 1)
    self.assertEqual(len(doctest_scopes), 1)

    module_scope = module_scopes[0]
    doctest_scope = doctest_scopes[0]

    self.assertIn('doctest_stuff', module_scope)
    self.assertIn('function_in_doctest', doctest_scope)

    self.assertEqual(len(function_scopes), 2)

    self.assertIn('f', function_scopes[0])
    self.assertIn('df', function_scopes[1])
    self.assertIn('m', function_scopes[1])

    self.assertNotIn('m', module_scope)
</t>
<t tx="ekr.20250430053637.114">def test_global_undefined(self):
    self.flakes("""
    global m

    def doctest_stuff():
        '''
            &gt;&gt;&gt; m
        '''
    """, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.115">def test_nested_class(self):
    """Doctest within nested class are processed."""
    self.flakes("""
    class C:
        class D:
            '''
                &gt;&gt;&gt; m
            '''
            def doctest_stuff(self):
                '''
                    &gt;&gt;&gt; m
                '''
                return 1
    """, m.UndefinedName, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.116">def test_ignore_nested_function(self):
    """Doctest module does not process doctest in nested functions."""
    # 'syntax error' would cause a SyntaxError if the doctest was processed.
    # However doctest does not find doctest in nested functions
    # (https://bugs.python.org/issue1650090). If nested functions were
    # processed, this use of m should cause UndefinedName, and the
    # name inner_function should probably exist in the doctest scope.
    self.flakes("""
    def doctest_stuff():
        def inner_function():
            '''
                &gt;&gt;&gt; syntax error
                &gt;&gt;&gt; inner_function()
                1
                &gt;&gt;&gt; m
            '''
            return 1
        m = inner_function()
        return m
    """)
</t>
<t tx="ekr.20250430053637.117">def test_inaccessible_scope_class(self):
    """Doctest may not access class scope."""
    self.flakes("""
    class C:
        def doctest_stuff(self):
            '''
                &gt;&gt;&gt; m
            '''
            return 1
        m = 1
    """, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.118">def test_importBeforeDoctest(self):
    self.flakes("""
    import foo

    def doctest_stuff():
        '''
            &gt;&gt;&gt; foo
        '''
    """)
</t>
<t tx="ekr.20250430053637.119">@skip("todo")
def test_importBeforeAndInDoctest(self):
    self.flakes('''
    import foo

    def doctest_stuff():
        """
            &gt;&gt;&gt; import foo
            &gt;&gt;&gt; foo
        """

    foo
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.12">def __init__(self, stdin):
    self._stdin = io.StringIO(stdin or '', newline=os.linesep)
</t>
<t tx="ekr.20250430053637.120">def test_importInDoctestAndAfter(self):
    self.flakes('''
    def doctest_stuff():
        """
            &gt;&gt;&gt; import foo
            &gt;&gt;&gt; foo
        """

    import foo
    foo()
    ''')
</t>
<t tx="ekr.20250430053637.121">def test_offsetInDoctests(self):
    exc = self.flakes('''

    def doctest_stuff():
        """
            &gt;&gt;&gt; x # line 5
        """

    ''', m.UndefinedName).messages[0]
    self.assertEqual(exc.lineno, 5)
    self.assertEqual(exc.col, 12)
</t>
<t tx="ekr.20250430053637.122">def test_offsetInLambdasInDoctests(self):
    exc = self.flakes('''

    def doctest_stuff():
        """
            &gt;&gt;&gt; lambda: x # line 5
        """

    ''', m.UndefinedName).messages[0]
    self.assertEqual(exc.lineno, 5)
    self.assertEqual(exc.col, 20)
</t>
<t tx="ekr.20250430053637.123">def test_offsetAfterDoctests(self):
    exc = self.flakes('''

    def doctest_stuff():
        """
            &gt;&gt;&gt; x = 5
        """

    x

    ''', m.UndefinedName).messages[0]
    self.assertEqual(exc.lineno, 8)
    self.assertEqual(exc.col, 0)
</t>
<t tx="ekr.20250430053637.124">def test_syntaxErrorInDoctest(self):
    exceptions = self.flakes(
        '''
        def doctest_stuff():
            """
                &gt;&gt;&gt; from # line 4
                &gt;&gt;&gt;     fortytwo = 42
                &gt;&gt;&gt; except Exception:
            """
        ''',
        m.DoctestSyntaxError,
        m.DoctestSyntaxError,
        m.DoctestSyntaxError).messages
    exc = exceptions[0]
    self.assertEqual(exc.lineno, 4)
    if not PYPY:
        self.assertEqual(exc.col, 18)
    else:
        self.assertEqual(exc.col, 26)

    # PyPy error column offset is 0,
    # for the second and third line of the doctest
    # i.e. at the beginning of the line
    exc = exceptions[1]
    self.assertEqual(exc.lineno, 5)
    if PYPY:
        self.assertEqual(exc.col, 13)
    else:
        self.assertEqual(exc.col, 16)
    exc = exceptions[2]
    self.assertEqual(exc.lineno, 6)
    self.assertEqual(exc.col, 13)
</t>
<t tx="ekr.20250430053637.125">def test_indentationErrorInDoctest(self):
    exc = self.flakes('''
    def doctest_stuff():
        """
            &gt;&gt;&gt; if True:
            ... pass
        """
    ''', m.DoctestSyntaxError).messages[0]
    self.assertEqual(exc.lineno, 5)
    self.assertEqual(exc.col, 13)
</t>
<t tx="ekr.20250430053637.126">def test_offsetWithMultiLineArgs(self):
    (exc1, exc2) = self.flakes(
        '''
        def doctest_stuff(arg1,
                          arg2,
                          arg3):
            """
                &gt;&gt;&gt; assert
                &gt;&gt;&gt; this
            """
        ''',
        m.DoctestSyntaxError,
        m.UndefinedName).messages
    self.assertEqual(exc1.lineno, 6)
    self.assertEqual(exc1.col, 19)
    self.assertEqual(exc2.lineno, 7)
    self.assertEqual(exc2.col, 12)
</t>
<t tx="ekr.20250430053637.127">def test_doctestCanReferToFunction(self):
    self.flakes("""
    def foo():
        '''
            &gt;&gt;&gt; foo
        '''
    """)
</t>
<t tx="ekr.20250430053637.128">def test_doctestCanReferToClass(self):
    self.flakes("""
    class Foo():
        '''
            &gt;&gt;&gt; Foo
        '''
        def bar(self):
            '''
                &gt;&gt;&gt; Foo
            '''
    """)
</t>
<t tx="ekr.20250430053637.129">def test_noOffsetSyntaxErrorInDoctest(self):
    exceptions = self.flakes(
        '''
        def buildurl(base, *args, **kwargs):
            """
            &gt;&gt;&gt; buildurl('/blah.php', ('a', '&amp;'), ('b', '=')
            '/blah.php?a=%26&amp;b=%3D'
            &gt;&gt;&gt; buildurl('/blah.php', a='&amp;', 'b'='=')
            '/blah.php?b=%3D&amp;a=%26'
            """
            pass
        ''',
        m.DoctestSyntaxError,
        m.DoctestSyntaxError).messages
    exc = exceptions[0]
    self.assertEqual(exc.lineno, 4)
    exc = exceptions[1]
    self.assertEqual(exc.lineno, 6)
</t>
<t tx="ekr.20250430053637.13">def __enter__(self):
    self._orig_stdin = sys.stdin
    self._orig_stdout = sys.stdout
    self._orig_stderr = sys.stderr

    sys.stdin = self._stdin
    sys.stdout = self._stdout_stringio = io.StringIO(newline=os.linesep)
    sys.stderr = self._stderr_stringio = io.StringIO(newline=os.linesep)

    return self
</t>
<t tx="ekr.20250430053637.130">def test_singleUnderscoreInDoctest(self):
    self.flakes('''
    def func():
        """A docstring

        &gt;&gt;&gt; func()
        1
        &gt;&gt;&gt; _
        1
        """
        return 1
    ''')
</t>
<t tx="ekr.20250430053637.131">def test_globalUnderscoreInDoctest(self):
    self.flakes("""
    from gettext import ugettext as _

    def doctest_stuff():
        '''
            &gt;&gt;&gt; pass
        '''
    """, m.UnusedImport)
</t>
<t tx="ekr.20250430053637.132">from pyflakes import messages as m
from pyflakes.checker import (
    FutureImportation,
    Importation,
    ImportationFrom,
    StarImportation,
    SubmoduleImportation,
)
from pyflakes.test.harness import TestCase, skip


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.133">class TestImportationObject(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.134">class Test(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.135">class TestSpecialAll(TestCase):
    """
    Tests for suppression of unused import warnings by C{__all__}.
    """
    @others
</t>
<t tx="ekr.20250430053637.136">def test_import_basic(self):
    binding = Importation('a', None, 'a')
    assert binding.source_statement == 'import a'
    assert str(binding) == 'a'
</t>
<t tx="ekr.20250430053637.137">def test_import_as(self):
    binding = Importation('c', None, 'a')
    assert binding.source_statement == 'import a as c'
    assert str(binding) == 'a as c'
</t>
<t tx="ekr.20250430053637.138">def test_import_submodule(self):
    binding = SubmoduleImportation('a.b', None)
    assert binding.source_statement == 'import a.b'
    assert str(binding) == 'a.b'
</t>
<t tx="ekr.20250430053637.139">def test_import_submodule_as(self):
    # A submodule import with an as clause is not a SubmoduleImportation
    binding = Importation('c', None, 'a.b')
    assert binding.source_statement == 'import a.b as c'
    assert str(binding) == 'a.b as c'
</t>
<t tx="ekr.20250430053637.14">def __exit__(self, *args):
    self.output = self._stdout_stringio.getvalue()
    self.error = self._stderr_stringio.getvalue()

    sys.stdin = self._orig_stdin
    sys.stdout = self._orig_stdout
    sys.stderr = self._orig_stderr
</t>
<t tx="ekr.20250430053637.140">def test_import_submodule_as_source_name(self):
    binding = Importation('a', None, 'a.b')
    assert binding.source_statement == 'import a.b as a'
    assert str(binding) == 'a.b as a'
</t>
<t tx="ekr.20250430053637.141">def test_importfrom_relative(self):
    binding = ImportationFrom('a', None, '.', 'a')
    assert binding.source_statement == 'from . import a'
    assert str(binding) == '.a'
</t>
<t tx="ekr.20250430053637.142">def test_importfrom_relative_parent(self):
    binding = ImportationFrom('a', None, '..', 'a')
    assert binding.source_statement == 'from .. import a'
    assert str(binding) == '..a'
</t>
<t tx="ekr.20250430053637.143">def test_importfrom_relative_with_module(self):
    binding = ImportationFrom('b', None, '..a', 'b')
    assert binding.source_statement == 'from ..a import b'
    assert str(binding) == '..a.b'
</t>
<t tx="ekr.20250430053637.144">def test_importfrom_relative_with_module_as(self):
    binding = ImportationFrom('c', None, '..a', 'b')
    assert binding.source_statement == 'from ..a import b as c'
    assert str(binding) == '..a.b as c'
</t>
<t tx="ekr.20250430053637.145">def test_importfrom_member(self):
    binding = ImportationFrom('b', None, 'a', 'b')
    assert binding.source_statement == 'from a import b'
    assert str(binding) == 'a.b'
</t>
<t tx="ekr.20250430053637.146">def test_importfrom_submodule_member(self):
    binding = ImportationFrom('c', None, 'a.b', 'c')
    assert binding.source_statement == 'from a.b import c'
    assert str(binding) == 'a.b.c'
</t>
<t tx="ekr.20250430053637.147">def test_importfrom_member_as(self):
    binding = ImportationFrom('c', None, 'a', 'b')
    assert binding.source_statement == 'from a import b as c'
    assert str(binding) == 'a.b as c'
</t>
<t tx="ekr.20250430053637.148">def test_importfrom_submodule_member_as(self):
    binding = ImportationFrom('d', None, 'a.b', 'c')
    assert binding.source_statement == 'from a.b import c as d'
    assert str(binding) == 'a.b.c as d'
</t>
<t tx="ekr.20250430053637.149">def test_importfrom_star(self):
    binding = StarImportation('a.b', None)
    assert binding.source_statement == 'from a.b import *'
    assert str(binding) == 'a.b.*'
</t>
<t tx="ekr.20250430053637.15">def __init__(self, log):
    """
    Construct a C{LoggingReporter}.

    @param log: A list to append log messages to.
    """
    self.log = log
</t>
<t tx="ekr.20250430053637.150">def test_importfrom_star_relative(self):
    binding = StarImportation('.b', None)
    assert binding.source_statement == 'from .b import *'
    assert str(binding) == '.b.*'
</t>
<t tx="ekr.20250430053637.151">def test_importfrom_future(self):
    binding = FutureImportation('print_function', None, None)
    assert binding.source_statement == 'from __future__ import print_function'
    assert str(binding) == '__future__.print_function'
</t>
<t tx="ekr.20250430053637.152">def test_unusedImport_underscore(self):
    """
    The magic underscore var should be reported as unused when used as an
    import alias.
    """
    self.flakes('import fu as _', m.UnusedImport)
</t>
<t tx="ekr.20250430053637.153">def test_unusedImport(self):
    self.flakes('import fu, bar', m.UnusedImport, m.UnusedImport)
    self.flakes('from baz import fu, bar', m.UnusedImport, m.UnusedImport)
</t>
<t tx="ekr.20250430053637.154">def test_unusedImport_relative(self):
    self.flakes('from . import fu', m.UnusedImport)
    self.flakes('from . import fu as baz', m.UnusedImport)
    self.flakes('from .. import fu', m.UnusedImport)
    self.flakes('from ... import fu', m.UnusedImport)
    self.flakes('from .. import fu as baz', m.UnusedImport)
    self.flakes('from .bar import fu', m.UnusedImport)
    self.flakes('from ..bar import fu', m.UnusedImport)
    self.flakes('from ...bar import fu', m.UnusedImport)
    self.flakes('from ...bar import fu as baz', m.UnusedImport)

    checker = self.flakes('from . import fu', m.UnusedImport)

    error = checker.messages[0]
    assert error.message == '%r imported but unused'
    assert error.message_args == ('.fu', )

    checker = self.flakes('from . import fu as baz', m.UnusedImport)

    error = checker.messages[0]
    assert error.message == '%r imported but unused'
    assert error.message_args == ('.fu as baz', )
</t>
<t tx="ekr.20250430053637.155">def test_aliasedImport(self):
    self.flakes('import fu as FU, bar as FU',
                m.RedefinedWhileUnused, m.UnusedImport)
    self.flakes('from moo import fu as FU, bar as FU',
                m.RedefinedWhileUnused, m.UnusedImport)
</t>
<t tx="ekr.20250430053637.156">def test_aliasedImportShadowModule(self):
    """Imported aliases can shadow the source of the import."""
    self.flakes('from moo import fu as moo; moo')
    self.flakes('import fu as fu; fu')
    self.flakes('import fu.bar as fu; fu')
</t>
<t tx="ekr.20250430053637.157">def test_usedImport(self):
    self.flakes('import fu; print(fu)')
    self.flakes('from baz import fu; print(fu)')
    self.flakes('import fu; del fu')
</t>
<t tx="ekr.20250430053637.158">def test_usedImport_relative(self):
    self.flakes('from . import fu; assert fu')
    self.flakes('from .bar import fu; assert fu')
    self.flakes('from .. import fu; assert fu')
    self.flakes('from ..bar import fu as baz; assert baz')
</t>
<t tx="ekr.20250430053637.159">def test_redefinedWhileUnused(self):
    self.flakes('import fu; fu = 3', m.RedefinedWhileUnused)
    self.flakes('import fu; fu, bar = 3', m.RedefinedWhileUnused)
    self.flakes('import fu; [fu, bar] = 3', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.16">def flake(self, message):
    self.log.append(('flake', str(message)))
</t>
<t tx="ekr.20250430053637.160">def test_redefinedIf(self):
    """
    Test that importing a module twice within an if
    block does raise a warning.
    """
    self.flakes('''
    i = 2
    if i==1:
        import os
        import os
    os.path''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.161">def test_redefinedIfElse(self):
    """
    Test that importing a module twice in if
    and else blocks does not raise a warning.
    """
    self.flakes('''
    i = 2
    if i==1:
        import os
    else:
        import os
    os.path''')
</t>
<t tx="ekr.20250430053637.162">def test_redefinedTry(self):
    """
    Test that importing a module twice in a try block
    does raise a warning.
    """
    self.flakes('''
    try:
        import os
        import os
    except:
        pass
    os.path''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.163">def test_redefinedTryExcept(self):
    """
    Test that importing a module twice in a try
    and except block does not raise a warning.
    """
    self.flakes('''
    try:
        import os
    except:
        import os
    os.path''')
</t>
<t tx="ekr.20250430053637.164">def test_redefinedTryNested(self):
    """
    Test that importing a module twice using a nested
    try/except and if blocks does not issue a warning.
    """
    self.flakes('''
    try:
        if True:
            if True:
                import os
    except:
        import os
    os.path''')
</t>
<t tx="ekr.20250430053637.165">def test_redefinedTryExceptMulti(self):
    self.flakes("""
    try:
        from aa import mixer
    except AttributeError:
        from bb import mixer
    except RuntimeError:
        from cc import mixer
    except:
        from dd import mixer
    mixer(123)
    """)
</t>
<t tx="ekr.20250430053637.166">def test_redefinedTryElse(self):
    self.flakes("""
    try:
        from aa import mixer
    except ImportError:
        pass
    else:
        from bb import mixer
    mixer(123)
    """, m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.167">def test_redefinedTryExceptElse(self):
    self.flakes("""
    try:
        import funca
    except ImportError:
        from bb import funca
        from bb import funcb
    else:
        from bbb import funcb
    print(funca, funcb)
    """)
</t>
<t tx="ekr.20250430053637.168">def test_redefinedTryExceptFinally(self):
    self.flakes("""
    try:
        from aa import a
    except ImportError:
        from bb import a
    finally:
        a = 42
    print(a)
    """)
</t>
<t tx="ekr.20250430053637.169">def test_redefinedTryExceptElseFinally(self):
    self.flakes("""
    try:
        import b
    except ImportError:
        b = Ellipsis
        from bb import a
    else:
        from aa import a
    finally:
        a = 42
    print(a, b)
    """)
</t>
<t tx="ekr.20250430053637.17">def unexpectedError(self, filename, message):
    self.log.append(('unexpectedError', filename, message))
</t>
<t tx="ekr.20250430053637.170">def test_redefinedByFunction(self):
    self.flakes('''
    import fu
    def fu():
        pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.171">def test_redefinedInNestedFunction(self):
    """
    Test that shadowing a global name with a nested function definition
    generates a warning.
    """
    self.flakes('''
    import fu
    def bar():
        def baz():
            def fu():
                pass
    ''', m.RedefinedWhileUnused, m.UnusedImport)
</t>
<t tx="ekr.20250430053637.172">def test_redefinedInNestedFunctionTwice(self):
    """
    Test that shadowing a global name with a nested function definition
    generates a warning.
    """
    self.flakes('''
    import fu
    def bar():
        import fu
        def baz():
            def fu():
                pass
    ''',
                m.RedefinedWhileUnused, m.RedefinedWhileUnused,
                m.UnusedImport, m.UnusedImport)
</t>
<t tx="ekr.20250430053637.173">def test_redefinedButUsedLater(self):
    """
    Test that a global import which is redefined locally,
    but used later in another scope does not generate a warning.
    """
    self.flakes('''
    import unittest, transport

    class GetTransportTestCase(unittest.TestCase):
        def test_get_transport(self):
            transport = 'transport'
            self.assertIsNotNone(transport)

    class TestTransportMethodArgs(unittest.TestCase):
        def test_send_defaults(self):
            transport.Transport()
    ''')
</t>
<t tx="ekr.20250430053637.174">def test_redefinedByClass(self):
    self.flakes('''
    import fu
    class fu:
        pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.175">def test_redefinedBySubclass(self):
    """
    If an imported name is redefined by a class statement which also uses
    that name in the bases list, no warning is emitted.
    """
    self.flakes('''
    from fu import bar
    class bar(bar):
        pass
    ''')
</t>
<t tx="ekr.20250430053637.176">def test_redefinedInClass(self):
    """
    Test that shadowing a global with a class attribute does not produce a
    warning.
    """
    self.flakes('''
    import fu
    class bar:
        fu = 1
    print(fu)
    ''')
</t>
<t tx="ekr.20250430053637.177">def test_importInClass(self):
    """
    Test that import within class is a locally scoped attribute.
    """
    self.flakes('''
    class bar:
        import fu
    ''')

    self.flakes('''
    class bar:
        import fu

    fu
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.178">def test_usedInFunction(self):
    self.flakes('''
    import fu
    def fun():
        print(fu)
    ''')
</t>
<t tx="ekr.20250430053637.179">def test_shadowedByParameter(self):
    self.flakes('''
    import fu
    def fun(fu):
        print(fu)
    ''', m.UnusedImport, m.RedefinedWhileUnused)

    self.flakes('''
    import fu
    def fun(fu):
        print(fu)
    print(fu)
    ''')
</t>
<t tx="ekr.20250430053637.18">def syntaxError(self, filename, msg, lineno, offset, line):
    self.log.append(('syntaxError', filename, msg, lineno, offset, line))
</t>
<t tx="ekr.20250430053637.180">def test_newAssignment(self):
    self.flakes('fu = None')
</t>
<t tx="ekr.20250430053637.181">def test_usedInGetattr(self):
    self.flakes('import fu; fu.bar.baz')
    self.flakes('import fu; "bar".fu.baz', m.UnusedImport)
</t>
<t tx="ekr.20250430053637.182">def test_usedInSlice(self):
    self.flakes('import fu; print(fu.bar[1:])')
</t>
<t tx="ekr.20250430053637.183">def test_usedInIfBody(self):
    self.flakes('''
    import fu
    if True: print(fu)
    ''')
</t>
<t tx="ekr.20250430053637.184">def test_usedInIfConditional(self):
    self.flakes('''
    import fu
    if fu: pass
    ''')
</t>
<t tx="ekr.20250430053637.185">def test_usedInElifConditional(self):
    self.flakes('''
    import fu
    if False: pass
    elif fu: pass
    ''')
</t>
<t tx="ekr.20250430053637.186">def test_usedInElse(self):
    self.flakes('''
    import fu
    if False: pass
    else: print(fu)
    ''')
</t>
<t tx="ekr.20250430053637.187">def test_usedInCall(self):
    self.flakes('import fu; fu.bar()')
</t>
<t tx="ekr.20250430053637.188">def test_usedInClass(self):
    self.flakes('''
    import fu
    class bar:
        bar = fu
    ''')
</t>
<t tx="ekr.20250430053637.189">def test_usedInClassBase(self):
    self.flakes('''
    import fu
    class bar(object, fu.baz):
        pass
    ''')
</t>
<t tx="ekr.20250430053637.19">def setUp(self):
    self.tempdir = tempfile.mkdtemp()
</t>
<t tx="ekr.20250430053637.190">def test_notUsedInNestedScope(self):
    self.flakes('''
    import fu
    def bleh():
        pass
    print(fu)
    ''')
</t>
<t tx="ekr.20250430053637.191">def test_usedInFor(self):
    self.flakes('''
    import fu
    for bar in range(9):
        print(fu)
    ''')
</t>
<t tx="ekr.20250430053637.192">def test_usedInForElse(self):
    self.flakes('''
    import fu
    for bar in range(10):
        pass
    else:
        print(fu)
    ''')
</t>
<t tx="ekr.20250430053637.193">def test_redefinedByFor(self):
    self.flakes('''
    import fu
    for fu in range(2):
        pass
    ''', m.ImportShadowedByLoopVar)
</t>
<t tx="ekr.20250430053637.194">def test_shadowedByFor(self):
    """
    Test that shadowing a global name with a for loop variable generates a
    warning.
    """
    self.flakes('''
    import fu
    fu.bar()
    for fu in ():
        pass
    ''', m.ImportShadowedByLoopVar)
</t>
<t tx="ekr.20250430053637.195">def test_shadowedByForDeep(self):
    """
    Test that shadowing a global name with a for loop variable nested in a
    tuple unpack generates a warning.
    """
    self.flakes('''
    import fu
    fu.bar()
    for (x, y, z, (a, b, c, (fu,))) in ():
        pass
    ''', m.ImportShadowedByLoopVar)
    # Same with a list instead of a tuple
    self.flakes('''
    import fu
    fu.bar()
    for [x, y, z, (a, b, c, (fu,))] in ():
        pass
    ''', m.ImportShadowedByLoopVar)
</t>
<t tx="ekr.20250430053637.196">def test_usedInReturn(self):
    self.flakes('''
    import fu
    def fun():
        return fu
    ''')
</t>
<t tx="ekr.20250430053637.197">def test_usedInOperators(self):
    self.flakes('import fu; 3 + fu.bar')
    self.flakes('import fu; 3 % fu.bar')
    self.flakes('import fu; 3 - fu.bar')
    self.flakes('import fu; 3 * fu.bar')
    self.flakes('import fu; 3 ** fu.bar')
    self.flakes('import fu; 3 / fu.bar')
    self.flakes('import fu; 3 // fu.bar')
    self.flakes('import fu; -fu.bar')
    self.flakes('import fu; ~fu.bar')
    self.flakes('import fu; 1 == fu.bar')
    self.flakes('import fu; 1 | fu.bar')
    self.flakes('import fu; 1 &amp; fu.bar')
    self.flakes('import fu; 1 ^ fu.bar')
    self.flakes('import fu; 1 &gt;&gt; fu.bar')
    self.flakes('import fu; 1 &lt;&lt; fu.bar')
</t>
<t tx="ekr.20250430053637.198">def test_usedInAssert(self):
    self.flakes('import fu; assert fu.bar')
</t>
<t tx="ekr.20250430053637.199">def test_usedInSubscript(self):
    self.flakes('import fu; fu.bar[1]')
</t>
<t tx="ekr.20250430053637.2">def withStderrTo(stderr, f, *args, **kwargs):
    """
    Call C{f} with C{sys.stderr} redirected to C{stderr}.
    """
    (outer, sys.stderr) = (sys.stderr, stderr)
    try:
        return f(*args, **kwargs)
    finally:
        sys.stderr = outer
</t>
<t tx="ekr.20250430053637.20">def tearDown(self):
    shutil.rmtree(self.tempdir)
</t>
<t tx="ekr.20250430053637.200">def test_usedInLogic(self):
    self.flakes('import fu; fu and False')
    self.flakes('import fu; fu or False')
    self.flakes('import fu; not fu.bar')
</t>
<t tx="ekr.20250430053637.201">def test_usedInList(self):
    self.flakes('import fu; [fu]')
</t>
<t tx="ekr.20250430053637.202">def test_usedInTuple(self):
    self.flakes('import fu; (fu,)')
</t>
<t tx="ekr.20250430053637.203">def test_usedInTry(self):
    self.flakes('''
    import fu
    try: fu
    except: pass
    ''')
</t>
<t tx="ekr.20250430053637.204">def test_usedInExcept(self):
    self.flakes('''
    import fu
    try: fu
    except: pass
    ''')
</t>
<t tx="ekr.20250430053637.205">def test_redefinedByExcept(self):
    expected = [m.RedefinedWhileUnused]
    # The exc variable is unused inside the exception handler.
    expected.append(m.UnusedVariable)
    self.flakes('''
    import fu
    try: pass
    except Exception as fu: pass
    ''', *expected)
</t>
<t tx="ekr.20250430053637.206">def test_usedInRaise(self):
    self.flakes('''
    import fu
    raise fu.bar
    ''')
</t>
<t tx="ekr.20250430053637.207">def test_usedInYield(self):
    self.flakes('''
    import fu
    def gen():
        yield fu
    ''')
</t>
<t tx="ekr.20250430053637.208">def test_usedInDict(self):
    self.flakes('import fu; {fu:None}')
    self.flakes('import fu; {1:fu}')
</t>
<t tx="ekr.20250430053637.209">def test_usedInParameterDefault(self):
    self.flakes('''
    import fu
    def f(bar=fu):
        pass
    ''')
</t>
<t tx="ekr.20250430053637.21">def makeEmptyFile(self, *parts):
    assert parts
    fpath = os.path.join(self.tempdir, *parts)
    open(fpath, 'a').close()
    return fpath
</t>
<t tx="ekr.20250430053637.210">def test_usedInAttributeAssign(self):
    self.flakes('import fu; fu.bar = 1')
</t>
<t tx="ekr.20250430053637.211">def test_usedInKeywordArg(self):
    self.flakes('import fu; fu.bar(stuff=fu)')
</t>
<t tx="ekr.20250430053637.212">def test_usedInAssignment(self):
    self.flakes('import fu; bar=fu')
    self.flakes('import fu; n=0; n+=fu')
</t>
<t tx="ekr.20250430053637.213">def test_usedInListComp(self):
    self.flakes('import fu; [fu for _ in range(1)]')
    self.flakes('import fu; [1 for _ in range(1) if fu]')
</t>
<t tx="ekr.20250430053637.214">def test_usedInTryFinally(self):
    self.flakes('''
    import fu
    try: pass
    finally: fu
    ''')

    self.flakes('''
    import fu
    try: fu
    finally: pass
    ''')
</t>
<t tx="ekr.20250430053637.215">def test_usedInWhile(self):
    self.flakes('''
    import fu
    while 0:
        fu
    ''')

    self.flakes('''
    import fu
    while fu: pass
    ''')
</t>
<t tx="ekr.20250430053637.216">def test_usedInGlobal(self):
    """
    A 'global' statement shadowing an unused import should not prevent it
    from being reported.
    """
    self.flakes('''
    import fu
    def f(): global fu
    ''', m.UnusedImport)
</t>
<t tx="ekr.20250430053637.217">def test_usedAndGlobal(self):
    """
    A 'global' statement shadowing a used import should not cause it to be
    reported as unused.
    """
    self.flakes('''
        import foo
        def f(): global foo
        def g(): foo.is_used()
    ''')
</t>
<t tx="ekr.20250430053637.218">def test_assignedToGlobal(self):
    """
    Binding an import to a declared global should not cause it to be
    reported as unused.
    """
    self.flakes('''
        def f(): global foo; import foo
        def g(): foo.is_used()
    ''')
</t>
<t tx="ekr.20250430053637.219">def test_usedInExec(self):
    exec_stmt = 'exec("print(1)", fu.bar)'
    self.flakes('import fu; %s' % exec_stmt)
</t>
<t tx="ekr.20250430053637.22">def test_emptyDirectory(self):
    """
    There are no Python files in an empty directory.
    """
    self.assertEqual(list(iterSourceCode([self.tempdir])), [])
</t>
<t tx="ekr.20250430053637.220">def test_usedInLambda(self):
    self.flakes('import fu; lambda: fu')
</t>
<t tx="ekr.20250430053637.221">def test_shadowedByLambda(self):
    self.flakes('import fu; lambda fu: fu',
                m.UnusedImport, m.RedefinedWhileUnused)
    self.flakes('import fu; lambda fu: fu\nfu()')
</t>
<t tx="ekr.20250430053637.222">def test_usedInSliceObj(self):
    self.flakes('import fu; "meow"[::fu]')
</t>
<t tx="ekr.20250430053637.223">def test_unusedInNestedScope(self):
    self.flakes('''
    def bar():
        import fu
    fu
    ''', m.UnusedImport, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.224">def test_methodsDontUseClassScope(self):
    self.flakes('''
    class bar:
        import fu
        def fun(self):
            fu
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.225">def test_nestedFunctionsNestScope(self):
    self.flakes('''
    def a():
        def b():
            fu
        import fu
    ''')
</t>
<t tx="ekr.20250430053637.226">def test_nestedClassAndFunctionScope(self):
    self.flakes('''
    def a():
        import fu
        class b:
            def c(self):
                print(fu)
    ''')
</t>
<t tx="ekr.20250430053637.227">def test_importStar(self):
    """Use of import * at module level is reported."""
    self.flakes('from fu import *', m.ImportStarUsed, m.UnusedImport)
    self.flakes('''
    try:
        from fu import *
    except:
        pass
    ''', m.ImportStarUsed, m.UnusedImport)

    checker = self.flakes('from fu import *',
                          m.ImportStarUsed, m.UnusedImport)

    error = checker.messages[0]
    assert error.message.startswith("'from %s import *' used; unable ")
    assert error.message_args == ('fu', )

    error = checker.messages[1]
    assert error.message == '%r imported but unused'
    assert error.message_args == ('fu.*', )
</t>
<t tx="ekr.20250430053637.228">def test_importStar_relative(self):
    """Use of import * from a relative import is reported."""
    self.flakes('from .fu import *', m.ImportStarUsed, m.UnusedImport)
    self.flakes('''
    try:
        from .fu import *
    except:
        pass
    ''', m.ImportStarUsed, m.UnusedImport)

    checker = self.flakes('from .fu import *',
                          m.ImportStarUsed, m.UnusedImport)

    error = checker.messages[0]
    assert error.message.startswith("'from %s import *' used; unable ")
    assert error.message_args == ('.fu', )

    error = checker.messages[1]
    assert error.message == '%r imported but unused'
    assert error.message_args == ('.fu.*', )

    checker = self.flakes('from .. import *',
                          m.ImportStarUsed, m.UnusedImport)

    error = checker.messages[0]
    assert error.message.startswith("'from %s import *' used; unable ")
    assert error.message_args == ('..', )

    error = checker.messages[1]
    assert error.message == '%r imported but unused'
    assert error.message_args == ('from .. import *', )
</t>
<t tx="ekr.20250430053637.229">def test_localImportStar(self):
    """import * is only allowed at module level."""
    self.flakes('''
    def a():
        from fu import *
    ''', m.ImportStarNotPermitted)
    self.flakes('''
    class a:
        from fu import *
    ''', m.ImportStarNotPermitted)

    checker = self.flakes('''
    class a:
        from .. import *
    ''', m.ImportStarNotPermitted)
    error = checker.messages[0]
    assert error.message == "'from %s import *' only allowed at module level"
    assert error.message_args == ('..', )
</t>
<t tx="ekr.20250430053637.23">def test_singleFile(self):
    """
    If the directory contains one Python file, C{iterSourceCode} will find
    it.
    """
    childpath = self.makeEmptyFile('foo.py')
    self.assertEqual(list(iterSourceCode([self.tempdir])), [childpath])
</t>
<t tx="ekr.20250430053637.230">def test_packageImport(self):
    """
    If a dotted name is imported and used, no warning is reported.
    """
    self.flakes('''
    import fu.bar
    fu.bar
    ''')
</t>
<t tx="ekr.20250430053637.231">def test_unusedPackageImport(self):
    """
    If a dotted name is imported and not used, an unused import warning is
    reported.
    """
    self.flakes('import fu.bar', m.UnusedImport)
</t>
<t tx="ekr.20250430053637.232">def test_duplicateSubmoduleImport(self):
    """
    If a submodule of a package is imported twice, an unused import warning
    and a redefined while unused warning are reported.
    """
    self.flakes('''
    import fu.bar, fu.bar
    fu.bar
    ''', m.RedefinedWhileUnused)
    self.flakes('''
    import fu.bar
    import fu.bar
    fu.bar
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.233">def test_differentSubmoduleImport(self):
    """
    If two different submodules of a package are imported, no duplicate
    import warning is reported for the package.
    """
    self.flakes('''
    import fu.bar, fu.baz
    fu.bar, fu.baz
    ''')
    self.flakes('''
    import fu.bar
    import fu.baz
    fu.bar, fu.baz
    ''')
</t>
<t tx="ekr.20250430053637.234">def test_used_package_with_submodule_import(self):
    """
    Usage of package marks submodule imports as used.
    """
    self.flakes('''
    import fu
    import fu.bar
    fu.x
    ''')

    self.flakes('''
    import fu.bar
    import fu
    fu.x
    ''')
</t>
<t tx="ekr.20250430053637.235">def test_used_package_with_submodule_import_of_alias(self):
    """
    Usage of package by alias marks submodule imports as used.
    """
    self.flakes('''
    import foo as f
    import foo.bar
    f.bar.do_something()
    ''')

    self.flakes('''
    import foo as f
    import foo.bar.blah
    f.bar.blah.do_something()
    ''')
</t>
<t tx="ekr.20250430053637.236">def test_unused_package_with_submodule_import(self):
    """
    When a package and its submodule are imported, only report once.
    """
    checker = self.flakes('''
    import fu
    import fu.bar
    ''', m.UnusedImport)
    error = checker.messages[0]
    assert error.message == '%r imported but unused'
    assert error.message_args == ('fu.bar', )
    assert error.lineno == 5 if self.withDoctest else 3
</t>
<t tx="ekr.20250430053637.237">def test_assignRHSFirst(self):
    self.flakes('import fu; fu = fu')
    self.flakes('import fu; fu, bar = fu')
    self.flakes('import fu; [fu, bar] = fu')
    self.flakes('import fu; fu += fu')
</t>
<t tx="ekr.20250430053637.238">def test_tryingMultipleImports(self):
    self.flakes('''
    try:
        import fu
    except ImportError:
        import bar as fu
    fu
    ''')
</t>
<t tx="ekr.20250430053637.239">def test_nonGlobalDoesNotRedefine(self):
    self.flakes('''
    import fu
    def a():
        fu = 3
        return fu
    fu
    ''')
</t>
<t tx="ekr.20250430053637.24">def test_onlyPythonSource(self):
    """
    Files that are not Python source files are not included.
    """
    self.makeEmptyFile('foo.pyc')
    self.assertEqual(list(iterSourceCode([self.tempdir])), [])
</t>
<t tx="ekr.20250430053637.240">def test_functionsRunLater(self):
    self.flakes('''
    def a():
        fu
    import fu
    ''')
</t>
<t tx="ekr.20250430053637.241">def test_functionNamesAreBoundNow(self):
    self.flakes('''
    import fu
    def fu():
        fu
    fu
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.242">def test_ignoreNonImportRedefinitions(self):
    self.flakes('a = 1; a = 2')
</t>
<t tx="ekr.20250430053637.243">@skip("todo")
def test_importingForImportError(self):
    self.flakes('''
    try:
        import fu
    except ImportError:
        pass
    ''')
</t>
<t tx="ekr.20250430053637.244">def test_importedInClass(self):
    """Imports in class scope can be used through self."""
    self.flakes('''
    class c:
        import i
        def __init__(self):
            self.i
    ''')
</t>
<t tx="ekr.20250430053637.245">def test_importUsedInMethodDefinition(self):
    """
    Method named 'foo' with default args referring to module named 'foo'.
    """
    self.flakes('''
    import foo

    class Thing(object):
        def foo(self, parser=foo.parse_foo):
            pass
    ''')
</t>
<t tx="ekr.20250430053637.246">def test_futureImport(self):
    """__future__ is special."""
    self.flakes('from __future__ import division')
    self.flakes('''
    "docstring is allowed before future import"
    from __future__ import division
    ''')
</t>
<t tx="ekr.20250430053637.247">def test_futureImportFirst(self):
    """
    __future__ imports must come before anything else.
    """
    self.flakes('''
    x = 5
    from __future__ import division
    ''', m.LateFutureImport)
    self.flakes('''
    from foo import bar
    from __future__ import division
    bar
    ''', m.LateFutureImport)
</t>
<t tx="ekr.20250430053637.248">def test_futureImportUsed(self):
    """__future__ is special, but names are injected in the namespace."""
    self.flakes('''
    from __future__ import division
    from __future__ import print_function

    assert print_function is not division
    ''')
</t>
<t tx="ekr.20250430053637.249">def test_futureImportUndefined(self):
    """Importing undefined names from __future__ fails."""
    self.flakes('''
    from __future__ import print_statement
    ''', m.FutureFeatureNotDefined)
</t>
<t tx="ekr.20250430053637.25">def test_recurses(self):
    """
    If the Python files are hidden deep down in child directories, we will
    find them.
    """
    os.mkdir(os.path.join(self.tempdir, 'foo'))
    apath = self.makeEmptyFile('foo', 'a.py')
    self.makeEmptyFile('foo', 'a.py~')
    os.mkdir(os.path.join(self.tempdir, 'bar'))
    bpath = self.makeEmptyFile('bar', 'b.py')
    cpath = self.makeEmptyFile('c.py')
    self.assertEqual(
        sorted(iterSourceCode([self.tempdir])),
        sorted([apath, bpath, cpath]))
</t>
<t tx="ekr.20250430053637.250">def test_futureImportStar(self):
    """Importing '*' from __future__ fails."""
    self.flakes('''
    from __future__ import *
    ''', m.FutureFeatureNotDefined)
</t>
<t tx="ekr.20250430053637.251">def test_ignoredInFunction(self):
    """
    An C{__all__} definition does not suppress unused import warnings in a
    function scope.
    """
    self.flakes('''
    def foo():
        import bar
        __all__ = ["bar"]
    ''', m.UnusedImport, m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.252">def test_ignoredInClass(self):
    """
    An C{__all__} definition in a class does not suppress unused import warnings.
    """
    self.flakes('''
    import bar
    class foo:
        __all__ = ["bar"]
    ''', m.UnusedImport)
</t>
<t tx="ekr.20250430053637.253">def test_ignored_when_not_directly_assigned(self):
    self.flakes('''
    import bar
    (__all__,) = ("foo",)
    ''', m.UnusedImport)
</t>
<t tx="ekr.20250430053637.254">def test_warningSuppressed(self):
    """
    If a name is imported and unused but is named in C{__all__}, no warning
    is reported.
    """
    self.flakes('''
    import foo
    __all__ = ["foo"]
    ''')
    self.flakes('''
    import foo
    __all__ = ("foo",)
    ''')
</t>
<t tx="ekr.20250430053637.255">def test_augmentedAssignment(self):
    """
    The C{__all__} variable is defined incrementally.
    """
    self.flakes('''
    import a
    import c
    __all__ = ['a']
    __all__ += ['b']
    if 1 &lt; 3:
        __all__ += ['c', 'd']
    ''', m.UndefinedExport, m.UndefinedExport)
</t>
<t tx="ekr.20250430053637.256">def test_list_concatenation_assignment(self):
    """
    The C{__all__} variable is defined through list concatenation.
    """
    self.flakes('''
    import sys
    __all__ = ['a'] + ['b'] + ['c']
    ''', m.UndefinedExport, m.UndefinedExport, m.UndefinedExport, m.UnusedImport)
</t>
<t tx="ekr.20250430053637.257">def test_tuple_concatenation_assignment(self):
    """
    The C{__all__} variable is defined through tuple concatenation.
    """
    self.flakes('''
    import sys
    __all__ = ('a',) + ('b',) + ('c',)
    ''', m.UndefinedExport, m.UndefinedExport, m.UndefinedExport, m.UnusedImport)
</t>
<t tx="ekr.20250430053637.258">def test_all_with_attributes(self):
    self.flakes('''
    from foo import bar
    __all__ = [bar.__name__]
    ''')
</t>
<t tx="ekr.20250430053637.259">def test_all_with_names(self):
    # not actually valid, but shouldn't produce a crash
    self.flakes('''
    from foo import bar
    __all__ = [bar]
    ''')
</t>
<t tx="ekr.20250430053637.26">def test_shebang(self):
    """
    Find Python files that don't end with `.py`, but contain a Python
    shebang.
    """
    python = os.path.join(self.tempdir, 'a')
    with open(python, 'w') as fd:
        fd.write('#!/usr/bin/env python\n')

    self.makeEmptyFile('b')

    with open(os.path.join(self.tempdir, 'c'), 'w') as fd:
        fd.write('hello\nworld\n')

    python3 = os.path.join(self.tempdir, 'e')
    with open(python3, 'w') as fd:
        fd.write('#!/usr/bin/env python3\n')

    pythonw = os.path.join(self.tempdir, 'f')
    with open(pythonw, 'w') as fd:
        fd.write('#!/usr/bin/env pythonw\n')

    python3args = os.path.join(self.tempdir, 'g')
    with open(python3args, 'w') as fd:
        fd.write('#!/usr/bin/python3 -u\n')

    python3d = os.path.join(self.tempdir, 'i')
    with open(python3d, 'w') as fd:
        fd.write('#!/usr/local/bin/python3d\n')

    python38m = os.path.join(self.tempdir, 'j')
    with open(python38m, 'w') as fd:
        fd.write('#! /usr/bin/env python3.8m\n')

    # Should NOT be treated as Python source
    notfirst = os.path.join(self.tempdir, 'l')
    with open(notfirst, 'w') as fd:
        fd.write('#!/bin/sh\n#!/usr/bin/python\n')

    self.assertEqual(
        sorted(iterSourceCode([self.tempdir])),
        sorted([
            python, python3, pythonw, python3args, python3d,
            python38m,
        ]))
</t>
<t tx="ekr.20250430053637.260">def test_all_with_attributes_added(self):
    self.flakes('''
    from foo import bar
    from bar import baz
    __all__ = [bar.__name__] + [baz.__name__]
    ''')
</t>
<t tx="ekr.20250430053637.261">def test_all_mixed_attributes_and_strings(self):
    self.flakes('''
    from foo import bar
    from foo import baz
    __all__ = ['bar', baz.__name__]
    ''')
</t>
<t tx="ekr.20250430053637.262">def test_unboundExported(self):
    """
    If C{__all__} includes a name which is not bound, a warning is emitted.
    """
    self.flakes('''
    __all__ = ["foo"]
    ''', m.UndefinedExport)

    # Skip this in __init__.py though, since the rules there are a little
    # different.
    for filename in ["foo/__init__.py", "__init__.py"]:
        self.flakes('''
        __all__ = ["foo"]
        ''', filename=filename)
</t>
<t tx="ekr.20250430053637.263">def test_importStarExported(self):
    """
    Report undefined if import * is used
    """
    self.flakes('''
    from math import *
    __all__ = ['sin', 'cos']
    csc(1)
    ''', m.ImportStarUsed, m.ImportStarUsage, m.ImportStarUsage, m.ImportStarUsage)
</t>
<t tx="ekr.20250430053637.264">def test_importStarNotExported(self):
    """Report unused import when not needed to satisfy __all__."""
    self.flakes('''
    from foolib import *
    a = 1
    __all__ = ['a']
    ''', m.ImportStarUsed, m.UnusedImport)
</t>
<t tx="ekr.20250430053637.265">def test_usedInGenExp(self):
    """
    Using a global in a generator expression results in no warnings.
    """
    self.flakes('import fu; (fu for _ in range(1))')
    self.flakes('import fu; (1 for _ in range(1) if fu)')
</t>
<t tx="ekr.20250430053637.266">def test_redefinedByGenExp(self):
    """
    Re-using a global name as the loop variable for a generator
    expression results in a redefinition warning.
    """
    self.flakes('import fu; (1 for fu in range(1))',
                m.RedefinedWhileUnused, m.UnusedImport)
</t>
<t tx="ekr.20250430053637.267">def test_usedAsDecorator(self):
    """
    Using a global name in a decorator statement results in no warnings,
    but using an undefined name in a decorator statement results in an
    undefined name warning.
    """
    self.flakes('''
    from interior import decorate
    @decorate
    def f():
        return "hello"
    ''')

    self.flakes('''
    from interior import decorate
    @decorate('value')
    def f():
        return "hello"
    ''')

    self.flakes('''
    @decorate
    def f():
        return "hello"
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.268">def test_usedAsClassDecorator(self):
    """
    Using an imported name as a class decorator results in no warnings,
    but using an undefined name as a class decorator results in an
    undefined name warning.
    """
    self.flakes('''
    from interior import decorate
    @decorate
    class foo:
        pass
    ''')

    self.flakes('''
    from interior import decorate
    @decorate("foo")
    class bar:
        pass
    ''')

    self.flakes('''
    @decorate
    class foo:
        pass
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.269">from pyflakes.messages import IsLiteral
from pyflakes.test.harness import TestCase


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.27">def test_multipleDirectories(self):
    """
    L{iterSourceCode} can be given multiple directories.  It will recurse
    into each of them.
    """
    foopath = os.path.join(self.tempdir, 'foo')
    barpath = os.path.join(self.tempdir, 'bar')
    os.mkdir(foopath)
    apath = self.makeEmptyFile('foo', 'a.py')
    os.mkdir(barpath)
    bpath = self.makeEmptyFile('bar', 'b.py')
    self.assertEqual(
        sorted(iterSourceCode([foopath, barpath])),
        sorted([apath, bpath]))
</t>
<t tx="ekr.20250430053637.270">class Test(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.271">def test_is_str(self):
    self.flakes("""
    x = 'foo'
    if x is 'foo':
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.272">def test_is_bytes(self):
    self.flakes("""
    x = b'foo'
    if x is b'foo':
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.273">def test_is_unicode(self):
    self.flakes("""
    x = u'foo'
    if x is u'foo':
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.274">def test_is_int(self):
    self.flakes("""
    x = 10
    if x is 10:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.275">def test_is_true(self):
    self.flakes("""
    x = True
    if x is True:
        pass
    """)
</t>
<t tx="ekr.20250430053637.276">def test_is_false(self):
    self.flakes("""
    x = False
    if x is False:
        pass
    """)
</t>
<t tx="ekr.20250430053637.277">def test_is_not_str(self):
    self.flakes("""
    x = 'foo'
    if x is not 'foo':
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.278">def test_is_not_bytes(self):
    self.flakes("""
    x = b'foo'
    if x is not b'foo':
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.279">def test_is_not_unicode(self):
    self.flakes("""
    x = u'foo'
    if x is not u'foo':
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.28">def test_explicitFiles(self):
    """
    If one of the paths given to L{iterSourceCode} is not a directory but
    a file, it will include that in its output.
    """
    epath = self.makeEmptyFile('e.py')
    self.assertEqual(list(iterSourceCode([epath])),
                     [epath])
</t>
<t tx="ekr.20250430053637.280">def test_is_not_int(self):
    self.flakes("""
    x = 10
    if x is not 10:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.281">def test_is_not_true(self):
    self.flakes("""
    x = True
    if x is not True:
        pass
    """)
</t>
<t tx="ekr.20250430053637.282">def test_is_not_false(self):
    self.flakes("""
    x = False
    if x is not False:
        pass
    """)
</t>
<t tx="ekr.20250430053637.283">def test_left_is_str(self):
    self.flakes("""
    x = 'foo'
    if 'foo' is x:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.284">def test_left_is_bytes(self):
    self.flakes("""
    x = b'foo'
    if b'foo' is x:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.285">def test_left_is_unicode(self):
    self.flakes("""
    x = u'foo'
    if u'foo' is x:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.286">def test_left_is_int(self):
    self.flakes("""
    x = 10
    if 10 is x:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.287">def test_left_is_true(self):
    self.flakes("""
    x = True
    if True is x:
        pass
    """)
</t>
<t tx="ekr.20250430053637.288">def test_left_is_false(self):
    self.flakes("""
    x = False
    if False is x:
        pass
    """)
</t>
<t tx="ekr.20250430053637.289">def test_left_is_not_str(self):
    self.flakes("""
    x = 'foo'
    if 'foo' is not x:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.29">def test_syntaxError(self):
    """
    C{syntaxError} reports that there was a syntax error in the source
    file.  It reports to the error stream and includes the filename, line
    number, error message, actual line of source and a caret pointing to
    where the error is.
    """
    err = io.StringIO()
    reporter = Reporter(None, err)
    reporter.syntaxError('foo.py', 'a problem', 3, 8, 'bad line of source')
    self.assertEqual(
        ("foo.py:3:8: a problem\n"
         "bad line of source\n"
         "       ^\n"),
        err.getvalue())
</t>
<t tx="ekr.20250430053637.290">def test_left_is_not_bytes(self):
    self.flakes("""
    x = b'foo'
    if b'foo' is not x:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.291">def test_left_is_not_unicode(self):
    self.flakes("""
    x = u'foo'
    if u'foo' is not x:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.292">def test_left_is_not_int(self):
    self.flakes("""
    x = 10
    if 10 is not x:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.293">def test_left_is_not_true(self):
    self.flakes("""
    x = True
    if True is not x:
        pass
    """)
</t>
<t tx="ekr.20250430053637.294">def test_left_is_not_false(self):
    self.flakes("""
    x = False
    if False is not x:
        pass
    """)
</t>
<t tx="ekr.20250430053637.295">def test_chained_operators_is_true(self):
    self.flakes("""
    x = 5
    if x is True &lt; 4:
        pass
    """)
</t>
<t tx="ekr.20250430053637.296">def test_chained_operators_is_str(self):
    self.flakes("""
    x = 5
    if x is 'foo' &lt; 4:
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.297">def test_chained_operators_is_true_end(self):
    self.flakes("""
    x = 5
    if 4 &lt; x is True:
        pass
    """)
</t>
<t tx="ekr.20250430053637.298">def test_chained_operators_is_str_end(self):
    self.flakes("""
    x = 5
    if 4 &lt; x is 'foo':
        pass
    """, IsLiteral)
</t>
<t tx="ekr.20250430053637.299">def test_is_tuple_constant(self):
    self.flakes('''\
        x = 5
        if x is ():
            pass
    ''', IsLiteral)
</t>
<t tx="ekr.20250430053637.3">class Node:
    """
    Mock an AST node.
    """
    @others
</t>
<t tx="ekr.20250430053637.30">def test_syntaxErrorNoOffset(self):
    """
    C{syntaxError} doesn't include a caret pointing to the error if
    C{offset} is passed as C{None}.
    """
    err = io.StringIO()
    reporter = Reporter(None, err)
    reporter.syntaxError('foo.py', 'a problem', 3, None,
                         'bad line of source')
    self.assertEqual(
        ("foo.py:3: a problem\n"
         "bad line of source\n"),
        err.getvalue())
</t>
<t tx="ekr.20250430053637.300">def test_is_tuple_constant_containing_constants(self):
    self.flakes('''\
        x = 5
        if x is (1, '2', True, (1.5, ())):
            pass
    ''', IsLiteral)
</t>
<t tx="ekr.20250430053637.301">def test_is_tuple_containing_variables_ok(self):
    # a bit nonsensical, but does not trigger a SyntaxWarning
    self.flakes('''\
        x = 5
        if x is (x,):
            pass
    ''')
</t>
<t tx="ekr.20250430053637.302">from sys import version_info

from pyflakes.test.harness import TestCase, skipIf


@skipIf(version_info &lt; (3, 10), "Python &gt;= 3.10 only")
@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.303">class TestMatch(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.304">def test_match_bindings(self):
    self.flakes('''
        def f():
            x = 1
            match x:
                case 1 as y:
                    print(f'matched as {y}')
    ''')
    self.flakes('''
        def f():
            x = [1, 2, 3]
            match x:
                case [1, y, 3]:
                    print(f'matched {y}')
    ''')
    self.flakes('''
        def f():
            x = {'foo': 1}
            match x:
                case {'foo': y}:
                    print(f'matched {y}')
    ''')
</t>
<t tx="ekr.20250430053637.305">def test_match_pattern_matched_class(self):
    self.flakes('''
        from a import B

        match 1:
            case B(x=1) as y:
                print(f'matched {y}')
    ''')
    self.flakes('''
        from a import B

        match 1:
            case B(a, x=z) as y:
                print(f'matched {y} {a} {z}')
    ''')
</t>
<t tx="ekr.20250430053637.306">def test_match_placeholder(self):
    self.flakes('''
        def f():
            match 1:
                case _:
                    print('catchall!')
    ''')
</t>
<t tx="ekr.20250430053637.307">def test_match_singleton(self):
    self.flakes('''
        match 1:
            case True:
                print('true')
    ''')
</t>
<t tx="ekr.20250430053637.308">def test_match_or_pattern(self):
    self.flakes('''
        match 1:
            case 1 | 2:
                print('one or two')
    ''')
</t>
<t tx="ekr.20250430053637.309">def test_match_star(self):
    self.flakes('''
        x = [1, 2, 3]
        match x:
            case [1, *y]:
                print(f'captured: {y}')
    ''')
</t>
<t tx="ekr.20250430053637.31">def test_syntaxErrorNoText(self):
    """
    C{syntaxError} doesn't include text or nonsensical offsets if C{text} is C{None}.

    This typically happens when reporting syntax errors from stdin.
    """
    err = io.StringIO()
    reporter = Reporter(None, err)
    reporter.syntaxError('&lt;stdin&gt;', 'a problem', 0, 0, None)
    self.assertEqual(("&lt;stdin&gt;:1:1: a problem\n"), err.getvalue())
</t>
<t tx="ekr.20250430053637.310">def test_match_double_star(self):
    self.flakes('''
        x = {'foo': 'bar', 'baz': 'womp'}
        match x:
            case {'foo': k1, **rest}:
                print(f'{k1=} {rest=}')
    ''')
</t>
<t tx="ekr.20250430053637.311">def test_defined_in_different_branches(self):
    self.flakes('''
        def f(x):
            match x:
                case 1:
                    def y(): pass
                case _:
                    def y(): print(1)
            return y
    ''')
</t>
<t tx="ekr.20250430053637.312">"""
Tests for various Pyflakes behavior.
"""

from sys import version_info

from pyflakes import messages as m
from pyflakes.test.harness import TestCase, skip, skipIf


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.313">class Test(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.314">class TestUnusedAssignment(TestCase):
    """
    Tests for warning about unused assignments.
    """
    @others
</t>
<t tx="ekr.20250430053637.315">class TestStringFormatting(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.316">class TestAsyncStatements(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.317">class TestIncompatiblePrintOperator(TestCase):
    """
    Tests for warning about invalid use of print function.
    """
    @others
</t>
<t tx="ekr.20250430053637.318">def test_duplicateArgs(self):
    self.flakes('def fu(bar, bar): pass', m.DuplicateArgument)
</t>
<t tx="ekr.20250430053637.319">def test_localReferencedBeforeAssignment(self):
    self.flakes('''
    a = 1
    def f():
        a; a=1
    f()
    ''', m.UndefinedLocal, m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.32">def test_multiLineSyntaxError(self):
    """
    If there's a multi-line syntax error, then we only report the last
    line.  The offset is adjusted so that it is relative to the start of
    the last line.
    """
    err = io.StringIO()
    lines = [
        'bad line of source',
        'more bad lines of source',
    ]
    reporter = Reporter(None, err)
    reporter.syntaxError('foo.py', 'a problem', 3, len(lines[0]) + 7,
                         '\n'.join(lines))
    self.assertEqual(
        ("foo.py:3:25: a problem\n" +
         lines[-1] + "\n" +
         " " * 24 + "^\n"),
        err.getvalue())
</t>
<t tx="ekr.20250430053637.320">def test_redefinedInGenerator(self):
    """
    Test that reusing a variable in a generator does not raise
    a warning.
    """
    self.flakes('''
    a = 1
    (1 for a, b in [(1, 2)])
    ''')
    self.flakes('''
    class A:
        a = 1
        list(1 for a, b in [(1, 2)])
    ''')
    self.flakes('''
    def f():
        a = 1
        (1 for a, b in [(1, 2)])
    ''', m.UnusedVariable)
    self.flakes('''
    (1 for a, b in [(1, 2)])
    (1 for a, b in [(1, 2)])
    ''')
    self.flakes('''
    for a, b in [(1, 2)]:
        pass
    (1 for a, b in [(1, 2)])
    ''')
</t>
<t tx="ekr.20250430053637.321">def test_redefinedInSetComprehension(self):
    """
    Test that reusing a variable in a set comprehension does not raise
    a warning.
    """
    self.flakes('''
    a = 1
    {1 for a, b in [(1, 2)]}
    ''')
    self.flakes('''
    class A:
        a = 1
        {1 for a, b in [(1, 2)]}
    ''')
    self.flakes('''
    def f():
        a = 1
        {1 for a, b in [(1, 2)]}
    ''', m.UnusedVariable)
    self.flakes('''
    {1 for a, b in [(1, 2)]}
    {1 for a, b in [(1, 2)]}
    ''')
    self.flakes('''
    for a, b in [(1, 2)]:
        pass
    {1 for a, b in [(1, 2)]}
    ''')
</t>
<t tx="ekr.20250430053637.322">def test_redefinedInDictComprehension(self):
    """
    Test that reusing a variable in a dict comprehension does not raise
    a warning.
    """
    self.flakes('''
    a = 1
    {1: 42 for a, b in [(1, 2)]}
    ''')
    self.flakes('''
    class A:
        a = 1
        {1: 42 for a, b in [(1, 2)]}
    ''')
    self.flakes('''
    def f():
        a = 1
        {1: 42 for a, b in [(1, 2)]}
    ''', m.UnusedVariable)
    self.flakes('''
    {1: 42 for a, b in [(1, 2)]}
    {1: 42 for a, b in [(1, 2)]}
    ''')
    self.flakes('''
    for a, b in [(1, 2)]:
        pass
    {1: 42 for a, b in [(1, 2)]}
    ''')
</t>
<t tx="ekr.20250430053637.323">def test_redefinedFunction(self):
    """
    Test that shadowing a function definition with another one raises a
    warning.
    """
    self.flakes('''
    def a(): pass
    def a(): pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.324">def test_redefined_function_shadows_variable(self):
    self.flakes('''
    x = 1
    def x(): pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.325">def test_redefinedUnderscoreFunction(self):
    """
    Test that shadowing a function definition named with underscore doesn't
    raise anything.
    """
    self.flakes('''
    def _(): pass
    def _(): pass
    ''')
</t>
<t tx="ekr.20250430053637.326">def test_redefinedUnderscoreImportation(self):
    """
    Test that shadowing an underscore importation raises a warning.
    """
    self.flakes('''
    from .i18n import _
    def _(): pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.327">def test_redefinedClassFunction(self):
    """
    Test that shadowing a function definition in a class suite with another
    one raises a warning.
    """
    self.flakes('''
    class A:
        def a(): pass
        def a(): pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.328">def test_redefinedIfElseFunction(self):
    """
    Test that shadowing a function definition twice in an if
    and else block does not raise a warning.
    """
    self.flakes('''
    if True:
        def a(): pass
    else:
        def a(): pass
    ''')
</t>
<t tx="ekr.20250430053637.329">def test_redefinedIfFunction(self):
    """
    Test that shadowing a function definition within an if block
    raises a warning.
    """
    self.flakes('''
    if True:
        def a(): pass
        def a(): pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.33">def test_unexpectedError(self):
    """
    C{unexpectedError} reports an error processing a source file.
    """
    err = io.StringIO()
    reporter = Reporter(None, err)
    reporter.unexpectedError('source.py', 'error message')
    self.assertEqual('source.py: error message\n', err.getvalue())
</t>
<t tx="ekr.20250430053637.330">def test_redefinedTryExceptFunction(self):
    """
    Test that shadowing a function definition twice in try
    and except block does not raise a warning.
    """
    self.flakes('''
    try:
        def a(): pass
    except:
        def a(): pass
    ''')
</t>
<t tx="ekr.20250430053637.331">def test_redefinedTryFunction(self):
    """
    Test that shadowing a function definition within a try block
    raises a warning.
    """
    self.flakes('''
    try:
        def a(): pass
        def a(): pass
    except:
        pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.332">def test_redefinedIfElseInListComp(self):
    """
    Test that shadowing a variable in a list comprehension in
    an if and else block does not raise a warning.
    """
    self.flakes('''
    if False:
        a = 1
    else:
        [a for a in '12']
    ''')
</t>
<t tx="ekr.20250430053637.333">def test_functionDecorator(self):
    """
    Test that shadowing a function definition with a decorated version of
    that function does not raise a warning.
    """
    self.flakes('''
    from somewhere import somedecorator

    def a(): pass
    a = somedecorator(a)
    ''')
</t>
<t tx="ekr.20250430053637.334">def test_classFunctionDecorator(self):
    """
    Test that shadowing a function definition in a class suite with a
    decorated version of that function does not raise a warning.
    """
    self.flakes('''
    class A:
        def a(): pass
        a = classmethod(a)
    ''')
</t>
<t tx="ekr.20250430053637.335">def test_modernProperty(self):
    self.flakes("""
    class A:
        @property
        def t(self):
            pass
        @t.setter
        def t(self, value):
            pass
        @t.deleter
        def t(self):
            pass
    """)
</t>
<t tx="ekr.20250430053637.336">def test_unaryPlus(self):
    """Don't die on unary +."""
    self.flakes('+1')
</t>
<t tx="ekr.20250430053637.337">def test_undefinedBaseClass(self):
    """
    If a name in the base list of a class definition is undefined, a
    warning is emitted.
    """
    self.flakes('''
    class foo(foo):
        pass
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.338">def test_classNameUndefinedInClassBody(self):
    """
    If a class name is used in the body of that class's definition and
    the name is not already defined, a warning is emitted.
    """
    self.flakes('''
    class foo:
        foo
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.339">def test_classNameDefinedPreviously(self):
    """
    If a class name is used in the body of that class's definition and
    the name was previously defined in some other way, no warning is
    emitted.
    """
    self.flakes('''
    foo = None
    class foo:
        foo
    ''')
</t>
<t tx="ekr.20250430053637.34">def test_flake(self):
    """
    C{flake} reports a code warning from Pyflakes.  It is exactly the
    str() of a L{pyflakes.messages.Message}.
    """
    out = io.StringIO()
    reporter = Reporter(out, None)
    message = UnusedImport('foo.py', Node(42), 'bar')
    reporter.flake(message)
    self.assertEqual(out.getvalue(), f"{message}\n")
</t>
<t tx="ekr.20250430053637.340">def test_classRedefinition(self):
    """
    If a class is defined twice in the same module, a warning is emitted.
    """
    self.flakes('''
    class Foo:
        pass
    class Foo:
        pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.341">def test_functionRedefinedAsClass(self):
    """
    If a function is redefined as a class, a warning is emitted.
    """
    self.flakes('''
    def Foo():
        pass
    class Foo:
        pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.342">def test_classRedefinedAsFunction(self):
    """
    If a class is redefined as a function, a warning is emitted.
    """
    self.flakes('''
    class Foo:
        pass
    def Foo():
        pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.343">def test_classWithReturn(self):
    """
    If a return is used inside a class, a warning is emitted.
    """
    self.flakes('''
    class Foo(object):
        return
    ''', m.ReturnOutsideFunction)
</t>
<t tx="ekr.20250430053637.344">def test_moduleWithReturn(self):
    """
    If a return is used at the module level, a warning is emitted.
    """
    self.flakes('''
    return
    ''', m.ReturnOutsideFunction)
</t>
<t tx="ekr.20250430053637.345">def test_classWithYield(self):
    """
    If a yield is used inside a class, a warning is emitted.
    """
    self.flakes('''
    class Foo(object):
        yield
    ''', m.YieldOutsideFunction)
</t>
<t tx="ekr.20250430053637.346">def test_moduleWithYield(self):
    """
    If a yield is used at the module level, a warning is emitted.
    """
    self.flakes('''
    yield
    ''', m.YieldOutsideFunction)
</t>
<t tx="ekr.20250430053637.347">def test_classWithYieldFrom(self):
    """
    If a yield from is used inside a class, a warning is emitted.
    """
    self.flakes('''
    class Foo(object):
        yield from range(10)
    ''', m.YieldOutsideFunction)
</t>
<t tx="ekr.20250430053637.348">def test_moduleWithYieldFrom(self):
    """
    If a yield from is used at the module level, a warning is emitted.
    """
    self.flakes('''
    yield from range(10)
    ''', m.YieldOutsideFunction)
</t>
<t tx="ekr.20250430053637.349">def test_continueOutsideLoop(self):
    self.flakes('''
    continue
    ''', m.ContinueOutsideLoop)

    self.flakes('''
    def f():
        continue
    ''', m.ContinueOutsideLoop)

    self.flakes('''
    while True:
        pass
    else:
        continue
    ''', m.ContinueOutsideLoop)

    self.flakes('''
    while True:
        pass
    else:
        if 1:
            if 2:
                continue
    ''', m.ContinueOutsideLoop)

    self.flakes('''
    while True:
        def f():
            continue
    ''', m.ContinueOutsideLoop)

    self.flakes('''
    while True:
        class A:
            continue
    ''', m.ContinueOutsideLoop)
</t>
<t tx="ekr.20250430053637.35">    @contextlib.contextmanager
    def makeTempFile(self, content):
        """
        Make a temporary file containing C{content} and return a path to it.
        """
        fd, name = tempfile.mkstemp()
        try:
            with os.fdopen(fd, 'wb') as f:
                if not hasattr(content, 'decode'):
                    content = content.encode('ascii')
                f.write(content)
            yield name
        finally:
            os.remove(name)
</t>
<t tx="ekr.20250430053637.350">def test_continueInsideLoop(self):
    self.flakes('''
    while True:
        continue
    ''')

    self.flakes('''
    for i in range(10):
        continue
    ''')

    self.flakes('''
    while True:
        if 1:
            continue
    ''')

    self.flakes('''
    for i in range(10):
        if 1:
            continue
    ''')

    self.flakes('''
    while True:
        while True:
            pass
        else:
            continue
    else:
        pass
    ''')

    self.flakes('''
    while True:
        try:
            pass
        finally:
            while True:
                continue
    ''')
</t>
<t tx="ekr.20250430053637.351">def test_breakOutsideLoop(self):
    self.flakes('''
    break
    ''', m.BreakOutsideLoop)

    self.flakes('''
    def f():
        break
    ''', m.BreakOutsideLoop)

    self.flakes('''
    while True:
        pass
    else:
        break
    ''', m.BreakOutsideLoop)

    self.flakes('''
    while True:
        pass
    else:
        if 1:
            if 2:
                break
    ''', m.BreakOutsideLoop)

    self.flakes('''
    while True:
        def f():
            break
    ''', m.BreakOutsideLoop)

    self.flakes('''
    while True:
        class A:
            break
    ''', m.BreakOutsideLoop)

    self.flakes('''
    try:
        pass
    finally:
        break
    ''', m.BreakOutsideLoop)
</t>
<t tx="ekr.20250430053637.352">def test_breakInsideLoop(self):
    self.flakes('''
    while True:
        break
    ''')

    self.flakes('''
    for i in range(10):
        break
    ''')

    self.flakes('''
    while True:
        if 1:
            break
    ''')

    self.flakes('''
    for i in range(10):
        if 1:
            break
    ''')

    self.flakes('''
    while True:
        while True:
            pass
        else:
            break
    else:
        pass
    ''')

    self.flakes('''
    while True:
        try:
            pass
        finally:
            while True:
                break
    ''')

    self.flakes('''
    while True:
        try:
            pass
        finally:
            break
    ''')

    self.flakes('''
    while True:
        try:
            pass
        finally:
            if 1:
                if 2:
                    break
    ''')
</t>
<t tx="ekr.20250430053637.353">def test_defaultExceptLast(self):
    """
    A default except block should be last.

    YES:

    try:
        ...
    except Exception:
        ...
    except:
        ...

    NO:

    try:
        ...
    except:
        ...
    except Exception:
        ...
    """
    self.flakes('''
    try:
        pass
    except ValueError:
        pass
    ''')

    self.flakes('''
    try:
        pass
    except ValueError:
        pass
    except:
        pass
    ''')

    self.flakes('''
    try:
        pass
    except:
        pass
    ''')

    self.flakes('''
    try:
        pass
    except ValueError:
        pass
    else:
        pass
    ''')

    self.flakes('''
    try:
        pass
    except:
        pass
    else:
        pass
    ''')

    self.flakes('''
    try:
        pass
    except ValueError:
        pass
    except:
        pass
    else:
        pass
    ''')
</t>
<t tx="ekr.20250430053637.354">def test_defaultExceptNotLast(self):
    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    except:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    except:
        pass
    except ValueError:
        pass
    ''', m.DefaultExceptNotLast, m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    else:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except:
        pass
    else:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    except:
        pass
    else:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    except:
        pass
    except ValueError:
        pass
    else:
        pass
    ''', m.DefaultExceptNotLast, m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    finally:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except:
        pass
    finally:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    except:
        pass
    finally:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    except:
        pass
    except ValueError:
        pass
    finally:
        pass
    ''', m.DefaultExceptNotLast, m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    else:
        pass
    finally:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except:
        pass
    else:
        pass
    finally:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    except:
        pass
    else:
        pass
    finally:
        pass
    ''', m.DefaultExceptNotLast)

    self.flakes('''
    try:
        pass
    except:
        pass
    except ValueError:
        pass
    except:
        pass
    except ValueError:
        pass
    else:
        pass
    finally:
        pass
    ''', m.DefaultExceptNotLast, m.DefaultExceptNotLast)
</t>
<t tx="ekr.20250430053637.355">def test_starredAssignmentNoError(self):
    """
    Python 3 extended iterable unpacking
    """
    self.flakes('''
    a, *b = range(10)
    ''')

    self.flakes('''
    *a, b = range(10)
    ''')

    self.flakes('''
    a, *b, c = range(10)
    ''')

    self.flakes('''
    (a, *b) = range(10)
    ''')

    self.flakes('''
    (*a, b) = range(10)
    ''')

    self.flakes('''
    (a, *b, c) = range(10)
    ''')

    self.flakes('''
    [a, *b] = range(10)
    ''')

    self.flakes('''
    [*a, b] = range(10)
    ''')

    self.flakes('''
    [a, *b, c] = range(10)
    ''')

    # Taken from test_unpack_ex.py in the cPython source
    s = ", ".join("a%d" % i for i in range(1 &lt;&lt; 8 - 1)) + \
        ", *rest = range(1&lt;&lt;8)"
    self.flakes(s)

    s = "(" + ", ".join("a%d" % i for i in range(1 &lt;&lt; 8 - 1)) + \
        ", *rest) = range(1&lt;&lt;8)"
    self.flakes(s)

    s = "[" + ", ".join("a%d" % i for i in range(1 &lt;&lt; 8 - 1)) + \
        ", *rest] = range(1&lt;&lt;8)"
    self.flakes(s)
</t>
<t tx="ekr.20250430053637.356">def test_starredAssignmentErrors(self):
    """
    SyntaxErrors (not encoded in the ast) surrounding Python 3 extended
    iterable unpacking
    """
    # Taken from test_unpack_ex.py in the cPython source
    s = ", ".join("a%d" % i for i in range(1 &lt;&lt; 8)) + \
        ", *rest = range(1&lt;&lt;8 + 1)"
    self.flakes(s, m.TooManyExpressionsInStarredAssignment)

    s = "(" + ", ".join("a%d" % i for i in range(1 &lt;&lt; 8)) + \
        ", *rest) = range(1&lt;&lt;8 + 1)"
    self.flakes(s, m.TooManyExpressionsInStarredAssignment)

    s = "[" + ", ".join("a%d" % i for i in range(1 &lt;&lt; 8)) + \
        ", *rest] = range(1&lt;&lt;8 + 1)"
    self.flakes(s, m.TooManyExpressionsInStarredAssignment)

    s = ", ".join("a%d" % i for i in range(1 &lt;&lt; 8 + 1)) + \
        ", *rest = range(1&lt;&lt;8 + 2)"
    self.flakes(s, m.TooManyExpressionsInStarredAssignment)

    s = "(" + ", ".join("a%d" % i for i in range(1 &lt;&lt; 8 + 1)) + \
        ", *rest) = range(1&lt;&lt;8 + 2)"
    self.flakes(s, m.TooManyExpressionsInStarredAssignment)

    s = "[" + ", ".join("a%d" % i for i in range(1 &lt;&lt; 8 + 1)) + \
        ", *rest] = range(1&lt;&lt;8 + 2)"
    self.flakes(s, m.TooManyExpressionsInStarredAssignment)

    # No way we can actually test this!
    # s = "*rest, " + ", ".join("a%d" % i for i in range(1&lt;&lt;24)) + \
    #    ", *rest = range(1&lt;&lt;24 + 1)"
    # self.flakes(s, m.TooManyExpressionsInStarredAssignment)

    self.flakes('''
    a, *b, *c = range(10)
    ''', m.TwoStarredExpressions)

    self.flakes('''
    a, *b, c, *d = range(10)
    ''', m.TwoStarredExpressions)

    self.flakes('''
    *a, *b, *c = range(10)
    ''', m.TwoStarredExpressions)

    self.flakes('''
    (a, *b, *c) = range(10)
    ''', m.TwoStarredExpressions)

    self.flakes('''
    (a, *b, c, *d) = range(10)
    ''', m.TwoStarredExpressions)

    self.flakes('''
    (*a, *b, *c) = range(10)
    ''', m.TwoStarredExpressions)

    self.flakes('''
    [a, *b, *c] = range(10)
    ''', m.TwoStarredExpressions)

    self.flakes('''
    [a, *b, c, *d] = range(10)
    ''', m.TwoStarredExpressions)

    self.flakes('''
    [*a, *b, *c] = range(10)
    ''', m.TwoStarredExpressions)
</t>
<t tx="ekr.20250430053637.357">@skip("todo: Too hard to make this warn but other cases stay silent")
def test_doubleAssignment(self):
    """
    If a variable is re-assigned to without being used, no warning is
    emitted.
    """
    self.flakes('''
    x = 10
    x = 20
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.358">def test_doubleAssignmentConditionally(self):
    """
    If a variable is re-assigned within a conditional, no warning is
    emitted.
    """
    self.flakes('''
    x = 10
    if True:
        x = 20
    ''')
</t>
<t tx="ekr.20250430053637.359">def test_doubleAssignmentWithUse(self):
    """
    If a variable is re-assigned to after being used, no warning is
    emitted.
    """
    self.flakes('''
    x = 10
    y = x * 2
    x = 20
    ''')
</t>
<t tx="ekr.20250430053637.36">    def assertHasErrors(self, path, errorList):
        """
        Assert that C{path} causes errors.

        @param path: A path to a file to check.
        @param errorList: A list of errors expected to be printed to stderr.
        """
        err = io.StringIO()
        count = withStderrTo(err, checkPath, path)
        self.assertEqual(
            (count, err.getvalue()), (len(errorList), ''.join(errorList)))
</t>
<t tx="ekr.20250430053637.360">def test_comparison(self):
    """
    If a defined name is used on either side of any of the six comparison
    operators, no warning is emitted.
    """
    self.flakes('''
    x = 10
    y = 20
    x &lt; y
    x &lt;= y
    x == y
    x != y
    x &gt;= y
    x &gt; y
    ''')
</t>
<t tx="ekr.20250430053637.361">def test_identity(self):
    """
    If a defined name is used on either side of an identity test, no
    warning is emitted.
    """
    self.flakes('''
    x = 10
    y = 20
    x is y
    x is not y
    ''')
</t>
<t tx="ekr.20250430053637.362">def test_containment(self):
    """
    If a defined name is used on either side of a containment test, no
    warning is emitted.
    """
    self.flakes('''
    x = 10
    y = 20
    x in y
    x not in y
    ''')
</t>
<t tx="ekr.20250430053637.363">def test_loopControl(self):
    """
    break and continue statements are supported.
    """
    self.flakes('''
    for x in [1, 2]:
        break
    ''')
    self.flakes('''
    for x in [1, 2]:
        continue
    ''')
</t>
<t tx="ekr.20250430053637.364">def test_ellipsis(self):
    """
    Ellipsis in a slice is supported.
    """
    self.flakes('''
    [1, 2][...]
    ''')
</t>
<t tx="ekr.20250430053637.365">def test_extendedSlice(self):
    """
    Extended slices are supported.
    """
    self.flakes('''
    x = 3
    [1, 2][x,:]
    ''')
</t>
<t tx="ekr.20250430053637.366">def test_varAugmentedAssignment(self):
    """
    Augmented assignment of a variable is supported.
    We don't care about var refs.
    """
    self.flakes('''
    foo = 0
    foo += 1
    ''')
</t>
<t tx="ekr.20250430053637.367">def test_attrAugmentedAssignment(self):
    """
    Augmented assignment of attributes is supported.
    We don't care about attr refs.
    """
    self.flakes('''
    foo = None
    foo.bar += foo.baz
    ''')
</t>
<t tx="ekr.20250430053637.368">def test_globalDeclaredInDifferentScope(self):
    """
    A 'global' can be declared in one scope and reused in another.
    """
    self.flakes('''
    def f(): global foo
    def g(): foo = 'anything'; foo.is_used()
    ''')
</t>
<t tx="ekr.20250430053637.369">def test_function_arguments(self):
    """
    Test to traverse ARG and ARGUMENT handler
    """
    self.flakes('''
    def foo(a, b):
        pass
    ''')

    self.flakes('''
    def foo(a, b, c=0):
        pass
    ''')

    self.flakes('''
    def foo(a, b, c=0, *args):
        pass
    ''')

    self.flakes('''
    def foo(a, b, c=0, *args, **kwargs):
        pass
    ''')
</t>
<t tx="ekr.20250430053637.37">    def getErrors(self, path):
        """
        Get any warnings or errors reported by pyflakes for the file at C{path}.

        @param path: The path to a Python file on disk that pyflakes will check.
        @return: C{(count, log)}, where C{count} is the number of warnings or
            errors generated, and log is a list of those warnings, presented
            as structured data.  See L{LoggingReporter} for more details.
        """
        log = []
        reporter = LoggingReporter(log)
        count = checkPath(path, reporter)
        return count, log
</t>
<t tx="ekr.20250430053637.370">def test_function_arguments_python3(self):
    self.flakes('''
    def foo(a, b, c=0, *args, d=0, **kwargs):
        pass
    ''')
</t>
<t tx="ekr.20250430053637.371">def test_unusedVariable(self):
    """
    Warn when a variable in a function is assigned a value that's never
    used.
    """
    self.flakes('''
    def a():
        b = 1
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.372">def test_unusedUnderscoreVariable(self):
    """
    Don't warn when the magic "_" (underscore) variable is unused.
    See issue #202.
    """
    self.flakes('''
    def a(unused_param):
        _ = unused_param
    ''')
</t>
<t tx="ekr.20250430053637.373">def test_unusedVariableAsLocals(self):
    """
    Using locals() it is perfectly valid to have unused variables
    """
    self.flakes('''
    def a():
        b = 1
        return locals()
    ''')
</t>
<t tx="ekr.20250430053637.374">def test_unusedVariableNoLocals(self):
    """
    Using locals() in wrong scope should not matter
    """
    self.flakes('''
    def a():
        locals()
        def a():
            b = 1
            return
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.375">@skip("todo: Difficult because it doesn't apply in the context of a loop")
def test_unusedReassignedVariable(self):
    """
    Shadowing a used variable can still raise an UnusedVariable warning.
    """
    self.flakes('''
    def a():
        b = 1
        b.foo()
        b = 2
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.376">def test_variableUsedInLoop(self):
    """
    Shadowing a used variable cannot raise an UnusedVariable warning in the
    context of a loop.
    """
    self.flakes('''
    def a():
        b = True
        while b:
            b = False
    ''')
</t>
<t tx="ekr.20250430053637.377">def test_assignToGlobal(self):
    """
    Assigning to a global and then not using that global is perfectly
    acceptable. Do not mistake it for an unused local variable.
    """
    self.flakes('''
    b = 0
    def a():
        global b
        b = 1
    ''')
</t>
<t tx="ekr.20250430053637.378">def test_assignToNonlocal(self):
    """
    Assigning to a nonlocal and then not using that binding is perfectly
    acceptable. Do not mistake it for an unused local variable.
    """
    self.flakes('''
    b = b'0'
    def a():
        nonlocal b
        b = b'1'
    ''')
</t>
<t tx="ekr.20250430053637.379">def test_assignToMember(self):
    """
    Assigning to a member of another object and then not using that member
    variable is perfectly acceptable. Do not mistake it for an unused
    local variable.
    """
    # XXX: Adding this test didn't generate a failure. Maybe not
    # necessary?
    self.flakes('''
    class b:
        pass
    def a():
        b.foo = 1
    ''')
</t>
<t tx="ekr.20250430053637.38">    def test_legacyScript(self):
        from pyflakes.scripts import pyflakes as script_pyflakes
        self.assertIs(script_pyflakes.checkPath, checkPath)
</t>
<t tx="ekr.20250430053637.380">def test_assignInForLoop(self):
    """
    Don't warn when a variable in a for loop is assigned to but not used.
    """
    self.flakes('''
    def f():
        for i in range(10):
            pass
    ''')
</t>
<t tx="ekr.20250430053637.381">def test_assignInListComprehension(self):
    """
    Don't warn when a variable in a list comprehension is
    assigned to but not used.
    """
    self.flakes('''
    def f():
        [None for i in range(10)]
    ''')
</t>
<t tx="ekr.20250430053637.382">def test_generatorExpression(self):
    """
    Don't warn when a variable in a generator expression is
    assigned to but not used.
    """
    self.flakes('''
    def f():
        (None for i in range(10))
    ''')
</t>
<t tx="ekr.20250430053637.383">def test_assignmentInsideLoop(self):
    """
    Don't warn when a variable assignment occurs lexically after its use.
    """
    self.flakes('''
    def f():
        x = None
        for i in range(10):
            if i &gt; 2:
                return x
            x = i * 2
    ''')
</t>
<t tx="ekr.20250430053637.384">def test_tupleUnpacking(self):
    """
    Don't warn when a variable included in tuple unpacking is unused. It's
    very common for variables in a tuple unpacking assignment to be unused
    in good Python code, so warning will only create false positives.
    """
    self.flakes('''
    def f(tup):
        (x, y) = tup
    ''')
    self.flakes('''
    def f():
        (x, y) = 1, 2
    ''', m.UnusedVariable, m.UnusedVariable)
    self.flakes('''
    def f():
        (x, y) = coords = 1, 2
        if x &gt; 1:
            print(coords)
    ''')
    self.flakes('''
    def f():
        (x, y) = coords = 1, 2
    ''', m.UnusedVariable)
    self.flakes('''
    def f():
        coords = (x, y) = 1, 2
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.385">def test_listUnpacking(self):
    """
    Don't warn when a variable included in list unpacking is unused.
    """
    self.flakes('''
    def f(tup):
        [x, y] = tup
    ''')
    self.flakes('''
    def f():
        [x, y] = [1, 2]
    ''', m.UnusedVariable, m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.386">def test_closedOver(self):
    """
    Don't warn when the assignment is used in an inner function.
    """
    self.flakes('''
    def barMaker():
        foo = 5
        def bar():
            return foo
        return bar
    ''')
</t>
<t tx="ekr.20250430053637.387">def test_doubleClosedOver(self):
    """
    Don't warn when the assignment is used in an inner function, even if
    that inner function itself is in an inner function.
    """
    self.flakes('''
    def barMaker():
        foo = 5
        def bar():
            def baz():
                return foo
        return bar
    ''')
</t>
<t tx="ekr.20250430053637.388">def test_tracebackhideSpecialVariable(self):
    """
    Do not warn about unused local variable __tracebackhide__, which is
    a special variable for py.test.
    """
    self.flakes("""
        def helper():
            __tracebackhide__ = True
    """)
</t>
<t tx="ekr.20250430053637.389">def test_ifexp(self):
    """
    Test C{foo if bar else baz} statements.
    """
    self.flakes("a = 'moo' if True else 'oink'")
    self.flakes("a = foo if True else 'oink'", m.UndefinedName)
    self.flakes("a = 'moo' if True else bar", m.UndefinedName)
</t>
<t tx="ekr.20250430053637.39">    def test_missingTrailingNewline(self):
        """
        Source which doesn't end with a newline shouldn't cause any
        exception to be raised nor an error indicator to be returned by
        L{check}.
        """
        with self.makeTempFile("def foo():\n\tpass\n\t") as fName:
            self.assertHasErrors(fName, [])
</t>
<t tx="ekr.20250430053637.390">def test_if_tuple(self):
    """
    Test C{if (foo,)} conditions.
    """
    self.flakes("""if (): pass""")
    self.flakes("""
    if (
        True
    ):
        pass
    """)
    self.flakes("""
    if (
        True,
    ):
        pass
    """, m.IfTuple)
    self.flakes("""
    x = 1 if (
        True,
    ) else 2
    """, m.IfTuple)
</t>
<t tx="ekr.20250430053637.391">def test_withStatementNoNames(self):
    """
    No warnings are emitted for using inside or after a nameless C{with}
    statement a name defined beforehand.
    """
    self.flakes('''
    bar = None
    with open("foo"):
        bar
    bar
    ''')
</t>
<t tx="ekr.20250430053637.392">def test_withStatementSingleName(self):
    """
    No warnings are emitted for using a name defined by a C{with} statement
    within the suite or afterwards.
    """
    self.flakes('''
    with open('foo') as bar:
        bar
    bar
    ''')
</t>
<t tx="ekr.20250430053637.393">def test_withStatementAttributeName(self):
    """
    No warnings are emitted for using an attribute as the target of a
    C{with} statement.
    """
    self.flakes('''
    import foo
    with open('foo') as foo.bar:
        pass
    ''')
</t>
<t tx="ekr.20250430053637.394">def test_withStatementSubscript(self):
    """
    No warnings are emitted for using a subscript as the target of a
    C{with} statement.
    """
    self.flakes('''
    import foo
    with open('foo') as foo[0]:
        pass
    ''')
</t>
<t tx="ekr.20250430053637.395">def test_withStatementSubscriptUndefined(self):
    """
    An undefined name warning is emitted if the subscript used as the
    target of a C{with} statement is not defined.
    """
    self.flakes('''
    import foo
    with open('foo') as foo[bar]:
        pass
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.396">def test_withStatementTupleNames(self):
    """
    No warnings are emitted for using any of the tuple of names defined by
    a C{with} statement within the suite or afterwards.
    """
    self.flakes('''
    with open('foo') as (bar, baz):
        bar, baz
    bar, baz
    ''')
</t>
<t tx="ekr.20250430053637.397">def test_withStatementListNames(self):
    """
    No warnings are emitted for using any of the list of names defined by a
    C{with} statement within the suite or afterwards.
    """
    self.flakes('''
    with open('foo') as [bar, baz]:
        bar, baz
    bar, baz
    ''')
</t>
<t tx="ekr.20250430053637.398">def test_withStatementComplicatedTarget(self):
    """
    If the target of a C{with} statement uses any or all of the valid forms
    for that part of the grammar (See
    U{http://docs.python.org/reference/compound_stmts.html#the-with-statement}),
    the names involved are checked both for definedness and any bindings
    created are respected in the suite of the statement and afterwards.
    """
    self.flakes('''
    c = d = e = g = h = i = None
    with open('foo') as [(a, b), c[d], e.f, g[h:i]]:
        a, b, c, d, e, g, h, i
    a, b, c, d, e, g, h, i
    ''')
</t>
<t tx="ekr.20250430053637.399">def test_withStatementSingleNameUndefined(self):
    """
    An undefined name warning is emitted if the name first defined by a
    C{with} statement is used before the C{with} statement.
    """
    self.flakes('''
    bar
    with open('foo') as bar:
        pass
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.4">class SysStreamCapturing:
    """Context manager capturing sys.stdin, sys.stdout and sys.stderr.
    
    The file handles are replaced with a StringIO object.
    """
    @others
</t>
<t tx="ekr.20250430053637.40">    def test_checkPathNonExisting(self):
        """
        L{checkPath} handles non-existing files.
        """
        count, errors = self.getErrors('extremo')
        self.assertEqual(count, 1)
        self.assertEqual(
            errors,
            [('unexpectedError', 'extremo', 'No such file or directory')])
</t>
<t tx="ekr.20250430053637.400">def test_withStatementTupleNamesUndefined(self):
    """
    An undefined name warning is emitted if a name first defined by the
    tuple-unpacking form of the C{with} statement is used before the
    C{with} statement.
    """
    self.flakes('''
    baz
    with open('foo') as (bar, baz):
        pass
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.401">def test_withStatementSingleNameRedefined(self):
    """
    A redefined name warning is emitted if a name bound by an import is
    rebound by the name defined by a C{with} statement.
    """
    self.flakes('''
    import bar
    with open('foo') as bar:
        pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.402">def test_withStatementTupleNamesRedefined(self):
    """
    A redefined name warning is emitted if a name bound by an import is
    rebound by one of the names defined by the tuple-unpacking form of a
    C{with} statement.
    """
    self.flakes('''
    import bar
    with open('foo') as (bar, baz):
        pass
    ''', m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.403">def test_withStatementUndefinedInside(self):
    """
    An undefined name warning is emitted if a name is used inside the
    body of a C{with} statement without first being bound.
    """
    self.flakes('''
    with open('foo') as bar:
        baz
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.404">def test_withStatementNameDefinedInBody(self):
    """
    A name defined in the body of a C{with} statement can be used after
    the body ends without warning.
    """
    self.flakes('''
    with open('foo') as bar:
        baz = 10
    baz
    ''')
</t>
<t tx="ekr.20250430053637.405">def test_withStatementUndefinedInExpression(self):
    """
    An undefined name warning is emitted if a name in the I{test}
    expression of a C{with} statement is undefined.
    """
    self.flakes('''
    with bar as baz:
        pass
    ''', m.UndefinedName)

    self.flakes('''
    with bar as bar:
        pass
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.406">def test_dictComprehension(self):
    """
    Dict comprehensions are properly handled.
    """
    self.flakes('''
    a = {1: x for x in range(10)}
    ''')
</t>
<t tx="ekr.20250430053637.407">def test_setComprehensionAndLiteral(self):
    """
    Set comprehensions are properly handled.
    """
    self.flakes('''
    a = {1, 2, 3}
    b = {x for x in range(10)}
    ''')
</t>
<t tx="ekr.20250430053637.408">def test_exceptionUsedInExcept(self):
    self.flakes('''
    try: pass
    except Exception as e: e
    ''')

    self.flakes('''
    def download_review():
        try: pass
        except Exception as e: e
    ''')
</t>
<t tx="ekr.20250430053637.409">def test_exceptionUnusedInExcept(self):
    self.flakes('''
    try: pass
    except Exception as e: pass
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.41">    def test_multilineSyntaxError(self):
        """
        Source which includes a syntax error which results in the raised
        L{SyntaxError.text} containing multiple lines of source are reported
        with only the last line of that source.
        """
        source = """\
def foo():
    '''

def bar():
    pass

def baz():
    '''quux'''
"""

        # Sanity check - SyntaxError.text should be multiple lines, if it
        # isn't, something this test was unprepared for has happened.
        def evaluate(source):
            exec(source)
        try:
            evaluate(source)
        except SyntaxError as e:
            if not PYPY and sys.version_info &lt; (3, 10):
                self.assertTrue(e.text.count('\n') &gt; 1)
        else:
            self.fail()

        with self.makeTempFile(source) as sourcePath:
            if PYPY:
                message = 'end of file (EOF) while scanning triple-quoted string literal'
            elif sys.version_info &gt;= (3, 10):
                message = 'unterminated triple-quoted string literal (detected at line 8)'  # noqa: E501
            else:
                message = 'invalid syntax'

            if PYPY or sys.version_info &gt;= (3, 10):
                column = 12
            else:
                column = 8
            self.assertHasErrors(
                sourcePath,
                ["""\
%s:8:%d: %s
    '''quux'''
%s^
""" % (sourcePath, column, message, ' ' * (column - 1))])
</t>
<t tx="ekr.20250430053637.410">@skipIf(version_info &lt; (3, 11), 'new in Python 3.11')
def test_exception_unused_in_except_star(self):
    self.flakes('''
        try:
            pass
        except* OSError as e:
            pass
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.411">def test_exceptionUnusedInExceptInFunction(self):
    self.flakes('''
    def download_review():
        try: pass
        except Exception as e: pass
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.412">def test_exceptWithoutNameInFunction(self):
    """
    Don't issue false warning when an unnamed exception is used.
    Previously, there would be a false warning, but only when the
    try..except was in a function
    """
    self.flakes('''
    import tokenize
    def foo():
        try: pass
        except tokenize.TokenError: pass
    ''')
</t>
<t tx="ekr.20250430053637.413">def test_exceptWithoutNameInFunctionTuple(self):
    """
    Don't issue false warning when an unnamed exception is used.
    This example catches a tuple of exception types.
    """
    self.flakes('''
    import tokenize
    def foo():
        try: pass
        except (tokenize.TokenError, IndentationError): pass
    ''')
</t>
<t tx="ekr.20250430053637.414">def test_augmentedAssignmentImportedFunctionCall(self):
    """
    Consider a function that is called on the right part of an
    augassign operation to be used.
    """
    self.flakes('''
    from foo import bar
    baz = 0
    baz += bar()
    ''')
</t>
<t tx="ekr.20250430053637.415">def test_assert_without_message(self):
    """An assert without a message is not an error."""
    self.flakes('''
    a = 1
    assert a
    ''')
</t>
<t tx="ekr.20250430053637.416">def test_assert_with_message(self):
    """An assert with a message is not an error."""
    self.flakes('''
    a = 1
    assert a, 'x'
    ''')
</t>
<t tx="ekr.20250430053637.417">def test_assert_tuple(self):
    """An assert of a non-empty tuple is always True."""
    self.flakes('''
    assert (False, 'x')
    assert (False, )
    ''', m.AssertTuple, m.AssertTuple)
</t>
<t tx="ekr.20250430053637.418">def test_assert_tuple_empty(self):
    """An assert of an empty tuple is always False."""
    self.flakes('''
    assert ()
    ''')
</t>
<t tx="ekr.20250430053637.419">def test_assert_static(self):
    """An assert of a static value is not an error."""
    self.flakes('''
    assert True
    assert 1
    ''')
</t>
<t tx="ekr.20250430053637.42">    def test_eofSyntaxError(self):
        """
        The error reported for source files which end prematurely causing a
        syntax error reflects the cause for the syntax error.
        """
        with self.makeTempFile("def foo(") as sourcePath:
            if PYPY:
                msg = 'parenthesis is never closed'
            elif sys.version_info &gt;= (3, 10):
                msg = "'(' was never closed"
            else:
                msg = 'unexpected EOF while parsing'

            if PYPY or sys.version_info &gt;= (3, 10):
                column = 8
            else:
                column = 9

            spaces = ' ' * (column - 1)
            expected = '{}:1:{}: {}\ndef foo(\n{}^\n'.format(
                sourcePath, column, msg, spaces
            )

            self.assertHasErrors(sourcePath, [expected])
</t>
<t tx="ekr.20250430053637.420">def test_yieldFromUndefined(self):
    """
    Test C{yield from} statement
    """
    self.flakes('''
    def bar():
        yield from foo()
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.421">def test_f_string(self):
    """Test PEP 498 f-strings are treated as a usage."""
    self.flakes('''
    baz = 0
    print(f'\x7b4*baz\N{RIGHT CURLY BRACKET}')
    ''')
</t>
<t tx="ekr.20250430053637.422">def test_assign_expr(self):
    """Test PEP 572 assignment expressions are treated as usage / write."""
    self.flakes('''
    from foo import y
    print(x := y)
    print(x)
    ''')
</t>
<t tx="ekr.20250430053637.423">def test_assign_expr_generator_scope(self):
    """Test assignment expressions in generator expressions."""
    self.flakes('''
    if (any((y := x[0]) for x in [[True]])):
        print(y)
    ''')
</t>
<t tx="ekr.20250430053637.424">def test_assign_expr_nested(self):
    """Test assignment expressions in nested expressions."""
    self.flakes('''
    if ([(y:=x) for x in range(4) if [(z:=q) for q in range(4)]]):
        print(y)
        print(z)
    ''')
</t>
<t tx="ekr.20250430053637.425">def test_f_string_without_placeholders(self):
    self.flakes("f'foo'", m.FStringMissingPlaceholders)
    self.flakes('''
        f"""foo
        bar
        """
    ''', m.FStringMissingPlaceholders)
    self.flakes('''
        print(
            f'foo'
            f'bar'
        )
    ''', m.FStringMissingPlaceholders)
    # this is an "escaped placeholder" but not a placeholder
    self.flakes("f'{{}}'", m.FStringMissingPlaceholders)
    # ok: f-string with placeholders
    self.flakes('''
        x = 5
        print(f'{x}')
    ''')
    # ok: f-string with format specifiers
    self.flakes('''
        x = 'a' * 90
        print(f'{x:.8}')
    ''')
    # ok: f-string with multiple format specifiers
    self.flakes('''
        x = y = 5
        print(f'{x:&gt;2} {y:&gt;2}')
    ''')
</t>
<t tx="ekr.20250430053637.426">def test_invalid_dot_format_calls(self):
    self.flakes('''
        '{'.format(1)
    ''', m.StringDotFormatInvalidFormat)
    self.flakes('''
        '{} {1}'.format(1, 2)
    ''', m.StringDotFormatMixingAutomatic)
    self.flakes('''
        '{0} {}'.format(1, 2)
    ''', m.StringDotFormatMixingAutomatic)
    self.flakes('''
        '{}'.format(1, 2)
    ''', m.StringDotFormatExtraPositionalArguments)
    self.flakes('''
        '{}'.format(1, bar=2)
    ''', m.StringDotFormatExtraNamedArguments)
    self.flakes('''
        '{} {}'.format(1)
    ''', m.StringDotFormatMissingArgument)
    self.flakes('''
        '{2}'.format()
    ''', m.StringDotFormatMissingArgument)
    self.flakes('''
        '{bar}'.format()
    ''', m.StringDotFormatMissingArgument)
    # too much string recursion (placeholder-in-placeholder)
    self.flakes('''
        '{:{:{}}}'.format(1, 2, 3)
    ''', m.StringDotFormatInvalidFormat)
    # ok: dotted / bracketed names need to handle the param differently
    self.flakes("'{.__class__}'.format('')")
    self.flakes("'{foo[bar]}'.format(foo={'bar': 'barv'})")
    # ok: placeholder-placeholders
    self.flakes('''
        print('{:{}} {}'.format(1, 15, 2))
    ''')
    # ok: not a placeholder-placeholder
    self.flakes('''
        print('{:2}'.format(1))
    ''')
    # ok: not mixed automatic
    self.flakes('''
        '{foo}-{}'.format(1, foo=2)
    ''')
    # ok: we can't determine statically the format args
    self.flakes('''
        a = ()
        "{}".format(*a)
    ''')
    self.flakes('''
        k = {}
        "{foo}".format(**k)
    ''')
</t>
<t tx="ekr.20250430053637.427">def test_invalid_percent_format_calls(self):
    self.flakes('''
        '%(foo)' % {'foo': 'bar'}
    ''', m.PercentFormatInvalidFormat)
    self.flakes('''
        '%s %(foo)s' % {'foo': 'bar'}
    ''', m.PercentFormatMixedPositionalAndNamed)
    self.flakes('''
        '%(foo)s %s' % {'foo': 'bar'}
    ''', m.PercentFormatMixedPositionalAndNamed)
    self.flakes('''
        '%j' % (1,)
    ''', m.PercentFormatUnsupportedFormatCharacter)
    self.flakes('''
        '%s %s' % (1,)
    ''', m.PercentFormatPositionalCountMismatch)
    self.flakes('''
        '%s %s' % (1, 2, 3)
    ''', m.PercentFormatPositionalCountMismatch)
    self.flakes('''
        '%(bar)s' % {}
    ''', m.PercentFormatMissingArgument,)
    self.flakes('''
        '%(bar)s' % {'bar': 1, 'baz': 2}
    ''', m.PercentFormatExtraNamedArguments)
    self.flakes('''
        '%(bar)s' % (1, 2, 3)
    ''', m.PercentFormatExpectedMapping)
    self.flakes('''
        '%s %s' % {'k': 'v'}
    ''', m.PercentFormatExpectedSequence)
    self.flakes('''
        '%(bar)*s' % {'bar': 'baz'}
    ''', m.PercentFormatStarRequiresSequence)
    # ok: single %s with mapping
    self.flakes('''
        '%s' % {'foo': 'bar', 'baz': 'womp'}
    ''')
    # ok: does not cause a MemoryError (the strings aren't evaluated)
    self.flakes('''
        "%1000000000000f" % 1
    ''')
    # ok: %% should not count towards placeholder count
    self.flakes('''
        '%% %s %% %s' % (1, 2)
    ''')
    # ok: * consumes one positional argument
    self.flakes('''
        '%.*f' % (2, 1.1234)
        '%*.*f' % (5, 2, 3.1234)
    ''')
</t>
<t tx="ekr.20250430053637.428">def test_ok_percent_format_cannot_determine_element_count(self):
    self.flakes('''
        a = []
        '%s %s' % [*a]
        '%s %s' % (*a,)
    ''')
    self.flakes('''
        k = {}
        '%(k)s' % {**k}
    ''')
</t>
<t tx="ekr.20250430053637.429">def test_asyncDef(self):
    self.flakes('''
    async def bar():
        return 42
    ''')
</t>
<t tx="ekr.20250430053637.43">    def test_eofSyntaxErrorWithTab(self):
        """
        The error reported for source files which end prematurely causing a
        syntax error reflects the cause for the syntax error.
        """
        with self.makeTempFile("if True:\n\tfoo =") as sourcePath:
            self.assertHasErrors(
                sourcePath,
                [f"""\
{sourcePath}:2:7: invalid syntax
\tfoo =
\t     ^
"""])
</t>
<t tx="ekr.20250430053637.430">def test_asyncDefAwait(self):
    self.flakes('''
    async def read_data(db):
        await db.fetch('SELECT ...')
    ''')
</t>
<t tx="ekr.20250430053637.431">def test_asyncDefUndefined(self):
    self.flakes('''
    async def bar():
        return foo()
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.432">def test_asyncFor(self):
    self.flakes('''
    async def read_data(db):
        output = []
        async for row in db.cursor():
            output.append(row)
        return output
    ''')
</t>
<t tx="ekr.20250430053637.433">def test_asyncForUnderscoreLoopVar(self):
    self.flakes('''
    async def coro(it):
        async for _ in it:
            pass
    ''')
</t>
<t tx="ekr.20250430053637.434">def test_loopControlInAsyncFor(self):
    self.flakes('''
    async def read_data(db):
        output = []
        async for row in db.cursor():
            if row[0] == 'skip':
                continue
            output.append(row)
        return output
    ''')

    self.flakes('''
    async def read_data(db):
        output = []
        async for row in db.cursor():
            if row[0] == 'stop':
                break
            output.append(row)
        return output
    ''')
</t>
<t tx="ekr.20250430053637.435">def test_loopControlInAsyncForElse(self):
    self.flakes('''
    async def read_data(db):
        output = []
        async for row in db.cursor():
            output.append(row)
        else:
            continue
        return output
    ''', m.ContinueOutsideLoop)

    self.flakes('''
    async def read_data(db):
        output = []
        async for row in db.cursor():
            output.append(row)
        else:
            break
        return output
    ''', m.BreakOutsideLoop)
</t>
<t tx="ekr.20250430053637.436">def test_asyncWith(self):
    self.flakes('''
    async def commit(session, data):
        async with session.transaction():
            await session.update(data)
    ''')
</t>
<t tx="ekr.20250430053637.437">def test_asyncWithItem(self):
    self.flakes('''
    async def commit(session, data):
        async with session.transaction() as trans:
            await trans.begin()
            ...
            await trans.end()
    ''')
</t>
<t tx="ekr.20250430053637.438">def test_matmul(self):
    self.flakes('''
    def foo(a, b):
        return a @ b
    ''')
</t>
<t tx="ekr.20250430053637.439">def test_formatstring(self):
    self.flakes('''
    hi = 'hi'
    mom = 'mom'
    f'{hi} {mom}'
    ''')
</t>
<t tx="ekr.20250430053637.44">    def test_nonDefaultFollowsDefaultSyntaxError(self):
        """
        Source which has a non-default argument following a default argument
        should include the line number of the syntax error.  However these
        exceptions do not include an offset.
        """
        source = """\
def foo(bar=baz, bax):
    pass
"""
        with self.makeTempFile(source) as sourcePath:
            if sys.version_info &gt;= (3, 12):
                msg = 'parameter without a default follows parameter with a default'  # noqa: E501
            else:
                msg = 'non-default argument follows default argument'

            if PYPY and sys.version_info &gt;= (3, 9):
                column = 18
            elif PYPY:
                column = 8
            elif sys.version_info &gt;= (3, 10):
                column = 18
            elif sys.version_info &gt;= (3, 9):
                column = 21
            else:
                column = 9
            last_line = ' ' * (column - 1) + '^\n'
            self.assertHasErrors(
                sourcePath,
                [f"""\
{sourcePath}:1:{column}: {msg}
def foo(bar=baz, bax):
{last_line}"""]
            )
</t>
<t tx="ekr.20250430053637.440">def test_raise_notimplemented(self):
    self.flakes('''
    raise NotImplementedError("This is fine")
    ''')

    self.flakes('''
    raise NotImplementedError
    ''')

    self.flakes('''
    raise NotImplemented("This isn't gonna work")
    ''', m.RaiseNotImplemented)

    self.flakes('''
    raise NotImplemented
    ''', m.RaiseNotImplemented)
</t>
<t tx="ekr.20250430053637.441">def test_valid_print(self):
    self.flakes('''
    print("Hello")
    ''')
</t>
<t tx="ekr.20250430053637.442">def test_invalid_print_when_imported_from_future(self):
    exc = self.flakes('''
    from __future__ import print_function
    import sys
    print &gt;&gt;sys.stderr, "Hello"
    ''', m.InvalidPrintSyntax).messages[0]

    self.assertEqual(exc.lineno, 4)
    self.assertEqual(exc.col, 0)
</t>
<t tx="ekr.20250430053637.443">def test_print_augmented_assign(self):
    # nonsense, but shouldn't crash pyflakes
    self.flakes('print += 1')
</t>
<t tx="ekr.20250430053637.444">def test_print_function_assignment(self):
    """
    A valid assignment, tested for catching false positives.
    """
    self.flakes('''
    from __future__ import print_function
    log = print
    log("Hello")
    ''')
</t>
<t tx="ekr.20250430053637.445">def test_print_in_lambda(self):
    self.flakes('''
    from __future__ import print_function
    a = lambda: print
    ''')
</t>
<t tx="ekr.20250430053637.446">def test_print_returned_in_function(self):
    self.flakes('''
    from __future__ import print_function
    def a():
        return print
    ''')
</t>
<t tx="ekr.20250430053637.447">def test_print_as_condition_test(self):
    self.flakes('''
    from __future__ import print_function
    if print: pass
    ''')
</t>
<t tx="ekr.20250430053637.448">"""
Tests for behaviour related to type annotations.
"""

from sys import version_info

from pyflakes import messages as m
from pyflakes.test.harness import TestCase, skipIf


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.449">class TestTypeAnnotations(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.45">    def test_nonKeywordAfterKeywordSyntaxError(self):
        """
        Source which has a non-keyword argument after a keyword argument should
        include the line number of the syntax error.  However these exceptions
        do not include an offset.
        """
        source = """\
foo(bar=baz, bax)
"""
        with self.makeTempFile(source) as sourcePath:
            if sys.version_info &gt;= (3, 9):
                column = 17
            elif not PYPY:
                column = 14
            else:
                column = 13
            last_line = ' ' * (column - 1) + '^\n'
            columnstr = '%d:' % column

            message = 'positional argument follows keyword argument'

            self.assertHasErrors(
                sourcePath,
                ["""\
{}:1:{} {}
foo(bar=baz, bax)
{}""".format(sourcePath, columnstr, message, last_line)])
</t>
<t tx="ekr.20250430053637.450">def test_typingOverload(self):
    """Allow intentional redefinitions via @typing.overload"""
    self.flakes("""
    import typing
    from typing import overload

    @overload
    def f(s: None) -&gt; None:
        pass

    @overload
    def f(s: int) -&gt; int:
        pass

    def f(s):
        return s

    @typing.overload
    def g(s: None) -&gt; None:
        pass

    @typing.overload
    def g(s: int) -&gt; int:
        pass

    def g(s):
        return s
    """)
</t>
<t tx="ekr.20250430053637.451">def test_typingExtensionsOverload(self):
    """Allow intentional redefinitions via @typing_extensions.overload"""
    self.flakes("""
    import typing_extensions
    from typing_extensions import overload

    @overload
    def f(s: None) -&gt; None:
        pass

    @overload
    def f(s: int) -&gt; int:
        pass

    def f(s):
        return s

    @typing_extensions.overload
    def g(s: None) -&gt; None:
        pass

    @typing_extensions.overload
    def g(s: int) -&gt; int:
        pass

    def g(s):
        return s
    """)
</t>
<t tx="ekr.20250430053637.452">def test_typingOverloadAsync(self):
    """Allow intentional redefinitions via @typing.overload (async)"""
    self.flakes("""
    from typing import overload

    @overload
    async def f(s: None) -&gt; None:
        pass

    @overload
    async def f(s: int) -&gt; int:
        pass

    async def f(s):
        return s
    """)
</t>
<t tx="ekr.20250430053637.453">def test_overload_with_multiple_decorators(self):
    self.flakes("""
        from typing import overload
        dec = lambda f: f

        @dec
        @overload
        def f(x: int) -&gt; int:
            pass

        @dec
        @overload
        def f(x: str) -&gt; str:
            pass

        @dec
        def f(x): return x
   """)
</t>
<t tx="ekr.20250430053637.454">def test_overload_in_class(self):
    self.flakes("""
    from typing import overload

    class C:
        @overload
        def f(self, x: int) -&gt; int:
            pass

        @overload
        def f(self, x: str) -&gt; str:
            pass

        def f(self, x): return x
    """)
</t>
<t tx="ekr.20250430053637.455">def test_aliased_import(self):
    """Detect when typing is imported as another name"""
    self.flakes("""
    import typing as t

    @t.overload
    def f(s: None) -&gt; None:
        pass

    @t.overload
    def f(s: int) -&gt; int:
        pass

    def f(s):
        return s
    """)
</t>
<t tx="ekr.20250430053637.456">def test_not_a_typing_overload(self):
    """regression test for @typing.overload detection bug in 2.1.0"""
    self.flakes("""
        def foo(x):
            return x

        @foo
        def bar():
            pass

        def bar():
            pass
    """, m.RedefinedWhileUnused)
</t>
<t tx="ekr.20250430053637.457">def test_variable_annotations(self):
    self.flakes('''
    name: str
    age: int
    ''')
    self.flakes('''
    name: str = 'Bob'
    age: int = 18
    ''')
    self.flakes('''
    class C:
        name: str
        age: int
    ''')
    self.flakes('''
    class C:
        name: str = 'Bob'
        age: int = 18
    ''')
    self.flakes('''
    def f():
        name: str
        age: int
    ''', m.UnusedAnnotation, m.UnusedAnnotation)
    self.flakes('''
    def f():
        name: str = 'Bob'
        age: int = 18
        foo: not_a_real_type = None
    ''', m.UnusedVariable, m.UnusedVariable, m.UnusedVariable, m.UndefinedName)
    self.flakes('''
    def f():
        name: str
        print(name)
    ''', m.UndefinedName)
    self.flakes('''
    from typing import Any
    def f():
        a: Any
    ''', m.UnusedAnnotation)
    self.flakes('''
    foo: not_a_real_type
    ''', m.UndefinedName)
    self.flakes('''
    foo: not_a_real_type = None
    ''', m.UndefinedName)
    self.flakes('''
    class C:
        foo: not_a_real_type
    ''', m.UndefinedName)
    self.flakes('''
    class C:
        foo: not_a_real_type = None
    ''', m.UndefinedName)
    self.flakes('''
    def f():
        class C:
            foo: not_a_real_type
    ''', m.UndefinedName)
    self.flakes('''
    def f():
        class C:
            foo: not_a_real_type = None
    ''', m.UndefinedName)
    self.flakes('''
    from foo import Bar
    bar: Bar
    ''')
    self.flakes('''
    from foo import Bar
    bar: 'Bar'
    ''')
    self.flakes('''
    import foo
    bar: foo.Bar
    ''')
    self.flakes('''
    import foo
    bar: 'foo.Bar'
    ''')
    self.flakes('''
    from foo import Bar
    def f(bar: Bar): pass
    ''')
    self.flakes('''
    from foo import Bar
    def f(bar: 'Bar'): pass
    ''')
    self.flakes('''
    from foo import Bar
    def f(bar) -&gt; Bar: return bar
    ''')
    self.flakes('''
    from foo import Bar
    def f(bar) -&gt; 'Bar': return bar
    ''')
    self.flakes('''
    bar: 'Bar'
    ''', m.UndefinedName)
    self.flakes('''
    bar: 'foo.Bar'
    ''', m.UndefinedName)
    self.flakes('''
    from foo import Bar
    bar: str
    ''', m.UnusedImport)
    self.flakes('''
    from foo import Bar
    def f(bar: str): pass
    ''', m.UnusedImport)
    self.flakes('''
    def f(a: A) -&gt; A: pass
    class A: pass
    ''', m.UndefinedName, m.UndefinedName)
    self.flakes('''
    def f(a: 'A') -&gt; 'A': return a
    class A: pass
    ''')
    self.flakes('''
    a: A
    class A: pass
    ''', m.UndefinedName)
    self.flakes('''
    a: 'A'
    class A: pass
    ''')
    self.flakes('''
    T: object
    def f(t: T): pass
    ''', m.UndefinedName)
    self.flakes('''
    T: object
    def g(t: 'T'): pass
    ''')
    self.flakes('''
    a: 'A B'
    ''', m.ForwardAnnotationSyntaxError)
    self.flakes('''
    a: 'A; B'
    ''', m.ForwardAnnotationSyntaxError)
    self.flakes('''
    a: '1 + 2'
    ''')
    self.flakes('''
    a: 'a: "A"'
    ''', m.ForwardAnnotationSyntaxError)
</t>
<t tx="ekr.20250430053637.458">def test_variable_annotation_references_self_name_undefined(self):
    self.flakes("""
    x: int = x
    """, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.459">def test_TypeAlias_annotations(self):
    self.flakes("""
    from typing_extensions import TypeAlias
    from foo import Bar

    bar: TypeAlias = Bar
    """)
    self.flakes("""
    from typing_extensions import TypeAlias
    from foo import Bar

    bar: TypeAlias = 'Bar'
    """)
    self.flakes("""
    from typing_extensions import TypeAlias
    from foo import Bar

    class A:
        bar: TypeAlias = Bar
    """)
    self.flakes("""
    from typing_extensions import TypeAlias
    from foo import Bar

    class A:
        bar: TypeAlias = 'Bar'
    """)
    self.flakes("""
    from typing_extensions import TypeAlias

    bar: TypeAlias
    """)
    self.flakes("""
    from typing_extensions import TypeAlias
    from foo import Bar

    bar: TypeAlias
    """, m.UnusedImport)
</t>
<t tx="ekr.20250430053637.46">    def test_invalidEscape(self):
        """
        The invalid escape syntax raises ValueError in Python 2
        """
        # ValueError: invalid \x escape
        with self.makeTempFile(r"foo = '\xyz'") as sourcePath:
            position_end = 1
            if PYPY and sys.version_info &gt;= (3, 9):
                column = 7
            elif PYPY:
                column = 6
            elif (3, 9) &lt;= sys.version_info &lt; (3, 12):
                column = 13
            else:
                column = 7

            last_line = '%s^\n' % (' ' * (column - 1))

            decoding_error = """\
%s:1:%d: (unicode error) 'unicodeescape' codec can't decode bytes \
in position 0-%d: truncated \\xXX escape
foo = '\\xyz'
</t>
<t tx="ekr.20250430053637.460">def test_annotating_an_import(self):
    self.flakes('''
        from a import b, c
        b: c
        print(b)
    ''')
</t>
<t tx="ekr.20250430053637.461">def test_unused_annotation(self):
    # Unused annotations are fine in module and class scope
    self.flakes('''
    x: int
    class Cls:
        y: int
    ''')
    self.flakes('''
    def f():
        x: int
    ''', m.UnusedAnnotation)
    # This should only print one UnusedVariable message
    self.flakes('''
    def f():
        x: int
        x = 3
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.462">def test_unused_annotation_in_outer_scope_reassigned_in_local_scope(self):
    self.flakes('''
    x: int
    x.__dict__
    def f(): x = 1
    ''', m.UndefinedName, m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.463">def test_unassigned_annotation_is_undefined(self):
    self.flakes('''
    name: str
    print(name)
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.464">def test_annotated_async_def(self):
    self.flakes('''
    class c: pass
    async def func(c: c) -&gt; None: pass
    ''')
</t>
<t tx="ekr.20250430053637.465">def test_postponed_annotations(self):
    self.flakes('''
    from __future__ import annotations
    def f(a: A) -&gt; A: pass
    class A:
        b: B
    class B: pass
    ''')

    self.flakes('''
    from __future__ import annotations
    def f(a: A) -&gt; A: pass
    class A:
        b: Undefined
    class B: pass
    ''', m.UndefinedName)

    self.flakes('''
    from __future__ import annotations
    T: object
    def f(t: T): pass
    def g(t: 'T'): pass
    ''')
</t>
<t tx="ekr.20250430053637.466">def test_type_annotation_clobbers_all(self):
    self.flakes('''\
    from typing import TYPE_CHECKING, List

    from y import z

    if not TYPE_CHECKING:
        __all__ = ("z",)
    else:
        __all__: List[str]
    ''')
</t>
<t tx="ekr.20250430053637.467">def test_return_annotation_is_class_scope_variable(self):
    self.flakes("""
    from typing import TypeVar
    class Test:
        Y = TypeVar('Y')

        def t(self, x: Y) -&gt; Y:
            return x
    """)
</t>
<t tx="ekr.20250430053637.468">def test_return_annotation_is_function_body_variable(self):
    self.flakes("""
    class Test:
        def t(self) -&gt; Y:
            Y = 2
            return Y
    """, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.469">def test_positional_only_argument_annotations(self):
    self.flakes("""
    from x import C

    def f(c: C, /): ...
    """)
</t>
<t tx="ekr.20250430053637.47">%s""" % (sourcePath, column, position_end, last_line)

            self.assertHasErrors(
                sourcePath, [decoding_error])

    @skipIf(sys.platform == 'win32', 'unsupported on Windows')
    def test_permissionDenied(self):
        """
        If the source file is not readable, this is reported on standard
        error.
        """
        if os.getuid() == 0:
            self.skipTest('root user can access all files regardless of '
                          'permissions')
        with self.makeTempFile('') as sourcePath:
            os.chmod(sourcePath, 0)
            count, errors = self.getErrors(sourcePath)
            self.assertEqual(count, 1)
            self.assertEqual(
                errors,
                [('unexpectedError', sourcePath, "Permission denied")])
</t>
<t tx="ekr.20250430053637.470">def test_partially_quoted_type_annotation(self):
    self.flakes("""
    from queue import Queue
    from typing import Optional

    def f() -&gt; Optional['Queue[str]']:
        return None
    """)
</t>
<t tx="ekr.20250430053637.471">def test_partially_quoted_type_assignment(self):
    self.flakes("""
    from queue import Queue
    from typing import Optional

    MaybeQueue = Optional['Queue[str]']
    """)
</t>
<t tx="ekr.20250430053637.472">def test_nested_partially_quoted_type_assignment(self):
    self.flakes("""
    from queue import Queue
    from typing import Callable

    Func = Callable[['Queue[str]'], None]
    """)
</t>
<t tx="ekr.20250430053637.473">def test_quoted_type_cast(self):
    self.flakes("""
    from typing import cast, Optional

    maybe_int = cast('Optional[int]', 42)
    """)
</t>
<t tx="ekr.20250430053637.474">def test_type_cast_literal_str_to_str(self):
    # Checks that our handling of quoted type annotations in the first
    # argument to `cast` doesn't cause issues when (only) the _second_
    # argument is a literal str which looks a bit like a type annotation.
    self.flakes("""
    from typing import cast

    a_string = cast(str, 'Optional[int]')
    """)
</t>
<t tx="ekr.20250430053637.475">def test_quoted_type_cast_renamed_import(self):
    self.flakes("""
    from typing import cast as tsac, Optional as Maybe

    maybe_int = tsac('Maybe[int]', 42)
    """)
</t>
<t tx="ekr.20250430053637.476">def test_quoted_TypeVar_constraints(self):
    self.flakes("""
    from typing import TypeVar, Optional

    T = TypeVar('T', 'str', 'Optional[int]', bytes)
    """)
</t>
<t tx="ekr.20250430053637.477">def test_quoted_TypeVar_bound(self):
    self.flakes("""
    from typing import TypeVar, Optional, List

    T = TypeVar('T', bound='Optional[int]')
    S = TypeVar('S', int, bound='List[int]')
    """)
</t>
<t tx="ekr.20250430053637.478">def test_literal_type_typing(self):
    self.flakes("""
    from typing import Literal

    def f(x: Literal['some string']) -&gt; None:
        return None
    """)
</t>
<t tx="ekr.20250430053637.479">def test_literal_type_typing_extensions(self):
    self.flakes("""
    from typing_extensions import Literal

    def f(x: Literal['some string']) -&gt; None:
        return None
    """)
</t>
<t tx="ekr.20250430053637.48">    def test_pyflakesWarning(self):
        """
        If the source file has a pyflakes warning, this is reported as a
        'flake'.
        """
        with self.makeTempFile("import foo") as sourcePath:
            count, errors = self.getErrors(sourcePath)
            self.assertEqual(count, 1)
            self.assertEqual(
                errors, [('flake', str(UnusedImport(sourcePath, Node(1), 'foo')))])
</t>
<t tx="ekr.20250430053637.480">def test_annotated_type_typing_missing_forward_type(self):
    self.flakes("""
    from typing import Annotated

    def f(x: Annotated['integer']) -&gt; None:
        return None
    """, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.481">def test_annotated_type_typing_missing_forward_type_multiple_args(self):
    self.flakes("""
    from typing import Annotated

    def f(x: Annotated['integer', 1]) -&gt; None:
        return None
    """, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.482">def test_annotated_type_typing_with_string_args(self):
    self.flakes("""
    from typing import Annotated

    def f(x: Annotated[int, '&gt; 0']) -&gt; None:
        return None
    """)
</t>
<t tx="ekr.20250430053637.483">def test_annotated_type_typing_with_string_args_in_union(self):
    self.flakes("""
    from typing import Annotated, Union

    def f(x: Union[Annotated['int', '&gt;0'], 'integer']) -&gt; None:
        return None
    """, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.484">def test_literal_type_some_other_module(self):
    """err on the side of false-negatives for types named Literal"""
    self.flakes("""
    from my_module import compat
    from my_module.compat import Literal

    def f(x: compat.Literal['some string']) -&gt; None:
        return None
    def g(x: Literal['some string']) -&gt; None:
        return None
    """)
</t>
<t tx="ekr.20250430053637.485">def test_literal_union_type_typing(self):
    self.flakes("""
    from typing import Literal

    def f(x: Literal['some string', 'foo bar']) -&gt; None:
        return None
    """)
</t>
<t tx="ekr.20250430053637.486">def test_deferred_twice_annotation(self):
    self.flakes("""
        from queue import Queue
        from typing import Optional


        def f() -&gt; "Optional['Queue[str]']":
            return None
    """)
</t>
<t tx="ekr.20250430053637.487">def test_partial_string_annotations_with_future_annotations(self):
    self.flakes("""
        from __future__ import annotations

        from queue import Queue
        from typing import Optional


        def f() -&gt; Optional['Queue[str]']:
            return None
    """)
</t>
<t tx="ekr.20250430053637.488">def test_forward_annotations_for_classes_in_scope(self):
    # see #749
    self.flakes("""
    from typing import Optional

    def f():
        class C:
            a: "D"
            b: Optional["D"]
            c: "Optional[D]"

        class D: pass
    """)
</t>
<t tx="ekr.20250430053637.489">def test_idomiatic_typing_guards(self):
    # typing.TYPE_CHECKING: python3.5.3+
    self.flakes("""
        from typing import TYPE_CHECKING

        if TYPE_CHECKING:
            from t import T

        def f() -&gt; T:
            pass
    """)
    # False: the old, more-compatible approach
    self.flakes("""
        if False:
            from t import T

        def f() -&gt; T:
            pass
    """)
    # some choose to assign a constant and do it that way
    self.flakes("""
        MYPY = False

        if MYPY:
            from t import T

        def f() -&gt; T:
            pass
    """)
</t>
<t tx="ekr.20250430053637.49">    def test_encodedFileUTF8(self):
        """
        If source file declares the correct encoding, no error is reported.
        """
        SNOWMAN = chr(0x2603)
        source = ("""\
</t>
<t tx="ekr.20250430053637.490">def test_typing_guard_for_protocol(self):
    self.flakes("""
        from typing import TYPE_CHECKING

        if TYPE_CHECKING:
            from typing import Protocol
        else:
            Protocol = object

        class C(Protocol):
            def f() -&gt; int:
                pass
    """)
</t>
<t tx="ekr.20250430053637.491">def test_typednames_correct_forward_ref(self):
    self.flakes("""
        from typing import TypedDict, List, NamedTuple

        List[TypedDict("x", {})]
        List[TypedDict("x", x=int)]
        List[NamedTuple("a", a=int)]
        List[NamedTuple("a", [("a", int)])]
    """)
    self.flakes("""
        from typing import TypedDict, List, NamedTuple, TypeVar

        List[TypedDict("x", {"x": "Y"})]
        List[TypedDict("x", x="Y")]
        List[NamedTuple("a", [("a", "Y")])]
        List[NamedTuple("a", a="Y")]
        List[TypedDict("x", {"x": List["a"]})]
        List[TypeVar("A", bound="C")]
        List[TypeVar("A", List["C"])]
    """, *[m.UndefinedName]*7)
    self.flakes("""
        from typing import NamedTuple, TypeVar, cast
        from t import A, B, C, D, E

        NamedTuple("A", [("a", A["C"])])
        TypeVar("A", bound=A["B"])
        TypeVar("A", A["D"])
        cast(A["E"], [])
    """)
</t>
<t tx="ekr.20250430053637.492">def test_namedtypes_classes(self):
    self.flakes("""
        from typing import TypedDict, NamedTuple
        class X(TypedDict):
            y: TypedDict("z", {"zz":int})

        class Y(NamedTuple):
            y: NamedTuple("v", [("vv", int)])
    """)
</t>
<t tx="ekr.20250430053637.493">@skipIf(version_info &lt; (3, 11), 'new in Python 3.11')
def test_variadic_generics(self):
    self.flakes("""
        from typing import Generic
        from typing import TypeVarTuple

        Ts = TypeVarTuple('Ts')

        class Shape(Generic[*Ts]): pass

        def f(*args: *Ts) -&gt; None: ...

        def g(x: Shape[*Ts]) -&gt; Shape[*Ts]: ...
    """)
</t>
<t tx="ekr.20250430053637.494">@skipIf(version_info &lt; (3, 12), 'new in Python 3.12')
def test_type_statements(self):
    self.flakes("""
        type ListOrSet[T] = list[T] | set[T]

        def f(x: ListOrSet[str]) -&gt; None: ...

        type RecursiveType = int | list[RecursiveType]

        type ForwardRef = int | C

        type ForwardRefInBounds[T: C] = T

        class C: pass
    """)
</t>
<t tx="ekr.20250430053637.495">@skipIf(version_info &lt; (3, 12), 'new in Python 3.12')
def test_type_parameters_functions(self):
    self.flakes("""
        def f[T](t: T) -&gt; T: return t

        async def g[T](t: T) -&gt; T: return t

        def with_forward_ref[T: C](t: T) -&gt; T: return t

        def can_access_inside[T](t: T) -&gt; T:
            print(T)
            return t

        class C: pass
    """)
</t>
<t tx="ekr.20250430053637.496">@skipIf(version_info &lt; (3, 12), 'new in Python 3.12')
def test_type_parameters_do_not_escape_function_scopes(self):
    self.flakes("""
        from x import g

        @g(T)  # not accessible in decorators
        def f[T](t: T) -&gt; T: return t

        T  # not accessible afterwards
    """, m.UndefinedName, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.497">@skipIf(version_info &lt; (3, 12), 'new in Python 3.12')
def test_type_parameters_classes(self):
    self.flakes("""
        class C[T](list[T]): pass

        class UsesForward[T: Forward](list[T]): pass

        class Forward: pass

        class WithinBody[T](list[T]):
            t = T
    """)
</t>
<t tx="ekr.20250430053637.498">@skipIf(version_info &lt; (3, 12), 'new in Python 3.12')
def test_type_parameters_do_not_escape_class_scopes(self):
    self.flakes("""
        from x import g

        @g(T)  # not accessible in decorators
        class C[T](list[T]): pass

        T  # not accessible afterwards
    """, m.UndefinedName, m.UndefinedName)
</t>
<t tx="ekr.20250430053637.499">@skipIf(version_info &lt; (3, 12), 'new in Python 3.12')
def test_type_parameters_TypeVarTuple(self):
    self.flakes("""
    def f[*T](*args: *T) -&gt; None: ...
    """)
</t>
<t tx="ekr.20250430053637.5">class LoggingReporter:
    """
    Implementation of Reporter that just appends any error to a list.
    """
    @others
</t>
<t tx="ekr.20250430053637.50"># coding: utf-8
x = "%s"
""" % SNOWMAN).encode('utf-8')
        with self.makeTempFile(source) as sourcePath:
            self.assertHasErrors(sourcePath, [])

    def test_CRLFLineEndings(self):
        """
        Source files with Windows CR LF line endings are parsed successfully.
        """
        with self.makeTempFile("x = 42\r\n") as sourcePath:
            self.assertHasErrors(sourcePath, [])
</t>
<t tx="ekr.20250430053637.500">@skipIf(version_info &lt; (3, 12), 'new in Python 3.12')
def test_type_parameters_ParamSpec(self):
    self.flakes("""
    from typing import Callable

    def f[R, **P](f: Callable[P, R]) -&gt; Callable[P, R]:
        def g(*args: P.args, **kwargs: P.kwargs) -&gt; R:
            return f(*args, **kwargs)
        return g
    """)
</t>
<t tx="ekr.20250430053637.501">import ast

from pyflakes import messages as m, checker
from pyflakes.test.harness import TestCase, skip


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.502">class Test(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.503">class NameTests(TestCase):
    """
    Tests for some extra cases of name handling.
    """
    @others
</t>
<t tx="ekr.20250430053637.504">def test_undefined(self):
    self.flakes('bar', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.505">def test_definedInListComp(self):
    self.flakes('[a for a in range(10) if a]')
</t>
<t tx="ekr.20250430053637.506">def test_undefinedInListComp(self):
    self.flakes('''
    [a for a in range(10)]
    a
    ''',
                m.UndefinedName)
</t>
<t tx="ekr.20250430053637.507">def test_undefinedExceptionName(self):
    """Exception names can't be used after the except: block.

    The exc variable is unused inside the exception handler."""
    self.flakes('''
    try:
        raise ValueError('ve')
    except ValueError as exc:
        pass
    exc
    ''', m.UndefinedName, m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.508">def test_namesDeclaredInExceptBlocks(self):
    """Locals declared in except: blocks can be used after the block.

    This shows the example in test_undefinedExceptionName is
    different."""
    self.flakes('''
    try:
        raise ValueError('ve')
    except ValueError as exc:
        e = exc
    e
    ''')
</t>
<t tx="ekr.20250430053637.509">@skip('error reporting disabled due to false positives below')
def test_undefinedExceptionNameObscuringLocalVariable(self):
    """Exception names obscure locals, can't be used after.

    Last line will raise UnboundLocalError on Python 3 after exiting
    the except: block. Note next two examples for false positives to
    watch out for."""
    self.flakes('''
    exc = 'Original value'
    try:
        raise ValueError('ve')
    except ValueError as exc:
        pass
    exc
    ''',
                m.UndefinedName)
</t>
<t tx="ekr.20250430053637.51">    def test_misencodedFileUTF8(self):
        """
        If a source file contains bytes which cannot be decoded, this is
        reported on stderr.
        """
        SNOWMAN = chr(0x2603)
        source = ("""\
</t>
<t tx="ekr.20250430053637.510">def test_undefinedExceptionNameObscuringLocalVariable2(self):
    """Exception names are unbound after the `except:` block.

    Last line will raise UnboundLocalError.
    The exc variable is unused inside the exception handler.
    """
    self.flakes('''
    try:
        raise ValueError('ve')
    except ValueError as exc:
        pass
    print(exc)
    exc = 'Original value'
    ''', m.UndefinedName, m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.511">def test_undefinedExceptionNameObscuringLocalVariableFalsePositive1(self):
    """Exception names obscure locals, can't be used after. Unless.

    Last line will never raise UnboundLocalError because it's only
    entered if no exception was raised."""
    self.flakes('''
    exc = 'Original value'
    try:
        raise ValueError('ve')
    except ValueError as exc:
        print('exception logged')
        raise
    exc
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.512">def test_delExceptionInExcept(self):
    """The exception name can be deleted in the except: block."""
    self.flakes('''
    try:
        pass
    except Exception as exc:
        del exc
    ''')
</t>
<t tx="ekr.20250430053637.513">def test_undefinedExceptionNameObscuringLocalVariableFalsePositive2(self):
    """Exception names obscure locals, can't be used after. Unless.

    Last line will never raise UnboundLocalError because `error` is
    only falsy if the `except:` block has not been entered."""
    self.flakes('''
    exc = 'Original value'
    error = None
    try:
        raise ValueError('ve')
    except ValueError as exc:
        error = 'exception logged'
    if error:
        print(error)
    else:
        exc
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.514">@skip('error reporting disabled due to false positives below')
def test_undefinedExceptionNameObscuringGlobalVariable(self):
    """Exception names obscure globals, can't be used after.

    Last line will raise UnboundLocalError because the existence of that
    exception name creates a local scope placeholder for it, obscuring any
    globals, etc."""
    self.flakes('''
    exc = 'Original value'
    def func():
        try:
            pass  # nothing is raised
        except ValueError as exc:
            pass  # block never entered, exc stays unbound
        exc
    ''',
                m.UndefinedLocal)
</t>
<t tx="ekr.20250430053637.515">@skip('error reporting disabled due to false positives below')
def test_undefinedExceptionNameObscuringGlobalVariable2(self):
    """Exception names obscure globals, can't be used after.

    Last line will raise NameError on Python 3 because the name is
    locally unbound after the `except:` block, even if it's
    nonlocal. We should issue an error in this case because code
    only working correctly if an exception isn't raised, is invalid.
    Unless it's explicitly silenced, see false positives below."""
    self.flakes('''
    exc = 'Original value'
    def func():
        global exc
        try:
            raise ValueError('ve')
        except ValueError as exc:
            pass  # block never entered, exc stays unbound
        exc
    ''',
                m.UndefinedLocal)
</t>
<t tx="ekr.20250430053637.516">def test_undefinedExceptionNameObscuringGlobalVariableFalsePositive1(self):
    """Exception names obscure globals, can't be used after. Unless.

    Last line will never raise NameError because it's only entered
    if no exception was raised."""
    self.flakes('''
    exc = 'Original value'
    def func():
        global exc
        try:
            raise ValueError('ve')
        except ValueError as exc:
            print('exception logged')
            raise
        exc
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.517">def test_undefinedExceptionNameObscuringGlobalVariableFalsePositive2(self):
    """Exception names obscure globals, can't be used after. Unless.

    Last line will never raise NameError because `error` is only
    falsy if the `except:` block has not been entered."""
    self.flakes('''
    exc = 'Original value'
    def func():
        global exc
        error = None
        try:
            raise ValueError('ve')
        except ValueError as exc:
            error = 'exception logged'
        if error:
            print(error)
        else:
            exc
    ''', m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.518">def test_functionsNeedGlobalScope(self):
    self.flakes('''
    class a:
        def b():
            fu
    fu = 1
    ''')
</t>
<t tx="ekr.20250430053637.519">def test_builtins(self):
    self.flakes('range(10)')
</t>
<t tx="ekr.20250430053637.52"># coding: ascii
x = "%s"
""" % SNOWMAN).encode('utf-8')
        with self.makeTempFile(source) as sourcePath:
            self.assertHasErrors(
                sourcePath,
                [f"{sourcePath}:1:1: 'ascii' codec can't decode byte 0xe2 in position 21: ordinal not in range(128)\n"])  # noqa: E501

    def test_misencodedFileUTF16(self):
        """
        If a source file contains bytes which cannot be decoded, this is
        reported on stderr.
        """
        SNOWMAN = chr(0x2603)
        source = ("""\
</t>
<t tx="ekr.20250430053637.520">def test_builtinWindowsError(self):
    """
    C{WindowsError} is sometimes a builtin name, so no warning is emitted
    for using it.
    """
    self.flakes('WindowsError')
</t>
<t tx="ekr.20250430053637.521">def test_moduleAnnotations(self):
    """
    Use of the C{__annotations__} in module scope should not emit
    an undefined name warning when version is greater than or equal to 3.6.
    """
    self.flakes('__annotations__')
</t>
<t tx="ekr.20250430053637.522">def test_magicGlobalsFile(self):
    """
    Use of the C{__file__} magic global should not emit an undefined name
    warning.
    """
    self.flakes('__file__')
</t>
<t tx="ekr.20250430053637.523">def test_magicGlobalsBuiltins(self):
    """
    Use of the C{__builtins__} magic global should not emit an undefined
    name warning.
    """
    self.flakes('__builtins__')
</t>
<t tx="ekr.20250430053637.524">def test_magicGlobalsName(self):
    """
    Use of the C{__name__} magic global should not emit an undefined name
    warning.
    """
    self.flakes('__name__')
</t>
<t tx="ekr.20250430053637.525">def test_magicGlobalsPath(self):
    """
    Use of the C{__path__} magic global should not emit an undefined name
    warning, if you refer to it from a file called __init__.py.
    """
    self.flakes('__path__', m.UndefinedName)
    self.flakes('__path__', filename='package/__init__.py')
</t>
<t tx="ekr.20250430053637.526">def test_magicModuleInClassScope(self):
    """
    Use of the C{__module__} magic builtin should not emit an undefined
    name warning if used in class scope.
    """
    self.flakes('__module__', m.UndefinedName)
    self.flakes('''
    class Foo:
        __module__
    ''')
    self.flakes('''
    class Foo:
        def bar(self):
            __module__
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.527">def test_magicQualnameInClassScope(self):
    """
    Use of the C{__qualname__} magic builtin should not emit an undefined
    name warning if used in class scope.
    """
    self.flakes('__qualname__', m.UndefinedName)
    self.flakes('''
    class Foo:
        __qualname__
    ''')
    self.flakes('''
    class Foo:
        def bar(self):
            __qualname__
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.528">def test_globalImportStar(self):
    """Can't find undefined names with import *."""
    self.flakes('from fu import *; bar',
                m.ImportStarUsed, m.ImportStarUsage)
</t>
<t tx="ekr.20250430053637.529">def test_definedByGlobal(self):
    """
    "global" can make an otherwise undefined name in another function
    defined.
    """
    self.flakes('''
    def a(): global fu; fu = 1
    def b(): fu
    ''')
    self.flakes('''
    def c(): bar
    def b(): global bar; bar = 1
    ''')
</t>
<t tx="ekr.20250430053637.53"># coding: ascii
x = "%s"
""" % SNOWMAN).encode('utf-16')
        with self.makeTempFile(source) as sourcePath:
            if sys.version_info &lt; (3, 11, 4):
                expected = f"{sourcePath}: problem decoding source\n"
            else:
                expected = f"{sourcePath}:1: source code string cannot contain null bytes\n"  # noqa: E501

            self.assertHasErrors(sourcePath, [expected])

    def test_checkRecursive(self):
        """
        L{checkRecursive} descends into each directory, finding Python files
        and reporting problems.
        """
        tempdir = tempfile.mkdtemp()
        try:
            os.mkdir(os.path.join(tempdir, 'foo'))
            file1 = os.path.join(tempdir, 'foo', 'bar.py')
            with open(file1, 'wb') as fd:
                fd.write(b"import baz\n")
            file2 = os.path.join(tempdir, 'baz.py')
            with open(file2, 'wb') as fd:
                fd.write(b"import contraband")
            log = []
            reporter = LoggingReporter(log)
            warnings = checkRecursive([tempdir], reporter)
            self.assertEqual(warnings, 2)
            self.assertEqual(
                sorted(log),
                sorted([('flake', str(UnusedImport(file1, Node(1), 'baz'))),
                        ('flake',
                         str(UnusedImport(file2, Node(1), 'contraband')))]))
        finally:
            shutil.rmtree(tempdir)
</t>
<t tx="ekr.20250430053637.530">def test_definedByGlobalMultipleNames(self):
    """
    "global" can accept multiple names.
    """
    self.flakes('''
    def a(): global fu, bar; fu = 1; bar = 2
    def b(): fu; bar
    ''')
</t>
<t tx="ekr.20250430053637.531">def test_globalInGlobalScope(self):
    """
    A global statement in the global scope is ignored.
    """
    self.flakes('''
    global x
    def foo():
        print(x)
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.532">def test_global_reset_name_only(self):
    """A global statement does not prevent other names being undefined."""
    # Only different undefined names are reported.
    # See following test that fails where the same name is used.
    self.flakes('''
    def f1():
        s

    def f2():
        global m
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.533">@skip("todo")
def test_unused_global(self):
    """An unused global statement does not define the name."""
    self.flakes('''
    def f1():
        m

    def f2():
        global m
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.534">def test_del(self):
    """Del deletes bindings."""
    self.flakes('a = 1; del a; a', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.535">def test_delGlobal(self):
    """Del a global binding from a function."""
    self.flakes('''
    a = 1
    def f():
        global a
        del a
    a
    ''')
</t>
<t tx="ekr.20250430053637.536">def test_delUndefined(self):
    """Del an undefined name."""
    self.flakes('del a', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.537">def test_delConditional(self):
    """
    Ignores conditional bindings deletion.
    """
    self.flakes('''
    context = None
    test = True
    if False:
        del(test)
    assert(test)
    ''')
</t>
<t tx="ekr.20250430053637.538">def test_delConditionalNested(self):
    """
    Ignored conditional bindings deletion even if they are nested in other
    blocks.
    """
    self.flakes('''
    context = None
    test = True
    if False:
        with context():
            del(test)
    assert(test)
    ''')
</t>
<t tx="ekr.20250430053637.539">def test_delWhile(self):
    """
    Ignore bindings deletion if called inside the body of a while
    statement.
    """
    self.flakes('''
    def test():
        foo = 'bar'
        while False:
            del foo
        assert(foo)
    ''')
</t>
<t tx="ekr.20250430053637.54">    def test_stdinReportsErrors(self):
        """
        L{check} reports syntax errors from stdin
        """
        source = "max(1 for i in range(10), key=lambda x: x+1)\n"
        err = io.StringIO()
        count = withStderrTo(err, check, source, "&lt;stdin&gt;")
        self.assertEqual(count, 1)
        errlines = err.getvalue().split("\n")[:-1]

        if sys.version_info &gt;= (3, 9):
            expected_error = [
                "&lt;stdin&gt;:1:5: Generator expression must be parenthesized",
                "max(1 for i in range(10), key=lambda x: x+1)",
                "    ^",
            ]
        elif PYPY:
            expected_error = [
                "&lt;stdin&gt;:1:4: Generator expression must be parenthesized if not sole argument",  # noqa: E501
                "max(1 for i in range(10), key=lambda x: x+1)",
                "   ^",
            ]
        else:
            expected_error = [
                "&lt;stdin&gt;:1:5: Generator expression must be parenthesized",
            ]

        self.assertEqual(errlines, expected_error)
</t>
<t tx="ekr.20250430053637.540">def test_delWhileTestUsage(self):
    """
    Ignore bindings deletion if called inside the body of a while
    statement and name is used inside while's test part.
    """
    self.flakes('''
    def _worker():
        o = True
        while o is not True:
            del o
            o = False
    ''')
</t>
<t tx="ekr.20250430053637.541">def test_delWhileNested(self):
    """
    Ignore bindings deletions if node is part of while's test, even when
    del is in a nested block.
    """
    self.flakes('''
    context = None
    def _worker():
        o = True
        while o is not True:
            while True:
                with context():
                    del o
            o = False
    ''')
</t>
<t tx="ekr.20250430053637.542">def test_globalFromNestedScope(self):
    """Global names are available from nested scopes."""
    self.flakes('''
    a = 1
    def b():
        def c():
            a
    ''')
</t>
<t tx="ekr.20250430053637.543">def test_laterRedefinedGlobalFromNestedScope(self):
    """
    Test that referencing a local name that shadows a global, before it is
    defined, generates a warning.
    """
    self.flakes('''
    a = 1
    def fun():
        a
        a = 2
        return a
    ''', m.UndefinedLocal)
</t>
<t tx="ekr.20250430053637.544">def test_laterRedefinedGlobalFromNestedScope2(self):
    """
    Test that referencing a local name in a nested scope that shadows a
    global declared in an enclosing scope, before it is defined, generates
    a warning.
    """
    self.flakes('''
        a = 1
        def fun():
            global a
            def fun2():
                a
                a = 2
                return a
    ''', m.UndefinedLocal)
</t>
<t tx="ekr.20250430053637.545">def test_intermediateClassScopeIgnored(self):
    """
    If a name defined in an enclosing scope is shadowed by a local variable
    and the name is used locally before it is bound, an unbound local
    warning is emitted, even if there is a class scope between the enclosing
    scope and the local scope.
    """
    self.flakes('''
    def f():
        x = 1
        class g:
            def h(self):
                a = x
                x = None
                print(x, a)
        print(x)
    ''', m.UndefinedLocal)
</t>
<t tx="ekr.20250430053637.546">def test_doubleNestingReportsClosestName(self):
    """
    Test that referencing a local name in a nested scope that shadows a
    variable declared in two different outer scopes before it is defined
    in the innermost scope generates an UnboundLocal warning which
    refers to the nearest shadowed name.
    """
    exc = self.flakes('''
        def a():
            x = 1
            def b():
                x = 2 # line 5
                def c():
                    x
                    x = 3
                    return x
                return x
            return x
    ''', m.UndefinedLocal).messages[0]

    # _DoctestMixin.flakes adds two lines preceding the code above.
    expected_line_num = 7 if self.withDoctest else 5

    self.assertEqual(exc.message_args, ('x', expected_line_num))
</t>
<t tx="ekr.20250430053637.547">def test_laterRedefinedGlobalFromNestedScope3(self):
    """
    Test that referencing a local name in a nested scope that shadows a
    global, before it is defined, generates a warning.
    """
    self.flakes('''
        def fun():
            a = 1
            def fun2():
                a
                a = 1
                return a
            return a
    ''', m.UndefinedLocal)
</t>
<t tx="ekr.20250430053637.548">def test_undefinedAugmentedAssignment(self):
    self.flakes(
        '''
        def f(seq):
            a = 0
            seq[a] += 1
            seq[b] /= 2
            c[0] *= 2
            a -= 3
            d += 4
            e[any] = 5
        ''',
        m.UndefinedName,    # b
        m.UndefinedName,    # c
        m.UndefinedName, m.UnusedVariable,  # d
        m.UndefinedName,    # e
    )
</t>
<t tx="ekr.20250430053637.549">def test_nestedClass(self):
    """Nested classes can access enclosing scope."""
    self.flakes('''
    def f(foo):
        class C:
            bar = foo
            def f(self):
                return foo
        return C()

    f(123).f()
    ''')
</t>
<t tx="ekr.20250430053637.55">def setUp(self):
    self.tempdir = tempfile.mkdtemp()
    self.tempfilepath = os.path.join(self.tempdir, 'temp')
</t>
<t tx="ekr.20250430053637.550">def test_badNestedClass(self):
    """Free variables in nested classes must bind at class creation."""
    self.flakes('''
    def f():
        class C:
            bar = foo
        foo = 456
        return foo
    f()
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.551">def test_definedAsStarArgs(self):
    """Star and double-star arg names are defined."""
    self.flakes('''
    def f(a, *b, **c):
        print(a, b, c)
    ''')
</t>
<t tx="ekr.20250430053637.552">def test_definedAsStarUnpack(self):
    """Star names in unpack are defined."""
    self.flakes('''
    a, *b = range(10)
    print(a, b)
    ''')
    self.flakes('''
    *a, b = range(10)
    print(a, b)
    ''')
    self.flakes('''
    a, *b, c = range(10)
    print(a, b, c)
    ''')
</t>
<t tx="ekr.20250430053637.553">def test_usedAsStarUnpack(self):
    """
    Star names in unpack are used if RHS is not a tuple/list literal.
    """
    self.flakes('''
    def f():
        a, *b = range(10)
    ''')
    self.flakes('''
    def f():
        (*a, b) = range(10)
    ''')
    self.flakes('''
    def f():
        [a, *b, c] = range(10)
    ''')
</t>
<t tx="ekr.20250430053637.554">def test_unusedAsStarUnpack(self):
    """
    Star names in unpack are unused if RHS is a tuple/list literal.
    """
    self.flakes('''
    def f():
        a, *b = any, all, 4, 2, 'un'
    ''', m.UnusedVariable, m.UnusedVariable)
    self.flakes('''
    def f():
        (*a, b) = [bool, int, float, complex]
    ''', m.UnusedVariable, m.UnusedVariable)
    self.flakes('''
    def f():
        [a, *b, c] = 9, 8, 7, 6, 5, 4
    ''', m.UnusedVariable, m.UnusedVariable, m.UnusedVariable)
</t>
<t tx="ekr.20250430053637.555">def test_keywordOnlyArgs(self):
    """Keyword-only arg names are defined."""
    self.flakes('''
    def f(*, a, b=None):
        print(a, b)
    ''')

    self.flakes('''
    import default_b
    def f(*, a, b=default_b):
        print(a, b)
    ''')
</t>
<t tx="ekr.20250430053637.556">def test_keywordOnlyArgsUndefined(self):
    """Typo in kwonly name."""
    self.flakes('''
    def f(*, a, b=default_c):
        print(a, b)
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.557">def test_annotationUndefined(self):
    """Undefined annotations."""
    self.flakes('''
    from abc import note1, note2, note3, note4, note5
    def func(a: note1, *args: note2,
             b: note3=12, **kw: note4) -&gt; note5: pass
    ''')

    self.flakes('''
    def func():
        d = e = 42
        def func(a: {1, d}) -&gt; (lambda c: e): pass
    ''')
</t>
<t tx="ekr.20250430053637.558">def test_metaClassUndefined(self):
    self.flakes('''
    from abc import ABCMeta
    class A(metaclass=ABCMeta): pass
    ''')
</t>
<t tx="ekr.20250430053637.559">def test_definedInGenExp(self):
    """
    Using the loop variable of a generator expression results in no
    warnings.
    """
    self.flakes('(a for a in [1, 2, 3] if a)')

    self.flakes('(b for b in (a for a in [1, 2, 3] if a) if b)')
</t>
<t tx="ekr.20250430053637.56">def tearDown(self):
    shutil.rmtree(self.tempdir)
</t>
<t tx="ekr.20250430053637.560">def test_undefinedInGenExpNested(self):
    """
    The loop variables of generator expressions nested together are
    not defined in the other generator.
    """
    self.flakes('(b for b in (a for a in [1, 2, 3] if b) if b)',
                m.UndefinedName)

    self.flakes('(b for b in (a for a in [1, 2, 3] if a) if a)',
                m.UndefinedName)
</t>
<t tx="ekr.20250430053637.561">def test_undefinedWithErrorHandler(self):
    """
    Some compatibility code checks explicitly for NameError.
    It should not trigger warnings.
    """
    self.flakes('''
    try:
        socket_map
    except NameError:
        socket_map = {}
    ''')
    self.flakes('''
    try:
        _memoryview.contiguous
    except (NameError, AttributeError):
        raise RuntimeError("Python &gt;= 3.3 is required")
    ''')
    # If NameError is not explicitly handled, generate a warning
    self.flakes('''
    try:
        socket_map
    except:
        socket_map = {}
    ''', m.UndefinedName)
    self.flakes('''
    try:
        socket_map
    except Exception:
        socket_map = {}
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.562">def test_definedInClass(self):
    """
    Defined name for generator expressions and dict/set comprehension.
    """
    self.flakes('''
    class A:
        T = range(10)

        Z = (x for x in T)
        L = [x for x in T]
        B = dict((i, str(i)) for i in T)
    ''')

    self.flakes('''
    class A:
        T = range(10)

        X = {x for x in T}
        Y = {x:x for x in T}
    ''')
</t>
<t tx="ekr.20250430053637.563">def test_definedInClassNested(self):
    """Defined name for nested generator expressions in a class."""
    self.flakes('''
    class A:
        T = range(10)

        Z = (x for x in (a for a in T))
    ''')
</t>
<t tx="ekr.20250430053637.564">def test_undefinedInLoop(self):
    """
    The loop variable is defined after the expression is computed.
    """
    self.flakes('''
    for i in range(i):
        print(i)
    ''', m.UndefinedName)
    self.flakes('''
    [42 for i in range(i)]
    ''', m.UndefinedName)
    self.flakes('''
    (42 for i in range(i))
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.565">def test_definedFromLambdaInDictionaryComprehension(self):
    """
    Defined name referenced from a lambda function within a dict/set
    comprehension.
    """
    self.flakes('''
    {lambda: id(x) for x in range(10)}
    ''')
</t>
<t tx="ekr.20250430053637.566">def test_definedFromLambdaInGenerator(self):
    """
    Defined name referenced from a lambda function within a generator
    expression.
    """
    self.flakes('''
    any(lambda: id(x) for x in range(10))
    ''')
</t>
<t tx="ekr.20250430053637.567">def test_undefinedFromLambdaInDictionaryComprehension(self):
    """
    Undefined name referenced from a lambda function within a dict/set
    comprehension.
    """
    self.flakes('''
    {lambda: id(y) for x in range(10)}
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.568">def test_undefinedFromLambdaInComprehension(self):
    """
    Undefined name referenced from a lambda function within a generator
    expression.
    """
    self.flakes('''
    any(lambda: id(y) for x in range(10))
    ''', m.UndefinedName)
</t>
<t tx="ekr.20250430053637.569">def test_dunderClass(self):
    code = '''
    class Test(object):
        def __init__(self):
            print(__class__.__name__)
            self.x = 1

    t = Test()
    '''
    self.flakes(code)
</t>
<t tx="ekr.20250430053637.57">def getPyflakesBinary(self):
    """
    Return the path to the pyflakes binary.
    """
    import pyflakes
    package_dir = os.path.dirname(pyflakes.__file__)
    return os.path.join(package_dir, '..', 'bin', 'pyflakes')
</t>
<t tx="ekr.20250430053637.570">def test_impossibleContext(self):
    """
    A Name node with an unrecognized context results in a RuntimeError being
    raised.
    """
    tree = ast.parse("x = 10")
    # Make it into something unrecognizable.
    tree.body[0].targets[0].ctx = object()
    self.assertRaises(RuntimeError, checker.Checker, tree)
</t>
<t tx="ekr.20250430053637.58">def runPyflakes(self, paths, stdin=None):
    """
    Launch a subprocess running C{pyflakes}.

    @param paths: Command-line arguments to pass to pyflakes.
    @param stdin: Text to use as stdin.
    @return: C{(returncode, stdout, stderr)} of the completed pyflakes
        process.
    """
    env = dict(os.environ)
    env['PYTHONPATH'] = os.pathsep.join(sys.path)
    command = [sys.executable, self.getPyflakesBinary()]
    command.extend(paths)
    if stdin:
        p = subprocess.Popen(command, env=env, stdin=subprocess.PIPE,
                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        (stdout, stderr) = p.communicate(stdin.encode('ascii'))
    else:
        p = subprocess.Popen(command, env=env,
                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        (stdout, stderr) = p.communicate()
    rv = p.wait()
    stdout = stdout.decode('utf-8')
    stderr = stderr.decode('utf-8')
    return (stdout, stderr, rv)
</t>
<t tx="ekr.20250430053637.59">def test_goodFile(self):
    """
    When a Python source file is all good, the return code is zero and no
    messages are printed to either stdout or stderr.
    """
    open(self.tempfilepath, 'a').close()
    d = self.runPyflakes([self.tempfilepath])
    self.assertEqual(d, ('', '', 0))
</t>
<t tx="ekr.20250430053637.6">class TestIterSourceCode(TestCase):
    """
    Tests for L{iterSourceCode}.
    """
    @others
</t>
<t tx="ekr.20250430053637.60">def test_fileWithFlakes(self):
    """
    When a Python source file has warnings, the return code is non-zero
    and the warnings are printed to stdout.
    """
    with open(self.tempfilepath, 'wb') as fd:
        fd.write(b"import contraband\n")
    d = self.runPyflakes([self.tempfilepath])
    expected = UnusedImport(self.tempfilepath, Node(1), 'contraband')
    self.assertEqual(d, (f"{expected}{os.linesep}", '', 1))
</t>
<t tx="ekr.20250430053637.61">def test_errors_io(self):
    """
    When pyflakes finds errors with the files it's given, (if they don't
    exist, say), then the return code is non-zero and the errors are
    printed to stderr.
    """
    d = self.runPyflakes([self.tempfilepath])
    error_msg = '{}: No such file or directory{}'.format(self.tempfilepath,
                                                         os.linesep)
    self.assertEqual(d, ('', error_msg, 1))
</t>
<t tx="ekr.20250430053637.62">def test_errors_syntax(self):
    """
    When pyflakes finds errors with the files it's given, (if they don't
    exist, say), then the return code is non-zero and the errors are
    printed to stderr.
    """
    with open(self.tempfilepath, 'wb') as fd:
        fd.write(b"import")
    d = self.runPyflakes([self.tempfilepath])
    error_msg = '{0}:1:7: invalid syntax{1}import{1}      ^{1}'.format(
        self.tempfilepath, os.linesep)
    self.assertEqual(d, ('', error_msg, 1))
</t>
<t tx="ekr.20250430053637.63">def test_readFromStdin(self):
    """
    If no arguments are passed to C{pyflakes} then it reads from stdin.
    """
    d = self.runPyflakes([], stdin='import contraband')
    expected = UnusedImport('&lt;stdin&gt;', Node(1), 'contraband')
    self.assertEqual(d, (f"{expected}{os.linesep}", '', 1))
</t>
<t tx="ekr.20250430053637.64">def runPyflakes(self, paths, stdin=None):
    try:
        with SysStreamCapturing(stdin) as capture:
            main(args=paths)
    except SystemExit as e:
        self.assertIsInstance(e.code, bool)
        rv = int(e.code)
        return (capture.output, capture.error, rv)
    else:
        raise RuntimeError('SystemExit not raised')
</t>
<t tx="ekr.20250430053637.65">"""
Tests for detecting redefinition of builtins.
"""
from pyflakes import messages as m
from pyflakes.test.harness import TestCase


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.66">class TestBuiltins(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.67">def test_builtin_unbound_local(self):
    self.flakes('''
    def foo():
        a = range(1, 10)
        range = a
        return range

    foo()

    print(range)
    ''', m.UndefinedLocal)
</t>
<t tx="ekr.20250430053637.68">def test_global_shadowing_builtin(self):
    self.flakes('''
    def f():
        global range
        range = None
        print(range)

    f()
    ''')
</t>
<t tx="ekr.20250430053637.69">from pyflakes import messages as m
from pyflakes.checker import (FunctionScope, ClassScope, ModuleScope,
                              Argument, FunctionDefinition, Assignment)
from pyflakes.test.harness import TestCase


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.7">class TestReporter(TestCase):
    """
    Tests for L{Reporter}.
    """
    @others
</t>
<t tx="ekr.20250430053637.70">class TestCodeSegments(TestCase):
    """
    Tests for segments of a module
    """
    @others
</t>
<t tx="ekr.20250430053637.71">def test_function_segment(self):
    self.flakes('''
    def foo():
        def bar():
            pass
    ''', is_segment=True)

    self.flakes('''
    def foo():
        def bar():
            x = 0
    ''', m.UnusedVariable, is_segment=True)
</t>
<t tx="ekr.20250430053637.72">def test_class_segment(self):
    self.flakes('''
    class Foo:
        class Bar:
            pass
    ''', is_segment=True)

    self.flakes('''
    class Foo:
        def bar():
            x = 0
    ''', m.UnusedVariable, is_segment=True)
</t>
<t tx="ekr.20250430053637.73">def test_scope_class(self):
    checker = self.flakes('''
    class Foo:
        x = 0
        def bar(a, b=1, *d, **e):
            pass
    ''', is_segment=True)

    scopes = checker.deadScopes
    module_scopes = [
        scope for scope in scopes if scope.__class__ is ModuleScope]
    class_scopes = [
        scope for scope in scopes if scope.__class__ is ClassScope]
    function_scopes = [
        scope for scope in scopes if scope.__class__ is FunctionScope]

    # Ensure module scope is not present because we are analysing
    # the inner contents of Foo
    self.assertEqual(len(module_scopes), 0)
    self.assertEqual(len(class_scopes), 1)
    self.assertEqual(len(function_scopes), 1)

    class_scope = class_scopes[0]
    function_scope = function_scopes[0]

    self.assertIsInstance(class_scope, ClassScope)
    self.assertIsInstance(function_scope, FunctionScope)

    self.assertIn('x', class_scope)
    self.assertIn('bar', class_scope)

    self.assertIn('a', function_scope)
    self.assertIn('b', function_scope)
    self.assertIn('d', function_scope)
    self.assertIn('e', function_scope)

    self.assertIsInstance(class_scope['bar'], FunctionDefinition)
    self.assertIsInstance(class_scope['x'], Assignment)

    self.assertIsInstance(function_scope['a'], Argument)
    self.assertIsInstance(function_scope['b'], Argument)
    self.assertIsInstance(function_scope['d'], Argument)
    self.assertIsInstance(function_scope['e'], Argument)
</t>
<t tx="ekr.20250430053637.74">def test_scope_function(self):
    checker = self.flakes('''
    def foo(a, b=1, *d, **e):
        def bar(f, g=1, *h, **i):
            pass
    ''', is_segment=True)

    scopes = checker.deadScopes
    module_scopes = [
        scope for scope in scopes if scope.__class__ is ModuleScope]
    function_scopes = [
        scope for scope in scopes if scope.__class__ is FunctionScope]

    # Ensure module scope is not present because we are analysing
    # the inner contents of foo
    self.assertEqual(len(module_scopes), 0)
    self.assertEqual(len(function_scopes), 2)

    function_scope_foo = function_scopes[1]
    function_scope_bar = function_scopes[0]

    self.assertIsInstance(function_scope_foo, FunctionScope)
    self.assertIsInstance(function_scope_bar, FunctionScope)

    self.assertIn('a', function_scope_foo)
    self.assertIn('b', function_scope_foo)
    self.assertIn('d', function_scope_foo)
    self.assertIn('e', function_scope_foo)
    self.assertIn('bar', function_scope_foo)

    self.assertIn('f', function_scope_bar)
    self.assertIn('g', function_scope_bar)
    self.assertIn('h', function_scope_bar)
    self.assertIn('i', function_scope_bar)

    self.assertIsInstance(function_scope_foo['bar'], FunctionDefinition)
    self.assertIsInstance(function_scope_foo['a'], Argument)
    self.assertIsInstance(function_scope_foo['b'], Argument)
    self.assertIsInstance(function_scope_foo['d'], Argument)
    self.assertIsInstance(function_scope_foo['e'], Argument)

    self.assertIsInstance(function_scope_bar['f'], Argument)
    self.assertIsInstance(function_scope_bar['g'], Argument)
    self.assertIsInstance(function_scope_bar['h'], Argument)
    self.assertIsInstance(function_scope_bar['i'], Argument)
</t>
<t tx="ekr.20250430053637.75">def test_scope_async_function(self):
    self.flakes('async def foo(): pass', is_segment=True)
</t>
<t tx="ekr.20250430053637.76">"""
Tests for dict duplicate keys Pyflakes behavior.
"""

from pyflakes import messages as m
from pyflakes.test.harness import TestCase


@others
@language python
@tabwidth -4
</t>
<t tx="ekr.20250430053637.77">class Test(TestCase):
    @others
</t>
<t tx="ekr.20250430053637.78">def test_duplicate_keys(self):
    self.flakes(
        "{'yes': 1, 'yes': 2}",
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.79">def test_duplicate_keys_bytes_vs_unicode_py3(self):
    self.flakes("{b'a': 1, u'a': 2}")
</t>
<t tx="ekr.20250430053637.8">class CheckTests(TestCase):
        """
        Tests for L{check} and L{checkPath} which check a file for flakes.
        """
@others
</t>
<t tx="ekr.20250430053637.80">def test_duplicate_values_bytes_vs_unicode_py3(self):
    self.flakes(
        "{1: b'a', 1: u'a'}",
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.81">def test_multiple_duplicate_keys(self):
    self.flakes(
        "{'yes': 1, 'yes': 2, 'no': 2, 'no': 3}",
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.82">def test_duplicate_keys_in_function(self):
    self.flakes(
        '''
        def f(thing):
            pass
        f({'yes': 1, 'yes': 2})
        ''',
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.83">def test_duplicate_keys_in_lambda(self):
    self.flakes(
        "lambda x: {(0,1): 1, (0,1): 2}",
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.84">def test_duplicate_keys_tuples(self):
    self.flakes(
        "{(0,1): 1, (0,1): 2}",
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.85">def test_duplicate_keys_tuples_int_and_float(self):
    self.flakes(
        "{(0,1): 1, (0,1.0): 2}",
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.86">def test_duplicate_keys_ints(self):
    self.flakes(
        "{1: 1, 1: 2}",
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.87">def test_duplicate_keys_bools(self):
    self.flakes(
        "{True: 1, True: 2}",
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.88">def test_duplicate_keys_bools_false(self):
    # Needed to ensure 2.x correctly coerces these from variables
    self.flakes(
        "{False: 1, False: 2}",
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.89">def test_duplicate_keys_none(self):
    self.flakes(
        "{None: 1, None: 2}",
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.9">class IntegrationTests(TestCase):
    """
    Tests of the pyflakes script that actually spawn the script.
    """
    @others
</t>
<t tx="ekr.20250430053637.90">def test_duplicate_variable_keys(self):
    self.flakes(
        '''
        a = 1
        {a: 1, a: 2}
        ''',
        m.MultiValueRepeatedKeyVariable,
        m.MultiValueRepeatedKeyVariable,
    )
</t>
<t tx="ekr.20250430053637.91">def test_duplicate_variable_values(self):
    self.flakes(
        '''
        a = 1
        b = 2
        {1: a, 1: b}
        ''',
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.92">def test_duplicate_variable_values_same_value(self):
    # Current behaviour is not to look up variable values. This is to
    # confirm that.
    self.flakes(
        '''
        a = 1
        b = 1
        {1: a, 1: b}
        ''',
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.93">def test_duplicate_key_float_and_int(self):
    """
    These do look like different values, but when it comes to their use as
    keys, they compare as equal and so are actually duplicates.
    The literal dict {1: 1, 1.0: 1} actually becomes {1.0: 1}.
    """
    self.flakes(
        '''
        {1: 1, 1.0: 2}
        ''',
        m.MultiValueRepeatedKeyLiteral,
        m.MultiValueRepeatedKeyLiteral,
    )
</t>
<t tx="ekr.20250430053637.94">def test_no_duplicate_key_error_same_value(self):
    self.flakes('''
    {'yes': 1, 'yes': 1}
    ''')
</t>
<t tx="ekr.20250430053637.95">def test_no_duplicate_key_errors(self):
    self.flakes('''
    {'yes': 1, 'no': 2}
    ''')
</t>
<t tx="ekr.20250430053637.96">def test_no_duplicate_keys_tuples_same_first_element(self):
    self.flakes("{(0,1): 1, (0,2): 1}")
</t>
<t tx="ekr.20250430053637.97">def test_no_duplicate_key_errors_func_call(self):
    self.flakes('''
    def test(thing):
        pass
    test({True: 1, None: 2, False: 1})
    ''')
</t>
<t tx="ekr.20250430053637.98">def test_no_duplicate_key_errors_bool_or_none(self):
    self.flakes("{True: 1, None: 2, False: 1}")
</t>
<t tx="ekr.20250430053637.99">def test_no_duplicate_key_errors_ints(self):
    self.flakes('''
    {1: 1, 2: 1}
    ''')
</t>
<t tx="ekr.20250430061458.1"></t>
<t tx="ekr.20250430062819.1">g.cls()
import re
import textwrap

message_pat = re.compile("^message = ['\"](.*)['\"]\s*$", re.MULTILINE)
ctor_pat = re.compile('^def __init__\((.*)\):\s*$', re.MULTILINE)
args_pat = re.compile('^\s+self.message_args = \((.*)\)\s*$', re.MULTILINE)

@others
h = '@@file messages.py'
p = g.findTopLevelNode(c, h)
messages = p.firstChild().next()
for message in messages.children():
    try:
        update_body(message)
    except Exception:
        g.es_exception()
        break
</t>
<t tx="ekr.20250430111617.1">def update_body(message) -&gt; bool:
    child = message.firstChild()
    if not child:
        return False
    message_m = message_pat.search(child.b)
    if message_m is None:
        print(f"No message line in {message.h}")
        return
    ctor_m = ctor_pat.search(child.b)
    if ctor_m is None:
        print(f"No ctor line in {message.h}")
        return
    args_m = args_pat.search(child.b)
    if args_m is None:
        print(f"No message_args line in {message.h}")
        return
    new_message_text = update_message(message_m.group(1), args_m.group(1))
    new_ctor_args = update_ctor(ctor_m.group(1))
    replace_body(new_ctor_args, new_message_text)
    return True</t>
<t tx="ekr.20250430114925.1">### to do: Handle {0}, etc.
percent_pat = re.compile('%[rs]')

def update_message(message_text, args_line) -&gt; str:
    replacements = list(percent_pat.finditer(message_text))
    args = [z.strip() for z in args_line.split(',') if z.strip()]
    if len(replacements) != len(args):
        g.printObj(replacements, tag='Replacements')
        g.printObj(args, tag='Args')
        g.trace(message_text, args_line)
        assert False, 'Mismatch between replacements and args'
        # return message_text
    new_message_text = message_text
    for arg in args:
        arg_m = percent_pat.search(new_message_text)
        assert arg_m, repr(f"Not found: {arg}")
        start = arg_m.start()
        new_message_text = new_message_text[:start] + f"{{{arg}}}" + new_message_text[start+2:]
    assert '"' not in new_message_text
    return 'f"' + new_message_text + '"'
</t>
<t tx="ekr.20250430121603.1">def update_ctor(ctor_args) -&gt; str:
    new_args = ctor_args.strip()
    return new_args + ', message' if new_args else new_args</t>
<t tx="ekr.20250430122722.1">def replace_body(new_ctor_args, new_message_text):
    """
    Replace @others by::
        
        message = '&lt;updated_message&gt;'
        def __init__(self, &lt;updated args_list&gt;):
            Message.__init__(self, &lt;updated_args_list&gt;)
            
    Delete the self.message_args line.
    """
    indent = 4 * ' '
    result = []
    lines = g.splitLines(message.b)  # @others adds to the lines.
    for s in lines:
        s_strip = s.strip()
        if not s_strip:  # Delete blanks lines.
            pass
        elif s.startswith('class'):
            result.append(s)
            result.append('\n')
        elif s_strip.startswith('@others'):
            child = message.firstChild()
            lines.extend(g.splitLines(child.b))
        elif s.startswith("message = '"):
            pass  # We'll write this line later.
        elif s_strip.startswith('self.message_args'):
            pass  # Delete the line.
        elif s_strip.startswith(f"Message.__init__"):
            result.append(f"{indent}{indent}message = {new_message_text}\n")
            result.append(f"{indent}{indent}Message.__init__({new_ctor_args}):\n")
        elif s.startswith('def __init__'):
            result.append(f"{indent}def __init__({new_ctor_args}):\n")
        else:
            g.trace(f"OOPS: {message.h}: Unexpected line: {s!r}")
    g.printObj(result, tag='Exit')
</t>
<t tx="ekr.20250430132623.1"># Found 2 marked nodes</t>
<t tx="ekr.20250501074230.1"></t>
<t tx="ekr.20250501074906.1"></t>
<t tx="ekr.20250512055230.1">@language ini

[bdist_wheel]

# Supports Python 3, so universal should be zero.
universal=0

[metadata]

# Becomes the home-page in `pip show leo`.
url = https://leo-editor.github.io/leo-editor/

[flake8]

exclude =
    .git,
    __pycache__,

extend-ignore =

    # Don't check B020. It conflicts with Leo idioms like `for p in p.children():
    # Found for loop that reassigns the iterable it is iterating with each iterable value.
    B020

    # blank line contains whitespace
    # W293

    # Comments and continuation lines...

    # expected an indented block (comment)
    E115

    # unexpected indentation (comment)
    E116

    # over-indented (comment)
    E117

    # continuation line over-indented for visual indent
    E127

    # continuation line missing indentation or outdented.
    E122

    # closing bracket does not match visual indentation.
    E124

    # continuation line with same indent as next logical line
    E125

    # continuation line under-indented for visual indent.
    E128

    # visually indented line with same indent as next logical line
    E129

    # continuation line unaligned for hanging indent.
    E131

    # whitespace before ':'
    # Conflict between black and flake8.
    E203

    # whitespace before '['
    # E211

    # multiple spaces before operator
    # Conflicts with extra spacing in @nobeautify nodes.
    E221

    # multiple spaces after operator
    # Conflicts with extra spacing in @nobeautify nodes.
    E222

    # missing whitespace around operator
    # Conflicts with extra spacing in @nobeautify nodes.
    # E225

    # missing whitespace after ','
    # E231

    # unexpected spaces around keyword / parameter equals
    # Conflicts with extra spacing in @nobeautify nodes.
    E251

    # at least two spaces before inline comment
    # E261

    # missing whitespace around parameter equals.
    # E252

    # inline comment should start with '# '
    E262

    # block comment should start with '# '.
    E265

    # too many leading '#' for block comment
    # Prohibits ### comments.
    E266

    # multiple spaces before keyword
    # Interferes with @nobeautify
    E272

    # missing whitespace after keyword.
    # Interferes with @nobeautify. Also warns about assert(whatever) and except(list).
    E275

    # trailing whitespace
    # E293

    # expected 1 blank line, found 0.
    E301

    # expected 2 blank lines, found 1.
    E302

    # whitespace before ':'
    # E203

    # too many blank lines (2)
    E303

    # expected 2 blank lines after class or function definition, found 0.
    E305

    # expected 1 blank line before a nested definition, found 0.
    E306
    
    # module level import not at top of file
    E402

    # Line too long.
    E501

    # global `whatever` is unused: name is never assigned in scope.
    # F824
</t>
<t tx="ekr.20250512060544.1">"""Run semantic_check.py with `python -c semanic_check.py."""
import os

g.cls()
os.chdir(r'C:\Repos\ekr-semantic-cache\src')
g.execute_shell_commands([
    # 'dir',
    "python semantic_cache.py"
])
print('Done')  # , os.getcwd())</t>
<t tx="ekr.20250512061621.1">"""Run all unittests"""
import os

g.cls()
os.chdir(r'C:\Repos\ekr-semantic-cache')
g.execute_shell_commands([
    "python -m unittest --verbose tests.test"
])
print('Done')
</t>
<t tx="ekr.20250512062255.1">def test_import(self):
    from unittest import TestCase  # pylint: disable=reimported
    assert TestCase is not None
</t>
<t tx="ekr.20250512071928.1"></t>
<t tx="ekr.20250512073231.1">class CacheTests(TestCase):
    @others
</t>
<t tx="ekr.20250512075040.1"></t>
</tnodes>
</leo_file>
